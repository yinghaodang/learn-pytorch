{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59c7f4f",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Cooper'\">\n",
    "SimpleOCR\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379a70e-4756-4e8d-99bc-7096b93a1125",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Cooper'\">\n",
    "Import necessary package\n",
    "<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9aab6b6-ea46-4433-b920-407792ac8494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing modules for handwritten text generation.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import v2\n",
    "from trdg.generators import GeneratorFromRandom\n",
    "\n",
    "# local file\n",
    "import ocr_batch_functions\n",
    "import ocr_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4afb096-1cbc-453f-8967-02e799c0ce7d",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Cooper'\"> \n",
    "Define the OCR model\n",
    "<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f535ca-d83a-4a81-94ba-92b148d69a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR(nn.Module):\n",
    "    @staticmethod\n",
    "    def CNNBlock(c, k):\n",
    "        \"\"\"\n",
    "        c: out_channel\n",
    "        k: kernel_size\n",
    "        the input tensor's shape is [batch_size, nums_features, height, width]\n",
    "        the output tensor's features will be unified to c, shape is [batch_size, c, height, width]\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(out_channels=c, kernel_size=k, padding=(k-1)//2),  # Zero-Padding\n",
    "            nn.BatchNorm2d(c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def __init__(self, numChars, outputSize, hiddenSize, numLayers):\n",
    "        \"\"\"\n",
    "        numChars: the number of characters that the network must learn to recognize\n",
    "        outputSize: the size of the features that the convolutional sub-network will pass into the RNN\n",
    "        hiddenSize: the dimensionality of RNN hidden state vectors\n",
    "        numLayers: the number of layers in the RNN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.numChars = numChars\n",
    "        self.outputSize = outputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.numLayers = numLayers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.cnnsubset = nn.Sequential(\n",
    "            self.CNNBlock(8, 3),\n",
    "            self.CNNBlock(16, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            self.CNNBlock(32, 3),\n",
    "            self.CNNBlock(64, 3),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.denselayer = nn.Linear(512, outputSize)  # 512 is 64 * 8\n",
    "        self.bidirectionallstm = nn.LSTM(outputSize, hiddenSize, numLayers, batch_first=True, bidirectional=True)\n",
    "        self.logitlayer = nn.Linear(2 * hiddenSize, numChars + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the images that inputed, with shape [batch_size, channel_size, height, width]\n",
    "        batch_size, _, _, width = x.size()\n",
    "        x = self.cnnsubset(x).reshape(batch_size, 512, width//4).transpose(1,2)\n",
    "        x = self.relu(self.denselayer(x))\n",
    "        o, _ = self.bidirectionallstm(x)  # Discard the result of the last time step in the RNN.\n",
    "        p = self.logitlayer(o).transpose(0,1)\n",
    "        result = torch.nn.functional.log_softmax(p, dim=-1) # shape is [T, batch_size, numChars+1]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a4882-40e9-4d14-8d99-2af52062953f",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Cooper'\"> \n",
    "Train the OCR model\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84acf7-c462-4f76-9087-7b44e4bf1da2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "    Generate the custom training and validation DataLoaders that use TRDG.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e322de3-fd72-4ae9-9c95-77ecca568942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy from CSCI6962/4140 HW5\n",
    "allChars = string.digits\n",
    "charsToLabelsDict = dict((char, ind + 1) for ind, char in enumerate(allChars))  # The string \"0\" is numbered as 1.\n",
    "labelsToCharsDict = dict((ind + 1, char) for ind, char in enumerate(allChars))\n",
    "\n",
    "# BaseGenerator\n",
    "trdgGenerator = GeneratorFromRandom(\n",
    "    use_symbols=False,\n",
    "    use_letters=False,\n",
    "    background_type=1\n",
    ")\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.PILToTensor(),  # Convert PIL image to tensor\n",
    "    v2.RandomAffine(degrees=2.0, translate=(0.04, 0.15), shear=4.0),\n",
    "    v2.ToDtype(torch.float32),  # Normalize expects float input\n",
    "    v2.Normalize(\n",
    "        mean=[255 * 0.485, 255 * 0.456, 255 * 0.406],\n",
    "        std=[255 * 0.229, 255 * 0.224, 255 * 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "epochSize = 10  # the number of minibatches per epoch\n",
    "batchSize = 100\n",
    "height = 32\n",
    "\n",
    "dgParams = {\n",
    "    \"epochSize\": epochSize,\n",
    "    \"batchSize\": batchSize,\n",
    "    \"height\": height,\n",
    "    \"transform\": transforms,\n",
    "    \"char_to_lbl_dict\": charsToLabelsDict\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "trainGenerator returns X, targets, targetLengths, inputLengths\n",
    "valGenerator returns images, X, targets, targetLengths, inputLengths\n",
    "\n",
    "images        :  the sequence of raw images generated by trdg in this minibatch (batch_size, PIL.Image)\n",
    "X             :  the tensor formed by converting the images to tensors; has shape (batch_size, channel, height, wight)\n",
    "targets       :  the tensor containing the character sequences corresponding to each image; \n",
    "targetLengths :  this has shape (batch_size, `max), where `max is the length of the longest character sequence in the minibatch; \n",
    "                 the other character sequences are padded to that length.\n",
    "inputLengths  :  this tensor contains the true length of each character sequence in the minibatch, and has shape batch_size.\n",
    "\"\"\"\n",
    "trainGenerator = ocr_batch_functions.OCR_generator(trdgGenerator, **dgParams)\n",
    "valGenerator = ocr_batch_functions.OCR_generator(trdgGenerator, **dgParams, isValidation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ea7a2-33f7-4e25-b34f-7503e0388f7a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "Use valGenerator to create 10 example input images, and display them in a 10 row array.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059efd0f-4e88-4ee9-9768-0409d4fe2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = next(valGenerator)[0]  # get the first minibatch\n",
    "# for image in images[:10]:\n",
    "#     display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d94b1",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "Create model, an instance of the OCR class. Take an appropriate numChars, outputSize = 128, hiddenSize = 64, and numLayers = 4\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058ffd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OCR(\n",
       "  (relu): ReLU()\n",
       "  (cnnsubset): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): LazyConv2d(0, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LazyConv2d(0, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Sequential(\n",
       "      (0): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (denselayer): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (bidirectionallstm): LSTM(128, 64, num_layers=4, batch_first=True, bidirectional=True)\n",
       "  (logitlayer): Linear(in_features=128, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_model = OCR(numChars=10, outputSize=128, hiddenSize=64, numLayers=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "ocr_model.to(device)\n",
    "\n",
    "# test the model\n",
    "# x = torch.randn(7, 3, 32, 128)  # batch_size, channel_size, height, width\n",
    "# ocr_model(x).shape  # T, batch_size, numChars+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8caed",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "Create an ADAM optimizer for training model, using initial global <strong>learning rate lr=5e-3</strong>.\n",
    "Control the stepsize of the optimizer by using a learning rate scheduler to reduce the global learning rate by 1/2 after every 50 optimization steps; use <strong>torch.optim.lr_scheduler.StepLR</strong> to accomplish this.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e77ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 5e-3\n",
    "optimizer = optim.Adam(ocr_model.parameters(), lr=initial_lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46127ede",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "Write standard training functions train_epoch(dataGenerator, model, optimizer, reportInterval=1) and validate(dataGenerator, model). \n",
    "The training function should print out the current minibatch loss every reportInterval minibatches. \n",
    "The validation function should accumulate the CTCLoss over the entire validation set, and print and return this accumulated loss divided by the number of minibatches in that epoch.\n",
    "</div>\n",
    "<div style=\"font-family: 'Consolas'\">\n",
    "Inside of these training functions, you must check when an epoch has ended and break out of the loop enumerating over the DataLoaders, as they do not automatically end themselves at the end of an epoch. You can do so by checking batchnum == len(dataGenerator), where batchnum is the index of the current minibatch.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b152a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Consolas\">\n",
    "    <strong>Note that:</strong> trainGenerator and varGenerator do not implement the logic to raise a StopIteration exception in the __next__() method. The iterators will not terminate normally.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0698aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataGenerator, model, optimizer, reportInterval=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batchnum, (X, targets, targetLengths, inputLengths) in enumerate(dataGenerator):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X.to(device))\n",
    "        loss = nn.CTCLoss(blank=0, reduction=\"sum\")(output, targets, inputLengths, targetLengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.item()\n",
    "        total_loss += loss\n",
    "        total_samples += len(inputLengths)\n",
    "        if (batchnum + 1) % reportInterval == 0:\n",
    "            print(f\"Batch {batchnum + 1}, Loss: {loss}, average loss {total_loss / total_samples}\")\n",
    "        if batchnum == len(dataGenerator) - 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f261e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataGenerator, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batchnum, (originalImages, X, targets, targetLengths, inputLengths) in enumerate(dataGenerator):\n",
    "            output = model(X.to(device))\n",
    "            loss = nn.CTCLoss(blank=0, reduction=\"sum\")(output, targets, inputLengths, targetLengths)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += len(inputLengths)\n",
    "\n",
    "            if batchnum == len(dataGenerator) - 1:\n",
    "                break\n",
    "    \n",
    "    average_loss = total_loss / total_samples\n",
    "    print(\"Total loss:\", total_loss)\n",
    "    print(\"Validation loss:\", average_loss)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c1d41",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "Set epochs=300 Train the model using the following code to periodically report its\n",
    "performance and to save the best performing model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed97baa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 8020.2666015625, average loss 80.202666015625\n",
      "Batch 2, Loss: 6349.8505859375, average loss 71.8505859375\n",
      "Batch 3, Loss: 2844.9990234375, average loss 57.383720703125\n",
      "Batch 4, Loss: 1672.936767578125, average loss 47.220132446289064\n",
      "Batch 5, Loss: 2203.06005859375, average loss 42.18222607421875\n",
      "Batch 6, Loss: 1967.20361328125, average loss 38.430527750651045\n",
      "Batch 7, Loss: 2115.9140625, average loss 35.96318673270089\n",
      "Batch 8, Loss: 1809.6080322265625, average loss 33.72979843139648\n",
      "Batch 9, Loss: 1662.2576904296875, average loss 31.828996039496527\n",
      "Batch 10, Loss: 1646.127197265625, average loss 30.2922236328125\n",
      "Total loss: 17579.650390625\n",
      "Validation loss: 17.579650390625\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 3\n",
      "\tInput: 76206\tPrediction: 3\n",
      "Epoch 2, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1620.703857421875, average loss 16.20703857421875\n",
      "Batch 2, Loss: 1690.470947265625, average loss 16.5558740234375\n",
      "Batch 3, Loss: 1740.721923828125, average loss 16.83965576171875\n",
      "Batch 4, Loss: 1741.2618408203125, average loss 16.982896423339845\n",
      "Batch 5, Loss: 1572.3701171875, average loss 16.731057373046873\n",
      "Batch 6, Loss: 1751.3046875, average loss 16.861388956705728\n",
      "Batch 7, Loss: 1688.0849609375, average loss 16.864169049944195\n",
      "Batch 8, Loss: 1687.253173828125, average loss 16.865214385986327\n",
      "Batch 9, Loss: 1653.1826171875, average loss 16.82817125108507\n",
      "Batch 10, Loss: 1696.38330078125, average loss 16.841737426757813\n",
      "Total loss: 16501.852661132812\n",
      "Validation loss: 16.501852661132812\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 3, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1605.6298828125, average loss 16.056298828125\n",
      "Batch 2, Loss: 1661.6109619140625, average loss 16.336204223632812\n",
      "Batch 3, Loss: 1742.52734375, average loss 16.699227294921876\n",
      "Batch 4, Loss: 1585.4814453125, average loss 16.488124084472656\n",
      "Batch 5, Loss: 1753.5230712890625, average loss 16.69754541015625\n",
      "Batch 6, Loss: 1665.580078125, average loss 16.690587972005208\n",
      "Batch 7, Loss: 1693.1142578125, average loss 16.72495291573661\n",
      "Batch 8, Loss: 1621.343505859375, average loss 16.66101318359375\n",
      "Batch 9, Loss: 1673.5206298828125, average loss 16.669256863064238\n",
      "Batch 10, Loss: 1457.1875, average loss 16.459518676757813\n",
      "Total loss: 16376.595092773438\n",
      "Validation loss: 16.37659509277344\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 4, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1658.739990234375, average loss 16.58739990234375\n",
      "Batch 2, Loss: 1773.724365234375, average loss 17.16232177734375\n",
      "Batch 3, Loss: 1574.5087890625, average loss 16.689910481770834\n",
      "Batch 4, Loss: 1760.9244384765625, average loss 16.919743957519533\n",
      "Batch 5, Loss: 1672.524658203125, average loss 16.880844482421875\n",
      "Batch 6, Loss: 1657.11328125, average loss 16.82922587076823\n",
      "Batch 7, Loss: 1610.37451171875, average loss 16.725585763113838\n",
      "Batch 8, Loss: 1670.930419921875, average loss 16.723550567626955\n",
      "Batch 9, Loss: 1465.82763671875, average loss 16.494075656467015\n",
      "Batch 10, Loss: 1587.10693359375, average loss 16.431775024414062\n",
      "Total loss: 16373.322143554688\n",
      "Validation loss: 16.373322143554688\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 5, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1736.8646240234375, average loss 17.368646240234376\n",
      "Batch 2, Loss: 1576.1162109375, average loss 16.56490417480469\n",
      "Batch 3, Loss: 1741.23388671875, average loss 16.847382405598957\n",
      "Batch 4, Loss: 1652.700439453125, average loss 16.76728790283203\n",
      "Batch 5, Loss: 1659.9207763671875, average loss 16.733671875\n",
      "Batch 6, Loss: 1612.026611328125, average loss 16.631437581380208\n",
      "Batch 7, Loss: 1667.50146484375, average loss 16.637662876674106\n",
      "Batch 8, Loss: 1452.1492919921875, average loss 16.373141632080078\n",
      "Batch 9, Loss: 1589.83984375, average loss 16.32039238823785\n",
      "Batch 10, Loss: 1661.676025390625, average loss 16.35002917480469\n",
      "Total loss: 16364.833251953125\n",
      "Validation loss: 16.364833251953126\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 6, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1568.1533203125, average loss 15.681533203125\n",
      "Batch 2, Loss: 1743.9034423828125, average loss 16.560283813476563\n",
      "Batch 3, Loss: 1655.421630859375, average loss 16.55826131184896\n",
      "Batch 4, Loss: 1660.407958984375, average loss 16.569715881347655\n",
      "Batch 5, Loss: 1610.9449462890625, average loss 16.47766259765625\n",
      "Batch 6, Loss: 1671.256103515625, average loss 16.516812337239582\n",
      "Batch 7, Loss: 1462.5701904296875, average loss 16.246653703962053\n",
      "Batch 8, Loss: 1584.50537109375, average loss 16.196453704833985\n",
      "Batch 9, Loss: 1655.0264892578125, average loss 16.235766059027778\n",
      "Batch 10, Loss: 1747.160400390625, average loss 16.359349853515624\n",
      "Total loss: 16335.592407226562\n",
      "Validation loss: 16.335592407226564\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 7, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1740.851806640625, average loss 17.40851806640625\n",
      "Batch 2, Loss: 1654.076171875, average loss 16.974639892578125\n",
      "Batch 3, Loss: 1658.043701171875, average loss 16.843238932291666\n",
      "Batch 4, Loss: 1609.869384765625, average loss 16.65710266113281\n",
      "Batch 5, Loss: 1669.7645263671875, average loss 16.665211181640625\n",
      "Batch 6, Loss: 1459.3841552734375, average loss 16.31998291015625\n",
      "Batch 7, Loss: 1586.7427978515625, average loss 16.25533220563616\n",
      "Batch 8, Loss: 1657.69287109375, average loss 16.295531768798828\n",
      "Batch 9, Loss: 1748.34912109375, average loss 16.427527262369793\n",
      "Batch 10, Loss: 1568.643310546875, average loss 16.353417846679687\n",
      "Total loss: 16333.351440429688\n",
      "Validation loss: 16.333351440429688\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 8, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1653.059814453125, average loss 16.53059814453125\n",
      "Batch 2, Loss: 1659.8839111328125, average loss 16.564718627929686\n",
      "Batch 3, Loss: 1610.511474609375, average loss 16.411517333984374\n",
      "Batch 4, Loss: 1668.09521484375, average loss 16.478876037597658\n",
      "Batch 5, Loss: 1456.16357421875, average loss 16.095427978515627\n",
      "Batch 6, Loss: 1583.5760498046875, average loss 16.05215006510417\n",
      "Batch 7, Loss: 1657.506103515625, average loss 16.126851632254464\n",
      "Batch 8, Loss: 1751.28271484375, average loss 16.300098571777344\n",
      "Batch 9, Loss: 1564.8043212890625, average loss 16.227647976345487\n",
      "Batch 10, Loss: 1742.748779296875, average loss 16.347631958007813\n",
      "Total loss: 16338.093627929688\n",
      "Validation loss: 16.338093627929688\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: \n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 9, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1660.8497314453125, average loss 16.608497314453125\n",
      "Batch 2, Loss: 1610.75634765625, average loss 16.35803039550781\n",
      "Batch 3, Loss: 1670.001220703125, average loss 16.47202433268229\n",
      "Batch 4, Loss: 1456.333740234375, average loss 15.994852600097657\n",
      "Batch 5, Loss: 1587.029296875, average loss 15.969940673828125\n",
      "Batch 6, Loss: 1656.295166015625, average loss 16.068775838216144\n",
      "Batch 7, Loss: 1750.0401611328125, average loss 16.273293805803572\n",
      "Batch 8, Loss: 1566.6805419921875, average loss 16.19748275756836\n",
      "Batch 9, Loss: 1738.18701171875, average loss 16.329081353081598\n",
      "Batch 10, Loss: 1654.8824462890625, average loss 16.3510556640625\n",
      "Total loss: 16344.89306640625\n",
      "Validation loss: 16.34489306640625\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 10, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1611.4613037109375, average loss 16.114613037109375\n",
      "Batch 2, Loss: 1671.41015625, average loss 16.41435729980469\n",
      "Batch 3, Loss: 1461.6922607421875, average loss 15.81521240234375\n",
      "Batch 4, Loss: 1583.3623046875, average loss 15.819815063476563\n",
      "Batch 5, Loss: 1656.728515625, average loss 15.96930908203125\n",
      "Batch 6, Loss: 1751.48046875, average loss 16.226891682942707\n",
      "Batch 7, Loss: 1567.0792236328125, average loss 16.14744890485491\n",
      "Batch 8, Loss: 1743.882080078125, average loss 16.3088703918457\n",
      "Batch 9, Loss: 1654.1580810546875, average loss 16.334727105034723\n",
      "Batch 10, Loss: 1661.408203125, average loss 16.36266259765625\n",
      "Total loss: 16337.664184570312\n",
      "Validation loss: 16.33766418457031\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 11, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1668.927490234375, average loss 16.68927490234375\n",
      "Batch 2, Loss: 1457.521728515625, average loss 15.63224609375\n",
      "Batch 3, Loss: 1584.510498046875, average loss 15.703199055989584\n",
      "Batch 4, Loss: 1657.8026123046875, average loss 15.921905822753907\n",
      "Batch 5, Loss: 1749.75146484375, average loss 16.237027587890626\n",
      "Batch 6, Loss: 1567.279296875, average loss 16.14298848470052\n",
      "Batch 7, Loss: 1739.187744140625, average loss 16.32140119280134\n",
      "Batch 8, Loss: 1655.2491455078125, average loss 16.350287475585937\n",
      "Batch 9, Loss: 1661.284423828125, average loss 16.37946044921875\n",
      "Batch 10, Loss: 1611.202880859375, average loss 16.35271728515625\n",
      "Total loss: 16339.148559570312\n",
      "Validation loss: 16.33914855957031\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 12, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1459.249267578125, average loss 14.59249267578125\n",
      "Batch 2, Loss: 1586.1156005859375, average loss 15.226824340820313\n",
      "Batch 3, Loss: 1655.794189453125, average loss 15.670530192057292\n",
      "Batch 4, Loss: 1750.869873046875, average loss 16.130072326660155\n",
      "Batch 5, Loss: 1568.060546875, average loss 16.040178955078126\n",
      "Batch 6, Loss: 1740.9375, average loss 16.268378295898437\n",
      "Batch 7, Loss: 1653.9351806640625, average loss 16.307088797433035\n",
      "Batch 8, Loss: 1662.401611328125, average loss 16.34670471191406\n",
      "Batch 9, Loss: 1610.057861328125, average loss 16.319357367621528\n",
      "Batch 10, Loss: 1670.04296875, average loss 16.357464599609376\n",
      "Total loss: 16334.025390625\n",
      "Validation loss: 16.334025390625\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 13, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1584.3974609375, average loss 15.843974609375\n",
      "Batch 2, Loss: 1656.8193359375, average loss 16.206083984375\n",
      "Batch 3, Loss: 1746.88720703125, average loss 16.62701334635417\n",
      "Batch 4, Loss: 1569.859130859375, average loss 16.39490783691406\n",
      "Batch 5, Loss: 1740.4295654296875, average loss 16.596785400390626\n",
      "Batch 6, Loss: 1652.9613037109375, average loss 16.585590006510415\n",
      "Batch 7, Loss: 1660.189208984375, average loss 16.587918875558035\n",
      "Batch 8, Loss: 1610.57470703125, average loss 16.527647399902342\n",
      "Batch 9, Loss: 1668.7896728515625, average loss 16.545452880859376\n",
      "Batch 10, Loss: 1456.1123046875, average loss 16.34701989746094\n",
      "Total loss: 16343.231201171875\n",
      "Validation loss: 16.343231201171875\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 14, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1656.7847900390625, average loss 16.567847900390625\n",
      "Batch 2, Loss: 1753.90087890625, average loss 17.053428344726562\n",
      "Batch 3, Loss: 1567.63525390625, average loss 16.594403076171876\n",
      "Batch 4, Loss: 1740.739501953125, average loss 16.79765106201172\n",
      "Batch 5, Loss: 1656.01708984375, average loss 16.750155029296874\n",
      "Batch 6, Loss: 1662.62841796875, average loss 16.72950988769531\n",
      "Batch 7, Loss: 1611.149169921875, average loss 16.641221575055802\n",
      "Batch 8, Loss: 1671.590087890625, average loss 16.65055648803711\n",
      "Batch 9, Loss: 1458.4652099609375, average loss 16.421011555989583\n",
      "Batch 10, Loss: 1585.723876953125, average loss 16.36463427734375\n",
      "Total loss: 16353.623413085938\n",
      "Validation loss: 16.353623413085938\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 15, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1752.885498046875, average loss 17.52885498046875\n",
      "Batch 2, Loss: 1567.7783203125, average loss 16.603319091796877\n",
      "Batch 3, Loss: 1743.697265625, average loss 16.88120361328125\n",
      "Batch 4, Loss: 1654.2774658203125, average loss 16.79659637451172\n",
      "Batch 5, Loss: 1661.9560546875, average loss 16.761189208984376\n",
      "Batch 6, Loss: 1612.8798828125, average loss 16.65579081217448\n",
      "Batch 7, Loss: 1669.2413330078125, average loss 16.66102260044643\n",
      "Batch 8, Loss: 1458.077392578125, average loss 16.40099151611328\n",
      "Batch 9, Loss: 1585.368896484375, average loss 16.340180121527776\n",
      "Batch 10, Loss: 1657.90185546875, average loss 16.36406396484375\n",
      "Total loss: 16358.333862304688\n",
      "Validation loss: 16.358333862304686\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 16, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1567.560791015625, average loss 15.67560791015625\n",
      "Batch 2, Loss: 1745.540771484375, average loss 16.5655078125\n",
      "Batch 3, Loss: 1659.4342041015625, average loss 16.57511922200521\n",
      "Batch 4, Loss: 1660.34326171875, average loss 16.58219757080078\n",
      "Batch 5, Loss: 1610.36962890625, average loss 16.486497314453125\n",
      "Batch 6, Loss: 1671.7325439453125, average loss 16.524968668619792\n",
      "Batch 7, Loss: 1461.64306640625, average loss 16.252320382254464\n",
      "Batch 8, Loss: 1584.440185546875, average loss 16.20133056640625\n",
      "Batch 9, Loss: 1656.016845703125, average loss 16.241201443142362\n",
      "Batch 10, Loss: 1749.7861328125, average loss 16.366867431640625\n",
      "Total loss: 16335.199829101562\n",
      "Validation loss: 16.335199829101562\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 17, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1741.780517578125, average loss 17.41780517578125\n",
      "Batch 2, Loss: 1651.9735107421875, average loss 16.968770141601563\n",
      "Batch 3, Loss: 1661.05322265625, average loss 16.84935750325521\n",
      "Batch 4, Loss: 1611.642578125, average loss 16.666124572753905\n",
      "Batch 5, Loss: 1671.11962890625, average loss 16.675138916015626\n",
      "Batch 6, Loss: 1460.36376953125, average loss 16.329888712565104\n",
      "Batch 7, Loss: 1587.7896728515625, average loss 16.265318429129465\n",
      "Batch 8, Loss: 1659.20703125, average loss 16.30616241455078\n",
      "Batch 9, Loss: 1756.16357421875, average loss 16.44565945095486\n",
      "Batch 10, Loss: 1566.893798828125, average loss 16.3679873046875\n",
      "Total loss: 16333.6865234375\n",
      "Validation loss: 16.3336865234375\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 18, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1654.578125, average loss 16.54578125\n",
      "Batch 2, Loss: 1663.695556640625, average loss 16.591368408203124\n",
      "Batch 3, Loss: 1611.6544189453125, average loss 16.43309366861979\n",
      "Batch 4, Loss: 1671.0362548828125, average loss 16.502410888671875\n",
      "Batch 5, Loss: 1457.56787109375, average loss 16.117064453125\n",
      "Batch 6, Loss: 1584.19873046875, average loss 16.07121826171875\n",
      "Batch 7, Loss: 1658.3760986328125, average loss 16.14443865094866\n",
      "Batch 8, Loss: 1757.7666015625, average loss 16.323592071533202\n",
      "Batch 9, Loss: 1565.703857421875, average loss 16.2495305718316\n",
      "Batch 10, Loss: 1741.971435546875, average loss 16.366548950195313\n",
      "Total loss: 16339.400024414062\n",
      "Validation loss: 16.33940002441406\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: \n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 19, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1661.5128173828125, average loss 16.615128173828126\n",
      "Batch 2, Loss: 1611.6644287109375, average loss 16.36588623046875\n",
      "Batch 3, Loss: 1671.9444580078125, average loss 16.483739013671876\n",
      "Batch 4, Loss: 1458.4573974609375, average loss 16.00894775390625\n",
      "Batch 5, Loss: 1587.515625, average loss 15.982189453125\n",
      "Batch 6, Loss: 1657.559326171875, average loss 16.081090087890626\n",
      "Batch 7, Loss: 1754.2769775390625, average loss 16.289901471819196\n",
      "Batch 8, Loss: 1566.52587890625, average loss 16.211821136474608\n",
      "Batch 9, Loss: 1739.4078369140625, average loss 16.343183051215277\n",
      "Batch 10, Loss: 1654.30224609375, average loss 16.3631669921875\n",
      "Total loss: 16358.003173828125\n",
      "Validation loss: 16.358003173828124\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 20, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1612.342529296875, average loss 16.12342529296875\n",
      "Batch 2, Loss: 1673.672607421875, average loss 16.43007568359375\n",
      "Batch 3, Loss: 1462.85498046875, average loss 15.829567057291667\n",
      "Batch 4, Loss: 1583.051513671875, average loss 15.829804077148438\n",
      "Batch 5, Loss: 1657.2274169921875, average loss 15.978298095703124\n",
      "Batch 6, Loss: 1758.00927734375, average loss 16.24526387532552\n",
      "Batch 7, Loss: 1567.1236572265625, average loss 16.163259974888394\n",
      "Batch 8, Loss: 1743.8406982421875, average loss 16.322653350830077\n",
      "Batch 9, Loss: 1654.2099609375, average loss 16.347036268446182\n",
      "Batch 10, Loss: 1664.07470703125, average loss 16.376407348632814\n",
      "Total loss: 16353.3642578125\n",
      "Validation loss: 16.3533642578125\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 21, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1671.9713134765625, average loss 16.719713134765627\n",
      "Batch 2, Loss: 1460.072021484375, average loss 15.660216674804687\n",
      "Batch 3, Loss: 1585.0770263671875, average loss 15.723734537760416\n",
      "Batch 4, Loss: 1659.0390625, average loss 15.940398559570312\n",
      "Batch 5, Loss: 1755.238037109375, average loss 16.262794921875\n",
      "Batch 6, Loss: 1567.713623046875, average loss 16.165185139973957\n",
      "Batch 7, Loss: 1739.712890625, average loss 16.341177106584823\n",
      "Batch 8, Loss: 1654.463623046875, average loss 16.366609497070314\n",
      "Batch 9, Loss: 1663.71337890625, average loss 16.39666775173611\n",
      "Batch 10, Loss: 1611.670166015625, average loss 16.368671142578126\n",
      "Total loss: 16343.992919921875\n",
      "Validation loss: 16.343992919921874\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 22, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1461.320068359375, average loss 14.61320068359375\n",
      "Batch 2, Loss: 1585.56396484375, average loss 15.234420166015624\n",
      "Batch 3, Loss: 1655.8472900390625, average loss 15.675771077473959\n",
      "Batch 4, Loss: 1754.511474609375, average loss 16.143106994628905\n",
      "Batch 5, Loss: 1568.0445556640625, average loss 16.05057470703125\n",
      "Batch 6, Loss: 1741.8271484375, average loss 16.278524169921877\n",
      "Batch 7, Loss: 1654.111328125, average loss 16.31603690011161\n",
      "Batch 8, Loss: 1663.84912109375, average loss 16.356343688964845\n",
      "Batch 9, Loss: 1610.9249267578125, average loss 16.328888753255207\n",
      "Batch 10, Loss: 1670.6939697265625, average loss 16.36669384765625\n",
      "Total loss: 16332.883178710938\n",
      "Validation loss: 16.332883178710937\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 23, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1584.1070556640625, average loss 15.841070556640625\n",
      "Batch 2, Loss: 1657.2255859375, average loss 16.20666320800781\n",
      "Batch 3, Loss: 1749.06494140625, average loss 16.63465861002604\n",
      "Batch 4, Loss: 1570.184326171875, average loss 16.40145477294922\n",
      "Batch 5, Loss: 1740.346435546875, average loss 16.601856689453125\n",
      "Batch 6, Loss: 1652.606689453125, average loss 16.589225056966146\n",
      "Batch 7, Loss: 1661.1845703125, average loss 16.592456577845983\n",
      "Batch 8, Loss: 1610.4930419921875, average loss 16.531515808105468\n",
      "Batch 9, Loss: 1669.568115234375, average loss 16.549756401909722\n",
      "Batch 10, Loss: 1455.9404296875, average loss 16.35072119140625\n",
      "Total loss: 16346.462280273438\n",
      "Validation loss: 16.34646228027344\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 24, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1656.765625, average loss 16.56765625\n",
      "Batch 2, Loss: 1756.350830078125, average loss 17.065582275390625\n",
      "Batch 3, Loss: 1567.84716796875, average loss 16.603212076822917\n",
      "Batch 4, Loss: 1741.46826171875, average loss 16.806079711914062\n",
      "Batch 5, Loss: 1655.84521484375, average loss 16.75655419921875\n",
      "Batch 6, Loss: 1663.9783935546875, average loss 16.73709248860677\n",
      "Batch 7, Loss: 1611.6700439453125, average loss 16.648465053013393\n",
      "Batch 8, Loss: 1671.828369140625, average loss 16.6571923828125\n",
      "Batch 9, Loss: 1458.04150390625, average loss 16.426439344618057\n",
      "Batch 10, Loss: 1585.965087890625, average loss 16.369760498046876\n",
      "Total loss: 16368.467041015625\n",
      "Validation loss: 16.368467041015624\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 25, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1756.717529296875, average loss 17.56717529296875\n",
      "Batch 2, Loss: 1568.43017578125, average loss 16.625738525390624\n",
      "Batch 3, Loss: 1743.8817138671875, average loss 16.896764729817708\n",
      "Batch 4, Loss: 1653.722412109375, average loss 16.806879577636717\n",
      "Batch 5, Loss: 1663.7491455078125, average loss 16.773001953125\n",
      "Batch 6, Loss: 1613.521728515625, average loss 16.666704508463543\n",
      "Batch 7, Loss: 1670.479248046875, average loss 16.67214564732143\n",
      "Batch 8, Loss: 1457.71875, average loss 16.41027587890625\n",
      "Batch 9, Loss: 1586.234130859375, average loss 16.34939425998264\n",
      "Batch 10, Loss: 1658.163818359375, average loss 16.37261865234375\n",
      "Total loss: 16372.868286132812\n",
      "Validation loss: 16.372868286132814\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 26, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1568.540771484375, average loss 15.68540771484375\n",
      "Batch 2, Loss: 1747.275390625, average loss 16.579080810546873\n",
      "Batch 3, Loss: 1659.529052734375, average loss 16.584484049479165\n",
      "Batch 4, Loss: 1661.02490234375, average loss 16.59092529296875\n",
      "Batch 5, Loss: 1610.740234375, average loss 16.494220703125\n",
      "Batch 6, Loss: 1673.4222412109375, average loss 16.53422098795573\n",
      "Batch 7, Loss: 1463.223876953125, average loss 16.262509242466518\n",
      "Batch 8, Loss: 1583.5389404296875, average loss 16.209119262695314\n",
      "Batch 9, Loss: 1656.94384765625, average loss 16.249154730902777\n",
      "Batch 10, Loss: 1753.097412109375, average loss 16.377336669921874\n",
      "Total loss: 16339.506713867188\n",
      "Validation loss: 16.339506713867188\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 27, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1742.84716796875, average loss 17.4284716796875\n",
      "Batch 2, Loss: 1651.7490234375, average loss 16.97298095703125\n",
      "Batch 3, Loss: 1661.462646484375, average loss 16.853529459635418\n",
      "Batch 4, Loss: 1612.220947265625, average loss 16.670699462890624\n",
      "Batch 5, Loss: 1672.581298828125, average loss 16.68172216796875\n",
      "Batch 6, Loss: 1461.25537109375, average loss 16.33686075846354\n",
      "Batch 7, Loss: 1587.422607421875, average loss 16.270770089285715\n",
      "Batch 8, Loss: 1658.6199951171875, average loss 16.310198822021484\n",
      "Batch 9, Loss: 1757.890869140625, average loss 16.451166585286458\n",
      "Batch 10, Loss: 1566.7664794921875, average loss 16.37281640625\n",
      "Total loss: 16333.4814453125\n",
      "Validation loss: 16.3334814453125\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 28, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1654.9486083984375, average loss 16.549486083984377\n",
      "Batch 2, Loss: 1663.95751953125, average loss 16.594530639648436\n",
      "Batch 3, Loss: 1611.69384765625, average loss 16.435333251953125\n",
      "Batch 4, Loss: 1671.7060546875, average loss 16.505765075683595\n",
      "Batch 5, Loss: 1457.574462890625, average loss 16.119760986328124\n",
      "Batch 6, Loss: 1583.6123046875, average loss 16.07248799641927\n",
      "Batch 7, Loss: 1658.94775390625, average loss 16.146343645368304\n",
      "Batch 8, Loss: 1758.7362060546875, average loss 16.326470947265626\n",
      "Batch 9, Loss: 1566.17431640625, average loss 16.2526123046875\n",
      "Batch 10, Loss: 1741.43896484375, average loss 16.3687900390625\n",
      "Total loss: 16339.955932617188\n",
      "Validation loss: 16.339955932617187\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: \n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 29, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1662.0225830078125, average loss 16.620225830078127\n",
      "Batch 2, Loss: 1611.98046875, average loss 16.370015258789063\n",
      "Batch 3, Loss: 1672.7476806640625, average loss 16.489169108072918\n",
      "Batch 4, Loss: 1458.420166015625, average loss 16.01292724609375\n",
      "Batch 5, Loss: 1587.443359375, average loss 15.985228515625\n",
      "Batch 6, Loss: 1657.40576171875, average loss 16.08336669921875\n",
      "Batch 7, Loss: 1755.4854736328125, average loss 16.293579275948662\n",
      "Batch 8, Loss: 1566.66650390625, average loss 16.21521499633789\n",
      "Batch 9, Loss: 1740.1376953125, average loss 16.347010769314235\n",
      "Batch 10, Loss: 1653.85693359375, average loss 16.36616662597656\n",
      "Total loss: 16361.572509765625\n",
      "Validation loss: 16.361572509765626\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 30, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1612.4276123046875, average loss 16.124276123046876\n",
      "Batch 2, Loss: 1674.490966796875, average loss 16.434592895507812\n",
      "Batch 3, Loss: 1462.787353515625, average loss 15.832353108723959\n",
      "Batch 4, Loss: 1582.4937744140625, average loss 15.830499267578125\n",
      "Batch 5, Loss: 1657.516845703125, average loss 15.97943310546875\n",
      "Batch 6, Loss: 1759.5777587890625, average loss 16.24882385253906\n",
      "Batch 7, Loss: 1567.317626953125, average loss 16.166588483537947\n",
      "Batch 8, Loss: 1743.3438720703125, average loss 16.324944763183595\n",
      "Batch 9, Loss: 1653.883544921875, average loss 16.348710394965277\n",
      "Batch 10, Loss: 1664.91259765625, average loss 16.378751953125\n",
      "Total loss: 16357.161987304688\n",
      "Validation loss: 16.35716198730469\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 31, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1673.060302734375, average loss 16.73060302734375\n",
      "Batch 2, Loss: 1460.369384765625, average loss 15.6671484375\n",
      "Batch 3, Loss: 1584.766845703125, average loss 15.72732177734375\n",
      "Batch 4, Loss: 1659.038330078125, average loss 15.943087158203125\n",
      "Batch 5, Loss: 1756.6624755859375, average loss 16.267794677734376\n",
      "Batch 6, Loss: 1568.106689453125, average loss 16.17000671386719\n",
      "Batch 7, Loss: 1740.1707763671875, average loss 16.345964006696427\n",
      "Batch 8, Loss: 1653.7578125, average loss 16.369915771484376\n",
      "Batch 9, Loss: 1664.249267578125, average loss 16.400202094184028\n",
      "Batch 10, Loss: 1611.6165771484375, average loss 16.37179846191406\n",
      "Total loss: 16344.385375976562\n",
      "Validation loss: 16.34438537597656\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 32, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1461.7027587890625, average loss 14.617027587890625\n",
      "Batch 2, Loss: 1584.839111328125, average loss 15.232709350585937\n",
      "Batch 3, Loss: 1655.7783203125, average loss 15.674400634765625\n",
      "Batch 4, Loss: 1755.09228515625, average loss 16.143531188964843\n",
      "Batch 5, Loss: 1568.0731201171875, average loss 16.05097119140625\n",
      "Batch 6, Loss: 1741.7841796875, average loss 16.278782958984376\n",
      "Batch 7, Loss: 1653.8968505859375, average loss 16.315952322823662\n",
      "Batch 8, Loss: 1663.7939453125, average loss 16.356200714111328\n",
      "Batch 9, Loss: 1611.076171875, average loss 16.328929714626735\n",
      "Batch 10, Loss: 1671.176025390625, average loss 16.367212768554687\n",
      "Total loss: 16331.4853515625\n",
      "Validation loss: 16.3314853515625\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 33, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1583.4241943359375, average loss 15.834241943359375\n",
      "Batch 2, Loss: 1657.00244140625, average loss 16.202133178710938\n",
      "Batch 3, Loss: 1749.04931640625, average loss 16.631586507161458\n",
      "Batch 4, Loss: 1570.49072265625, average loss 16.399916687011718\n",
      "Batch 5, Loss: 1740.5859375, average loss 16.601105224609373\n",
      "Batch 6, Loss: 1652.103515625, average loss 16.587760213216146\n",
      "Batch 7, Loss: 1661.065185546875, average loss 16.59103044782366\n",
      "Batch 8, Loss: 1610.1756591796875, average loss 16.52987121582031\n",
      "Batch 9, Loss: 1670.0255126953125, average loss 16.548802761501737\n",
      "Batch 10, Loss: 1455.80322265625, average loss 16.349725708007814\n",
      "Total loss: 16344.052734375\n",
      "Validation loss: 16.344052734375\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 34, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1656.4935302734375, average loss 16.564935302734376\n",
      "Batch 2, Loss: 1755.977783203125, average loss 17.062356567382814\n",
      "Batch 3, Loss: 1567.82666015625, average loss 16.60099324544271\n",
      "Batch 4, Loss: 1741.5303955078125, average loss 16.804570922851564\n",
      "Batch 5, Loss: 1655.4566650390625, average loss 16.754570068359374\n",
      "Batch 6, Loss: 1663.925537109375, average loss 16.735350952148437\n",
      "Batch 7, Loss: 1611.4617919921875, average loss 16.6466748046875\n",
      "Batch 8, Loss: 1671.837890625, average loss 16.65563781738281\n",
      "Batch 9, Loss: 1457.765869140625, average loss 16.42475124782986\n",
      "Batch 10, Loss: 1585.563232421875, average loss 16.36783935546875\n",
      "Total loss: 16368.357543945312\n",
      "Validation loss: 16.36835754394531\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 35, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1756.813232421875, average loss 17.56813232421875\n",
      "Batch 2, Loss: 1568.781982421875, average loss 16.62797607421875\n",
      "Batch 3, Loss: 1743.5439453125, average loss 16.897130533854167\n",
      "Batch 4, Loss: 1653.0322265625, average loss 16.805428466796876\n",
      "Batch 5, Loss: 1663.77294921875, average loss 16.771888671875\n",
      "Batch 6, Loss: 1613.27978515625, average loss 16.66537353515625\n",
      "Batch 7, Loss: 1670.8623046875, average loss 16.671552036830356\n",
      "Batch 8, Loss: 1457.35791015625, average loss 16.409305419921875\n",
      "Batch 9, Loss: 1585.7451171875, average loss 16.34798828125\n",
      "Batch 10, Loss: 1657.975341796875, average loss 16.371164794921874\n",
      "Total loss: 16372.72314453125\n",
      "Validation loss: 16.37272314453125\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 36, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1568.572509765625, average loss 15.68572509765625\n",
      "Batch 2, Loss: 1747.6416015625, average loss 16.581070556640626\n",
      "Batch 3, Loss: 1659.3529052734375, average loss 16.585223388671874\n",
      "Batch 4, Loss: 1660.84130859375, average loss 16.59102081298828\n",
      "Batch 5, Loss: 1610.636474609375, average loss 16.494089599609374\n",
      "Batch 6, Loss: 1673.6744384765625, average loss 16.534532063802082\n",
      "Batch 7, Loss: 1463.36376953125, average loss 16.262975725446427\n",
      "Batch 8, Loss: 1582.8096923828125, average loss 16.208615875244142\n",
      "Batch 9, Loss: 1656.84619140625, average loss 16.24859876844618\n",
      "Batch 10, Loss: 1752.8231201171875, average loss 16.37656201171875\n",
      "Total loss: 16338.041625976562\n",
      "Validation loss: 16.33804162597656\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 37, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1742.788330078125, average loss 17.42788330078125\n",
      "Batch 2, Loss: 1651.281005859375, average loss 16.9703466796875\n",
      "Batch 3, Loss: 1661.14892578125, average loss 16.8507275390625\n",
      "Batch 4, Loss: 1611.904296875, average loss 16.667806396484377\n",
      "Batch 5, Loss: 1672.8760986328125, average loss 16.679997314453125\n",
      "Batch 6, Loss: 1461.24560546875, average loss 16.335407104492187\n",
      "Batch 7, Loss: 1586.75048828125, average loss 16.26856392996652\n",
      "Batch 8, Loss: 1658.15380859375, average loss 16.30768569946289\n",
      "Batch 9, Loss: 1757.3388671875, average loss 16.448319363064236\n",
      "Batch 10, Loss: 1566.7105712890625, average loss 16.370197998046876\n",
      "Total loss: 16332.138305664062\n",
      "Validation loss: 16.332138305664063\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 38, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1654.7618408203125, average loss 16.547618408203125\n",
      "Batch 2, Loss: 1663.696044921875, average loss 16.592289428710938\n",
      "Batch 3, Loss: 1611.401611328125, average loss 16.432864990234375\n",
      "Batch 4, Loss: 1671.666259765625, average loss 16.503814392089843\n",
      "Batch 5, Loss: 1457.28759765625, average loss 16.117626708984375\n",
      "Batch 6, Loss: 1583.0289306640625, average loss 16.06973714192708\n",
      "Batch 7, Loss: 1658.78955078125, average loss 16.143759765625\n",
      "Batch 8, Loss: 1758.0152587890625, average loss 16.323308868408205\n",
      "Batch 9, Loss: 1566.379638671875, average loss 16.250029703776043\n",
      "Batch 10, Loss: 1741.1317138671875, average loss 16.366158447265626\n",
      "Total loss: 16337.989379882812\n",
      "Validation loss: 16.337989379882814\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: \n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 39, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1661.65234375, average loss 16.6165234375\n",
      "Batch 2, Loss: 1611.7774658203125, average loss 16.367149047851562\n",
      "Batch 3, Loss: 1673.0386962890625, average loss 16.488228352864585\n",
      "Batch 4, Loss: 1458.364013671875, average loss 16.012081298828125\n",
      "Batch 5, Loss: 1586.75927734375, average loss 15.98318359375\n",
      "Batch 6, Loss: 1657.08251953125, average loss 16.081123860677085\n",
      "Batch 7, Loss: 1755.029296875, average loss 16.291005161830356\n",
      "Batch 8, Loss: 1566.7275390625, average loss 16.213038940429687\n",
      "Batch 9, Loss: 1740.56884765625, average loss 16.345555555555556\n",
      "Batch 10, Loss: 1653.4365234375, average loss 16.3644365234375\n",
      "Total loss: 16357.969970703125\n",
      "Validation loss: 16.357969970703124\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 40, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1612.05615234375, average loss 16.1205615234375\n",
      "Batch 2, Loss: 1674.4879150390625, average loss 16.432720336914063\n",
      "Batch 3, Loss: 1462.900634765625, average loss 15.83148234049479\n",
      "Batch 4, Loss: 1581.8131103515625, average loss 15.82814453125\n",
      "Batch 5, Loss: 1657.372802734375, average loss 15.97726123046875\n",
      "Batch 6, Loss: 1758.511962890625, average loss 16.245237630208333\n",
      "Batch 7, Loss: 1567.287353515625, average loss 16.16347133091518\n",
      "Batch 8, Loss: 1743.17822265625, average loss 16.322010192871094\n",
      "Batch 9, Loss: 1653.8446044921875, average loss 16.346058620876736\n",
      "Batch 10, Loss: 1663.9345703125, average loss 16.375387329101564\n",
      "Total loss: 16353.60302734375\n",
      "Validation loss: 16.35360302734375\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 41, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1673.023681640625, average loss 16.73023681640625\n",
      "Batch 2, Loss: 1460.3638916015625, average loss 15.666937866210937\n",
      "Batch 3, Loss: 1584.07275390625, average loss 15.724867757161459\n",
      "Batch 4, Loss: 1658.6328125, average loss 15.940232849121093\n",
      "Batch 5, Loss: 1755.40380859375, average loss 16.262993896484375\n",
      "Batch 6, Loss: 1568.1495361328125, average loss 16.16607747395833\n",
      "Batch 7, Loss: 1740.4813232421875, average loss 16.34303972516741\n",
      "Batch 8, Loss: 1653.2447509765625, average loss 16.36671569824219\n",
      "Batch 9, Loss: 1663.5286865234375, average loss 16.396556939019096\n",
      "Batch 10, Loss: 1611.065673828125, average loss 16.367966918945314\n",
      "Total loss: 16341.498291015625\n",
      "Validation loss: 16.341498291015625\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 42, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1461.30712890625, average loss 14.6130712890625\n",
      "Batch 2, Loss: 1584.37548828125, average loss 15.2284130859375\n",
      "Batch 3, Loss: 1655.6087646484375, average loss 15.670971272786458\n",
      "Batch 4, Loss: 1754.0528564453125, average loss 16.138360595703126\n",
      "Batch 5, Loss: 1568.037841796875, average loss 16.04676416015625\n",
      "Batch 6, Loss: 1741.4873046875, average loss 16.274782307942708\n",
      "Batch 7, Loss: 1653.601318359375, average loss 16.312101004464285\n",
      "Batch 8, Loss: 1663.2420654296875, average loss 16.352140960693358\n",
      "Batch 9, Loss: 1610.739990234375, average loss 16.324947509765625\n",
      "Batch 10, Loss: 1671.170166015625, average loss 16.363622924804687\n",
      "Total loss: 16329.666015625\n",
      "Validation loss: 16.329666015625\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 43, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1582.9852294921875, average loss 15.829852294921874\n",
      "Batch 2, Loss: 1656.8388671875, average loss 16.19912048339844\n",
      "Batch 3, Loss: 1748.50341796875, average loss 16.62775838216146\n",
      "Batch 4, Loss: 1570.565185546875, average loss 16.39723175048828\n",
      "Batch 5, Loss: 1740.723388671875, average loss 16.599232177734375\n",
      "Batch 6, Loss: 1651.5712890625, average loss 16.58531229654948\n",
      "Batch 7, Loss: 1660.889404296875, average loss 16.588681117466518\n",
      "Batch 8, Loss: 1609.84423828125, average loss 16.527401275634766\n",
      "Batch 9, Loss: 1670.183349609375, average loss 16.546782633463543\n",
      "Batch 10, Loss: 1455.645751953125, average loss 16.347750122070313\n",
      "Total loss: 16342.384155273438\n",
      "Validation loss: 16.342384155273436\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 44, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1656.365234375, average loss 16.56365234375\n",
      "Batch 2, Loss: 1755.5111083984375, average loss 17.059381713867186\n",
      "Batch 3, Loss: 1567.7623291015625, average loss 16.598795572916668\n",
      "Batch 4, Loss: 1741.3929443359375, average loss 16.802579040527345\n",
      "Batch 5, Loss: 1655.111083984375, average loss 16.752285400390626\n",
      "Batch 6, Loss: 1663.365234375, average loss 16.732513224283853\n",
      "Batch 7, Loss: 1611.189208984375, average loss 16.64385306222098\n",
      "Batch 8, Loss: 1671.95751953125, average loss 16.653318328857424\n",
      "Batch 9, Loss: 1457.8507080078125, average loss 16.42278374565972\n",
      "Batch 10, Loss: 1585.0205078125, average loss 16.36552587890625\n",
      "Total loss: 16364.903442382812\n",
      "Validation loss: 16.36490344238281\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 45, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1756.0888671875, average loss 17.560888671875\n",
      "Batch 2, Loss: 1568.8065185546875, average loss 16.624476928710937\n",
      "Batch 3, Loss: 1743.503662109375, average loss 16.894663492838543\n",
      "Batch 4, Loss: 1652.5924072265625, average loss 16.802478637695312\n",
      "Batch 5, Loss: 1663.2437744140625, average loss 16.768470458984375\n",
      "Batch 6, Loss: 1612.86279296875, average loss 16.661830037434896\n",
      "Batch 7, Loss: 1670.953125, average loss 16.668644496372767\n",
      "Batch 8, Loss: 1457.298095703125, average loss 16.40668655395508\n",
      "Batch 9, Loss: 1585.168701171875, average loss 16.34501993815104\n",
      "Batch 10, Loss: 1657.57666015625, average loss 16.36809460449219\n",
      "Total loss: 16368.354614257812\n",
      "Validation loss: 16.36835461425781\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 46, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1568.3900146484375, average loss 15.683900146484374\n",
      "Batch 2, Loss: 1747.452880859375, average loss 16.57921447753906\n",
      "Batch 3, Loss: 1659.15478515625, average loss 16.58332560221354\n",
      "Batch 4, Loss: 1660.3447265625, average loss 16.588356018066406\n",
      "Batch 5, Loss: 1610.27001953125, average loss 16.491224853515625\n",
      "Batch 6, Loss: 1673.405029296875, average loss 16.531695760091146\n",
      "Batch 7, Loss: 1463.14892578125, average loss 16.260237688337053\n",
      "Batch 8, Loss: 1582.260986328125, average loss 16.205534210205077\n",
      "Batch 9, Loss: 1656.673095703125, average loss 16.24566718207465\n",
      "Batch 10, Loss: 1751.62744140625, average loss 16.372727905273436\n",
      "Total loss: 16335.214599609375\n",
      "Validation loss: 16.335214599609376\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 47, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1742.5712890625, average loss 17.425712890625\n",
      "Batch 2, Loss: 1650.8814697265625, average loss 16.967263793945314\n",
      "Batch 3, Loss: 1660.6136474609375, average loss 16.846888020833333\n",
      "Batch 4, Loss: 1611.3990478515625, average loss 16.663663635253908\n",
      "Batch 5, Loss: 1672.6937255859375, average loss 16.676318359375\n",
      "Batch 6, Loss: 1460.98876953125, average loss 16.331913248697916\n",
      "Batch 7, Loss: 1586.2025146484375, average loss 16.264786376953126\n",
      "Batch 8, Loss: 1657.7879638671875, average loss 16.303923034667967\n",
      "Batch 9, Loss: 1756.244873046875, average loss 16.44375922309028\n",
      "Batch 10, Loss: 1566.711181640625, average loss 16.366094482421875\n",
      "Total loss: 16330.663818359375\n",
      "Validation loss: 16.330663818359376\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 48, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1654.45849609375, average loss 16.5445849609375\n",
      "Batch 2, Loss: 1663.032958984375, average loss 16.587457275390626\n",
      "Batch 3, Loss: 1611.01171875, average loss 16.42834391276042\n",
      "Batch 4, Loss: 1671.5684814453125, average loss 16.500179138183594\n",
      "Batch 5, Loss: 1457.1553955078125, average loss 16.1144541015625\n",
      "Batch 6, Loss: 1582.411376953125, average loss 16.066064046223957\n",
      "Batch 7, Loss: 1658.49951171875, average loss 16.140197056361608\n",
      "Batch 8, Loss: 1757.074462890625, average loss 16.319015502929688\n",
      "Batch 9, Loss: 1566.3338623046875, average loss 16.24616251627604\n",
      "Batch 10, Loss: 1741.1875, average loss 16.36273376464844\n",
      "Total loss: 16335.282958984375\n",
      "Validation loss: 16.335282958984376\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: \n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 49, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1661.2264404296875, average loss 16.612264404296877\n",
      "Batch 2, Loss: 1611.3953857421875, average loss 16.363109130859375\n",
      "Batch 3, Loss: 1673.024658203125, average loss 16.48548828125\n",
      "Batch 4, Loss: 1458.1710205078125, average loss 16.00954376220703\n",
      "Batch 5, Loss: 1586.24267578125, average loss 15.980120361328124\n",
      "Batch 6, Loss: 1656.7265625, average loss 16.07797790527344\n",
      "Batch 7, Loss: 1754.101318359375, average loss 16.286982945033483\n",
      "Batch 8, Loss: 1566.773193359375, average loss 16.209576568603516\n",
      "Batch 9, Loss: 1740.6962890625, average loss 16.34261949327257\n",
      "Batch 10, Loss: 1652.890869140625, average loss 16.36124841308594\n",
      "Total loss: 16353.580932617188\n",
      "Validation loss: 16.353580932617188\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 50, lr=[0.005]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1611.5869140625, average loss 16.115869140625\n",
      "Batch 2, Loss: 1674.1810302734375, average loss 16.428839721679687\n",
      "Batch 3, Loss: 1462.506103515625, average loss 15.827580159505208\n",
      "Batch 4, Loss: 1581.401123046875, average loss 15.824187927246093\n",
      "Batch 5, Loss: 1657.13330078125, average loss 15.973616943359374\n",
      "Batch 6, Loss: 1757.5469970703125, average loss 16.240592447916665\n",
      "Batch 7, Loss: 1567.115966796875, average loss 16.15924490792411\n",
      "Batch 8, Loss: 1742.890625, average loss 16.317952575683595\n",
      "Batch 9, Loss: 1653.7171630859375, average loss 16.342310248480903\n",
      "Batch 10, Loss: 1663.3033447265625, average loss 16.371382568359376\n",
      "Total loss: 16350.011840820312\n",
      "Validation loss: 16.35001184082031\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 51, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1672.914306640625, average loss 16.72914306640625\n",
      "Batch 2, Loss: 1460.799072265625, average loss 15.66856689453125\n",
      "Batch 3, Loss: 1582.44482421875, average loss 15.72052734375\n",
      "Batch 4, Loss: 1656.51611328125, average loss 15.931685791015624\n",
      "Batch 5, Loss: 1745.985107421875, average loss 16.23731884765625\n",
      "Batch 6, Loss: 1567.423095703125, average loss 16.143470865885416\n",
      "Batch 7, Loss: 1739.9468994140625, average loss 16.322899169921875\n",
      "Batch 8, Loss: 1654.3004150390625, average loss 16.35041229248047\n",
      "Batch 9, Loss: 1656.1539306640625, average loss 16.373870849609375\n",
      "Batch 10, Loss: 1609.712158203125, average loss 16.346195922851564\n",
      "Total loss: 16325.446899414062\n",
      "Validation loss: 16.32544689941406\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 52, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1454.7056884765625, average loss 14.547056884765626\n",
      "Batch 2, Loss: 1582.661376953125, average loss 15.186835327148437\n",
      "Batch 3, Loss: 1655.1669921875, average loss 15.641780192057292\n",
      "Batch 4, Loss: 1742.9088134765625, average loss 16.088607177734374\n",
      "Batch 5, Loss: 1569.11328125, average loss 16.0091123046875\n",
      "Batch 6, Loss: 1740.02392578125, average loss 16.240966796875\n",
      "Batch 7, Loss: 1651.289794921875, average loss 16.279814104352678\n",
      "Batch 8, Loss: 1659.76171875, average loss 16.319539489746095\n",
      "Batch 9, Loss: 1608.37255859375, average loss 16.293337944878473\n",
      "Batch 10, Loss: 1670.16748046875, average loss 16.334171630859377\n",
      "Total loss: 16326.146118164062\n",
      "Validation loss: 16.32614611816406\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 53, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1582.4456787109375, average loss 15.824456787109375\n",
      "Batch 2, Loss: 1654.9373779296875, average loss 16.186915283203124\n",
      "Batch 3, Loss: 1744.2279052734375, average loss 16.605369873046875\n",
      "Batch 4, Loss: 1569.00830078125, average loss 16.37654815673828\n",
      "Batch 5, Loss: 1739.491455078125, average loss 16.580221435546875\n",
      "Batch 6, Loss: 1651.12158203125, average loss 16.56872049967448\n",
      "Batch 7, Loss: 1657.7286376953125, average loss 16.569944196428573\n",
      "Batch 8, Loss: 1608.941162109375, average loss 16.50987762451172\n",
      "Batch 9, Loss: 1669.21240234375, average loss 16.530127224392363\n",
      "Batch 10, Loss: 1456.681640625, average loss 16.333796142578127\n",
      "Total loss: 16326.550903320312\n",
      "Validation loss: 16.326550903320314\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 54, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1655.4307861328125, average loss 16.554307861328127\n",
      "Batch 2, Loss: 1746.3677978515625, average loss 17.008992919921877\n",
      "Batch 3, Loss: 1567.777587890625, average loss 16.56525390625\n",
      "Batch 4, Loss: 1739.7667236328125, average loss 16.77335723876953\n",
      "Batch 5, Loss: 1653.59521484375, average loss 16.725876220703125\n",
      "Batch 6, Loss: 1657.0704345703125, average loss 16.700014241536458\n",
      "Batch 7, Loss: 1609.133056640625, average loss 16.61305943080357\n",
      "Batch 8, Loss: 1670.180908203125, average loss 16.62415313720703\n",
      "Batch 9, Loss: 1455.7353515625, average loss 16.394508734809026\n",
      "Batch 10, Loss: 1583.04638671875, average loss 16.338104248046875\n",
      "Total loss: 16325.311157226562\n",
      "Validation loss: 16.32531115722656\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 55, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1743.2021484375, average loss 17.432021484375\n",
      "Batch 2, Loss: 1567.9716796875, average loss 16.555869140625\n",
      "Batch 3, Loss: 1740.34814453125, average loss 16.838406575520832\n",
      "Batch 4, Loss: 1651.751953125, average loss 16.758184814453124\n",
      "Batch 5, Loss: 1658.015380859375, average loss 16.72257861328125\n",
      "Batch 6, Loss: 1610.0035400390625, average loss 16.618821411132814\n",
      "Batch 7, Loss: 1668.5325927734375, average loss 16.628322056361608\n",
      "Batch 8, Loss: 1455.1724853515625, average loss 16.36874740600586\n",
      "Batch 9, Loss: 1581.62158203125, average loss 16.307355007595486\n",
      "Batch 10, Loss: 1657.02685546875, average loss 16.333646362304687\n",
      "Total loss: 16333.232055664062\n",
      "Validation loss: 16.333232055664062\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 56, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1566.831298828125, average loss 15.66831298828125\n",
      "Batch 2, Loss: 1742.5025634765625, average loss 16.546669311523438\n",
      "Batch 3, Loss: 1655.044677734375, average loss 16.547928466796876\n",
      "Batch 4, Loss: 1657.10693359375, average loss 16.55371368408203\n",
      "Batch 5, Loss: 1609.430908203125, average loss 16.461832763671875\n",
      "Batch 6, Loss: 1668.952880859375, average loss 16.499782104492187\n",
      "Batch 7, Loss: 1455.4541015625, average loss 16.221890520368305\n",
      "Batch 8, Loss: 1581.9986572265625, average loss 16.171652526855468\n",
      "Batch 9, Loss: 1654.2615966796875, average loss 16.21287068684896\n",
      "Batch 10, Loss: 1743.2374267578125, average loss 16.334821044921874\n",
      "Total loss: 16325.98193359375\n",
      "Validation loss: 16.32598193359375\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 57, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1739.640869140625, average loss 17.39640869140625\n",
      "Batch 2, Loss: 1650.4859619140625, average loss 16.950634155273438\n",
      "Batch 3, Loss: 1659.9481201171875, average loss 16.83358317057292\n",
      "Batch 4, Loss: 1609.3753662109375, average loss 16.648625793457033\n",
      "Batch 5, Loss: 1670.87451171875, average loss 16.660649658203123\n",
      "Batch 6, Loss: 1458.4049072265625, average loss 16.314549560546876\n",
      "Batch 7, Loss: 1583.591552734375, average loss 16.246173270089287\n",
      "Batch 8, Loss: 1656.149169921875, average loss 16.28558807373047\n",
      "Batch 9, Loss: 1748.592041015625, average loss 16.418958333333332\n",
      "Batch 10, Loss: 1569.002197265625, average loss 16.346064697265625\n",
      "Total loss: 16328.902587890625\n",
      "Validation loss: 16.328902587890624\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 58, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1652.1737060546875, average loss 16.521737060546876\n",
      "Batch 2, Loss: 1655.8543701171875, average loss 16.540140380859373\n",
      "Batch 3, Loss: 1608.6795654296875, average loss 16.38902547200521\n",
      "Batch 4, Loss: 1668.680419921875, average loss 16.463470153808593\n",
      "Batch 5, Loss: 1455.864990234375, average loss 16.082506103515627\n",
      "Batch 6, Loss: 1581.0289306640625, average loss 16.03713663736979\n",
      "Batch 7, Loss: 1656.361328125, average loss 16.112347586495535\n",
      "Batch 8, Loss: 1744.706787109375, average loss 16.279187622070314\n",
      "Batch 9, Loss: 1565.985595703125, average loss 16.21037299262153\n",
      "Batch 10, Loss: 1742.6993408203125, average loss 16.332035034179686\n",
      "Total loss: 16328.574951171875\n",
      "Validation loss: 16.328574951171873\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: \n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 59, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1659.104248046875, average loss 16.59104248046875\n",
      "Batch 2, Loss: 1609.5223388671875, average loss 16.343132934570313\n",
      "Batch 3, Loss: 1669.407958984375, average loss 16.460115152994792\n",
      "Batch 4, Loss: 1452.8759765625, average loss 15.977276306152344\n",
      "Batch 5, Loss: 1583.307861328125, average loss 15.948436767578125\n",
      "Batch 6, Loss: 1655.24560546875, average loss 16.049106648763022\n",
      "Batch 7, Loss: 1748.2520751953125, average loss 16.253880092075892\n",
      "Batch 8, Loss: 1565.489990234375, average loss 16.179007568359374\n",
      "Batch 9, Loss: 1738.28857421875, average loss 16.312771809895832\n",
      "Batch 10, Loss: 1654.025146484375, average loss 16.335519775390626\n",
      "Total loss: 16331.226318359375\n",
      "Validation loss: 16.331226318359374\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 60, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1609.90087890625, average loss 16.0990087890625\n",
      "Batch 2, Loss: 1671.307861328125, average loss 16.406043701171875\n",
      "Batch 3, Loss: 1459.686279296875, average loss 15.8029833984375\n",
      "Batch 4, Loss: 1581.2337646484375, average loss 15.805321960449218\n",
      "Batch 5, Loss: 1655.178955078125, average loss 15.954615478515626\n",
      "Batch 6, Loss: 1744.15625, average loss 16.202439982096354\n",
      "Batch 7, Loss: 1568.326171875, average loss 16.12827165876116\n",
      "Batch 8, Loss: 1741.0985107421875, average loss 16.28861083984375\n",
      "Batch 9, Loss: 1651.239990234375, average loss 16.31347629123264\n",
      "Batch 10, Loss: 1657.991943359375, average loss 16.34012060546875\n",
      "Total loss: 16324.213500976562\n",
      "Validation loss: 16.324213500976562\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 61, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1668.3851318359375, average loss 16.683851318359373\n",
      "Batch 2, Loss: 1454.18603515625, average loss 15.612855834960937\n",
      "Batch 3, Loss: 1581.661376953125, average loss 15.680775146484375\n",
      "Batch 4, Loss: 1656.4228515625, average loss 15.901638488769532\n",
      "Batch 5, Loss: 1746.46826171875, average loss 16.214247314453125\n",
      "Batch 6, Loss: 1567.0869140625, average loss 16.123684285481772\n",
      "Batch 7, Loss: 1739.91748046875, average loss 16.305897216796875\n",
      "Batch 8, Loss: 1652.4398193359375, average loss 16.333209838867187\n",
      "Batch 9, Loss: 1658.9453125, average loss 16.361681315104168\n",
      "Batch 10, Loss: 1610.1221923828125, average loss 16.335635375976562\n",
      "Total loss: 16328.632202148438\n",
      "Validation loss: 16.328632202148437\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 62, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1456.833251953125, average loss 14.56833251953125\n",
      "Batch 2, Loss: 1582.28564453125, average loss 15.195594482421875\n",
      "Batch 3, Loss: 1655.0198974609375, average loss 15.647129313151042\n",
      "Batch 4, Loss: 1745.13427734375, average loss 16.098182678222656\n",
      "Batch 5, Loss: 1567.552978515625, average loss 16.013652099609374\n",
      "Batch 6, Loss: 1739.361083984375, average loss 16.243645222981772\n",
      "Batch 7, Loss: 1652.7791748046875, average loss 16.28423758370536\n",
      "Batch 8, Loss: 1657.7545166015625, average loss 16.32090103149414\n",
      "Batch 9, Loss: 1608.0537109375, average loss 16.294193929036457\n",
      "Batch 10, Loss: 1670.4188232421875, average loss 16.335193359375\n",
      "Total loss: 16325.9951171875\n",
      "Validation loss: 16.3259951171875\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 63, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1582.2252197265625, average loss 15.822252197265625\n",
      "Batch 2, Loss: 1654.96484375, average loss 16.18595031738281\n",
      "Batch 3, Loss: 1741.89306640625, average loss 16.59694376627604\n",
      "Batch 4, Loss: 1570.8046875, average loss 16.374719543457033\n",
      "Batch 5, Loss: 1739.9442138671875, average loss 16.5796640625\n",
      "Batch 6, Loss: 1649.872802734375, average loss 16.56617472330729\n",
      "Batch 7, Loss: 1658.1484375, average loss 16.56836181640625\n",
      "Batch 8, Loss: 1608.7479248046875, average loss 16.50825149536133\n",
      "Batch 9, Loss: 1668.9287109375, average loss 16.52836656358507\n",
      "Batch 10, Loss: 1455.414794921875, average loss 16.330944702148436\n",
      "Total loss: 16328.878051757812\n",
      "Validation loss: 16.328878051757812\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 64, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1655.6085205078125, average loss 16.556085205078126\n",
      "Batch 2, Loss: 1748.921630859375, average loss 17.022650756835937\n",
      "Batch 3, Loss: 1567.2698974609375, average loss 16.57266682942708\n",
      "Batch 4, Loss: 1740.3421630859375, average loss 16.780355529785155\n",
      "Batch 5, Loss: 1654.023193359375, average loss 16.732330810546873\n",
      "Batch 6, Loss: 1657.2886962890625, average loss 16.7057568359375\n",
      "Batch 7, Loss: 1609.2518310546875, average loss 16.61815133231027\n",
      "Batch 8, Loss: 1670.88134765625, average loss 16.6294841003418\n",
      "Batch 9, Loss: 1456.833740234375, average loss 16.400467800564236\n",
      "Batch 10, Loss: 1582.4307861328125, average loss 16.342851806640624\n",
      "Total loss: 16323.677856445312\n",
      "Validation loss: 16.32367785644531\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 65, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1742.48193359375, average loss 17.4248193359375\n",
      "Batch 2, Loss: 1568.009521484375, average loss 16.552457275390626\n",
      "Batch 3, Loss: 1740.2694091796875, average loss 16.83586954752604\n",
      "Batch 4, Loss: 1651.769287109375, average loss 16.75632537841797\n",
      "Batch 5, Loss: 1657.3992919921875, average loss 16.71985888671875\n",
      "Batch 6, Loss: 1609.8408203125, average loss 16.61628377278646\n",
      "Batch 7, Loss: 1668.382080078125, average loss 16.625931919642856\n",
      "Batch 8, Loss: 1454.840087890625, average loss 16.36624053955078\n",
      "Batch 9, Loss: 1581.3563232421875, average loss 16.30483194986979\n",
      "Batch 10, Loss: 1656.95263671875, average loss 16.331301391601563\n",
      "Total loss: 16331.98486328125\n",
      "Validation loss: 16.33198486328125\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 66, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1566.988525390625, average loss 15.66988525390625\n",
      "Batch 2, Loss: 1742.592529296875, average loss 16.5479052734375\n",
      "Batch 3, Loss: 1654.360107421875, average loss 16.546470540364584\n",
      "Batch 4, Loss: 1657.1319580078125, average loss 16.55268280029297\n",
      "Batch 5, Loss: 1609.1661376953125, average loss 16.460478515625\n",
      "Batch 6, Loss: 1669.1640625, average loss 16.499005533854167\n",
      "Batch 7, Loss: 1455.7242431640625, average loss 16.221610804966517\n",
      "Batch 8, Loss: 1581.628662109375, average loss 16.170945281982423\n",
      "Batch 9, Loss: 1654.16552734375, average loss 16.212135281032985\n",
      "Batch 10, Loss: 1743.57421875, average loss 16.334495971679686\n",
      "Total loss: 16324.07763671875\n",
      "Validation loss: 16.32407763671875\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 67, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1739.6337890625, average loss 17.396337890625\n",
      "Batch 2, Loss: 1650.5579833984375, average loss 16.950958862304688\n",
      "Batch 3, Loss: 1659.40087890625, average loss 16.831975504557292\n",
      "Batch 4, Loss: 1609.244873046875, average loss 16.647093811035155\n",
      "Batch 5, Loss: 1670.8359375, average loss 16.659346923828124\n",
      "Batch 6, Loss: 1458.255126953125, average loss 16.313214314778644\n",
      "Batch 7, Loss: 1583.2127685546875, average loss 16.24448765345982\n",
      "Batch 8, Loss: 1656.1688232421875, average loss 16.284137725830078\n",
      "Batch 9, Loss: 1748.568603515625, average loss 16.417643093532988\n",
      "Batch 10, Loss: 1569.0235595703125, average loss 16.34490234375\n",
      "Total loss: 16326.808715820312\n",
      "Validation loss: 16.326808715820313\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 68, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1651.7923583984375, average loss 16.517923583984373\n",
      "Batch 2, Loss: 1655.930908203125, average loss 16.538616333007813\n",
      "Batch 3, Loss: 1608.5648193359375, average loss 16.387626953125\n",
      "Batch 4, Loss: 1668.88330078125, average loss 16.462928466796875\n",
      "Batch 5, Loss: 1456.0693359375, average loss 16.0824814453125\n",
      "Batch 6, Loss: 1580.6298828125, average loss 16.036451009114582\n",
      "Batch 7, Loss: 1656.190185546875, average loss 16.11151541573661\n",
      "Batch 8, Loss: 1745.2041015625, average loss 16.279081115722658\n",
      "Batch 9, Loss: 1565.704833984375, average loss 16.209966362847222\n",
      "Batch 10, Loss: 1742.7904052734375, average loss 16.33176013183594\n",
      "Total loss: 16327.202392578125\n",
      "Validation loss: 16.327202392578126\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: \n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 69, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1658.8133544921875, average loss 16.588133544921874\n",
      "Batch 2, Loss: 1609.4698486328125, average loss 16.341416015625\n",
      "Batch 3, Loss: 1669.478515625, average loss 16.459205729166666\n",
      "Batch 4, Loss: 1452.861572265625, average loss 15.976558227539062\n",
      "Batch 5, Loss: 1582.90576171875, average loss 15.94705810546875\n",
      "Batch 6, Loss: 1655.1654052734375, average loss 16.047824096679687\n",
      "Batch 7, Loss: 1747.8492431640625, average loss 16.252205287388392\n",
      "Batch 8, Loss: 1565.636474609375, average loss 16.17772521972656\n",
      "Batch 9, Loss: 1738.20556640625, average loss 16.311539713541666\n",
      "Batch 10, Loss: 1653.709716796875, average loss 16.334095458984375\n",
      "Total loss: 16331.279907226562\n",
      "Validation loss: 16.331279907226563\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 70, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1609.943359375, average loss 16.09943359375\n",
      "Batch 2, Loss: 1671.490966796875, average loss 16.407171630859374\n",
      "Batch 3, Loss: 1459.683837890625, average loss 15.803727213541666\n",
      "Batch 4, Loss: 1580.8564453125, average loss 15.8049365234375\n",
      "Batch 5, Loss: 1654.92333984375, average loss 15.9537958984375\n",
      "Batch 6, Loss: 1744.967041015625, average loss 16.203108317057293\n",
      "Batch 7, Loss: 1568.1419677734375, average loss 16.12858136858259\n",
      "Batch 8, Loss: 1741.433349609375, average loss 16.289300384521486\n",
      "Batch 9, Loss: 1651.2257080078125, average loss 16.314073350694443\n",
      "Batch 10, Loss: 1657.875, average loss 16.340541015625\n",
      "Total loss: 16323.007568359375\n",
      "Validation loss: 16.323007568359376\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 71, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1668.474365234375, average loss 16.68474365234375\n",
      "Batch 2, Loss: 1454.464599609375, average loss 15.61469482421875\n",
      "Batch 3, Loss: 1581.09619140625, average loss 15.6801171875\n",
      "Batch 4, Loss: 1656.3485107421875, average loss 15.900959167480469\n",
      "Batch 5, Loss: 1745.9830322265625, average loss 16.2127333984375\n",
      "Batch 6, Loss: 1567.048828125, average loss 16.122359212239584\n",
      "Batch 7, Loss: 1739.877685546875, average loss 16.30470458984375\n",
      "Batch 8, Loss: 1652.422119140625, average loss 16.33214416503906\n",
      "Batch 9, Loss: 1658.8333740234375, average loss 16.360609673394098\n",
      "Batch 10, Loss: 1610.0523681640625, average loss 16.33460107421875\n",
      "Total loss: 16326.609741210938\n",
      "Validation loss: 16.326609741210937\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 72, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1456.4105224609375, average loss 14.564105224609374\n",
      "Batch 2, Loss: 1582.053955078125, average loss 15.192322387695313\n",
      "Batch 3, Loss: 1654.80810546875, average loss 15.644241943359376\n",
      "Batch 4, Loss: 1745.70654296875, average loss 16.097447814941408\n",
      "Batch 5, Loss: 1567.55615234375, average loss 16.013070556640624\n",
      "Batch 6, Loss: 1739.4722900390625, average loss 16.243345947265624\n",
      "Batch 7, Loss: 1652.49267578125, average loss 16.28357177734375\n",
      "Batch 8, Loss: 1657.8875732421875, average loss 16.320484771728516\n",
      "Batch 9, Loss: 1608.0301513671875, average loss 16.293797743055556\n",
      "Batch 10, Loss: 1670.5986328125, average loss 16.3350166015625\n",
      "Total loss: 16325.209838867188\n",
      "Validation loss: 16.32520983886719\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 73, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1581.781982421875, average loss 15.81781982421875\n",
      "Batch 2, Loss: 1654.922119140625, average loss 16.1835205078125\n",
      "Batch 3, Loss: 1741.844970703125, average loss 16.59516357421875\n",
      "Batch 4, Loss: 1570.67724609375, average loss 16.373065795898437\n",
      "Batch 5, Loss: 1739.9871826171875, average loss 16.578427001953123\n",
      "Batch 6, Loss: 1649.83251953125, average loss 16.565076700846355\n",
      "Batch 7, Loss: 1657.7464599609375, average loss 16.566846400669643\n",
      "Batch 8, Loss: 1608.7041015625, average loss 16.506870727539063\n",
      "Batch 9, Loss: 1668.849853515625, average loss 16.527051595052082\n",
      "Batch 10, Loss: 1455.100830078125, average loss 16.329447265625\n",
      "Total loss: 16328.053100585938\n",
      "Validation loss: 16.328053100585937\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 74, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1655.557373046875, average loss 16.55557373046875\n",
      "Batch 2, Loss: 1748.88720703125, average loss 17.022222900390624\n",
      "Batch 3, Loss: 1567.2149658203125, average loss 16.572198486328126\n",
      "Batch 4, Loss: 1740.26611328125, average loss 16.779814147949217\n",
      "Batch 5, Loss: 1653.708984375, average loss 16.731269287109374\n",
      "Batch 6, Loss: 1657.5198974609375, average loss 16.705257568359375\n",
      "Batch 7, Loss: 1609.269287109375, average loss 16.617748325892858\n",
      "Batch 8, Loss: 1671.0751953125, average loss 16.629373779296873\n",
      "Batch 9, Loss: 1456.8507080078125, average loss 16.400388590494792\n",
      "Batch 10, Loss: 1582.071044921875, average loss 16.342420776367188\n",
      "Total loss: 16322.65478515625\n",
      "Validation loss: 16.32265478515625\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 75, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1742.992431640625, average loss 17.42992431640625\n",
      "Batch 2, Loss: 1567.7750244140625, average loss 16.553837280273438\n",
      "Batch 3, Loss: 1740.406005859375, average loss 16.837244873046874\n",
      "Batch 4, Loss: 1651.7327880859375, average loss 16.757265625\n",
      "Batch 5, Loss: 1657.0921630859375, average loss 16.719996826171876\n",
      "Batch 6, Loss: 1609.7869873046875, average loss 16.616309000651043\n",
      "Batch 7, Loss: 1668.382080078125, average loss 16.625953543526787\n",
      "Batch 8, Loss: 1455.0147705078125, average loss 16.366477813720703\n",
      "Batch 9, Loss: 1580.8438720703125, average loss 16.304473470052084\n",
      "Batch 10, Loss: 1656.7781982421875, average loss 16.33080432128906\n",
      "Total loss: 16329.978881835938\n",
      "Validation loss: 16.329978881835938\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 76, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1566.953857421875, average loss 15.66953857421875\n",
      "Batch 2, Loss: 1742.58935546875, average loss 16.547716064453127\n",
      "Batch 3, Loss: 1654.1429443359375, average loss 16.54562052408854\n",
      "Batch 4, Loss: 1657.02734375, average loss 16.551783752441406\n",
      "Batch 5, Loss: 1609.0645751953125, average loss 16.45955615234375\n",
      "Batch 6, Loss: 1669.2042236328125, average loss 16.498303833007814\n",
      "Batch 7, Loss: 1455.60791015625, average loss 16.220843157087053\n",
      "Batch 8, Loss: 1581.3033447265625, average loss 16.169866943359374\n",
      "Batch 9, Loss: 1653.9664306640625, average loss 16.210955539279514\n",
      "Batch 10, Loss: 1743.7294921875, average loss 16.333589477539064\n",
      "Total loss: 16322.269775390625\n",
      "Validation loss: 16.322269775390627\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 77, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1739.6253662109375, average loss 17.396253662109373\n",
      "Batch 2, Loss: 1650.41552734375, average loss 16.95020446777344\n",
      "Batch 3, Loss: 1659.18994140625, average loss 16.83076944986979\n",
      "Batch 4, Loss: 1609.193603515625, average loss 16.646061096191406\n",
      "Batch 5, Loss: 1670.790771484375, average loss 16.658430419921874\n",
      "Batch 6, Loss: 1458.1357421875, average loss 16.312251586914062\n",
      "Batch 7, Loss: 1582.880859375, average loss 16.24318830217634\n",
      "Batch 8, Loss: 1656.170654296875, average loss 16.28300308227539\n",
      "Batch 9, Loss: 1748.8741455078125, average loss 16.416974012586806\n",
      "Batch 10, Loss: 1568.9708251953125, average loss 16.34424743652344\n",
      "Total loss: 16324.414428710938\n",
      "Validation loss: 16.324414428710938\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 78, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1651.449462890625, average loss 16.51449462890625\n",
      "Batch 2, Loss: 1655.939453125, average loss 16.536944580078124\n",
      "Batch 3, Loss: 1608.449462890625, average loss 16.3861279296875\n",
      "Batch 4, Loss: 1669.0380859375, average loss 16.462191162109374\n",
      "Batch 5, Loss: 1456.263916015625, average loss 16.08228076171875\n",
      "Batch 6, Loss: 1580.217041015625, average loss 16.035595703125\n",
      "Batch 7, Loss: 1655.91552734375, average loss 16.11038992745536\n",
      "Batch 8, Loss: 1745.7041015625, average loss 16.278721313476563\n",
      "Batch 9, Loss: 1565.33984375, average loss 16.20924099392361\n",
      "Batch 10, Loss: 1742.7801513671875, average loss 16.33109704589844\n",
      "Total loss: 16324.72705078125\n",
      "Validation loss: 16.32472705078125\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: \n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 79, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1658.39599609375, average loss 16.5839599609375\n",
      "Batch 2, Loss: 1609.364013671875, average loss 16.338800048828126\n",
      "Batch 3, Loss: 1669.37353515625, average loss 16.45711181640625\n",
      "Batch 4, Loss: 1452.761474609375, average loss 15.974737548828125\n",
      "Batch 5, Loss: 1582.348876953125, average loss 15.94448779296875\n",
      "Batch 6, Loss: 1654.92724609375, average loss 16.045285237630207\n",
      "Batch 7, Loss: 1747.2005615234375, average loss 16.249102434430803\n",
      "Batch 8, Loss: 1565.62939453125, average loss 16.175001373291014\n",
      "Batch 9, Loss: 1738.031982421875, average loss 16.30892564561632\n",
      "Batch 10, Loss: 1653.2457275390625, average loss 16.33127880859375\n",
      "Total loss: 16329.930541992188\n",
      "Validation loss: 16.329930541992187\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 80, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1609.8424072265625, average loss 16.098424072265626\n",
      "Batch 2, Loss: 1671.4390869140625, average loss 16.406407470703126\n",
      "Batch 3, Loss: 1459.2901611328125, average loss 15.801905517578126\n",
      "Batch 4, Loss: 1580.3486328125, average loss 15.802300720214843\n",
      "Batch 5, Loss: 1654.4122314453125, average loss 15.9506650390625\n",
      "Batch 6, Loss: 1745.966064453125, average loss 16.202164306640626\n",
      "Batch 7, Loss: 1567.7530517578125, average loss 16.127216622488838\n",
      "Batch 8, Loss: 1741.467041015625, average loss 16.288148345947267\n",
      "Batch 9, Loss: 1650.791015625, average loss 16.312566324869792\n",
      "Batch 10, Loss: 1657.6982421875, average loss 16.33900793457031\n",
      "Total loss: 16319.349609375\n",
      "Validation loss: 16.319349609375\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 81, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1668.364501953125, average loss 16.68364501953125\n",
      "Batch 2, Loss: 1454.725341796875, average loss 15.61544921875\n",
      "Batch 3, Loss: 1580.2105712890625, average loss 15.677668050130208\n",
      "Batch 4, Loss: 1655.8780517578125, average loss 15.897946166992188\n",
      "Batch 5, Loss: 1745.419189453125, average loss 16.2091953125\n",
      "Batch 6, Loss: 1566.525390625, average loss 16.118538411458335\n",
      "Batch 7, Loss: 1739.543212890625, average loss 16.300951799665178\n",
      "Batch 8, Loss: 1651.9620361328125, average loss 16.328285369873047\n",
      "Batch 9, Loss: 1658.201171875, average loss 16.35647718641493\n",
      "Batch 10, Loss: 1609.498291015625, average loss 16.330327758789064\n",
      "Total loss: 16318.896484375\n",
      "Validation loss: 16.318896484375\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 82, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1455.37939453125, average loss 14.5537939453125\n",
      "Batch 2, Loss: 1581.333740234375, average loss 15.183565673828125\n",
      "Batch 3, Loss: 1654.0291748046875, average loss 15.635807698567708\n",
      "Batch 4, Loss: 1745.6422119140625, average loss 16.090961303710937\n",
      "Batch 5, Loss: 1566.882080078125, average loss 16.006533203125\n",
      "Batch 6, Loss: 1738.82568359375, average loss 16.236820475260416\n",
      "Batch 7, Loss: 1651.2509765625, average loss 16.276204659598214\n",
      "Batch 8, Loss: 1657.858154296875, average loss 16.31400177001953\n",
      "Batch 9, Loss: 1607.3758544921875, average loss 16.287308078342015\n",
      "Batch 10, Loss: 1669.78076171875, average loss 16.328358032226564\n",
      "Total loss: 16313.830322265625\n",
      "Validation loss: 16.313830322265623\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: \n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 83, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1580.42578125, average loss 15.8042578125\n",
      "Batch 2, Loss: 1653.5968017578125, average loss 16.170112915039063\n",
      "Batch 3, Loss: 1741.572998046875, average loss 16.585318603515624\n",
      "Batch 4, Loss: 1569.009765625, average loss 16.361513366699217\n",
      "Batch 5, Loss: 1738.9661865234375, average loss 16.56714306640625\n",
      "Batch 6, Loss: 1648.349609375, average loss 16.553201904296873\n",
      "Batch 7, Loss: 1656.4444580078125, average loss 16.554808000837053\n",
      "Batch 8, Loss: 1607.32666015625, average loss 16.494615325927736\n",
      "Batch 9, Loss: 1667.261962890625, average loss 16.514393581814236\n",
      "Batch 10, Loss: 1453.1436767578125, average loss 16.316097900390623\n",
      "Total loss: 16313.224609375\n",
      "Validation loss: 16.313224609375\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 84, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1653.712890625, average loss 16.53712890625\n",
      "Batch 2, Loss: 1747.3428955078125, average loss 17.005278930664062\n",
      "Batch 3, Loss: 1564.9881591796875, average loss 16.553479817708332\n",
      "Batch 4, Loss: 1737.8121337890625, average loss 16.759640197753907\n",
      "Batch 5, Loss: 1650.042236328125, average loss 16.707796630859374\n",
      "Batch 6, Loss: 1657.708251953125, average loss 16.68601094563802\n",
      "Batch 7, Loss: 1607.376953125, average loss 16.598547886439732\n",
      "Batch 8, Loss: 1667.7294921875, average loss 16.60839126586914\n",
      "Batch 9, Loss: 1451.566162109375, average loss 16.375865749782985\n",
      "Batch 10, Loss: 1581.138671875, average loss 16.319417846679688\n",
      "Total loss: 16305.49169921875\n",
      "Validation loss: 16.30549169921875\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: \n",
      "Epoch 85, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1744.97802734375, average loss 17.4497802734375\n",
      "Batch 2, Loss: 1563.34375, average loss 16.54160888671875\n",
      "Batch 3, Loss: 1736.515625, average loss 16.816124674479166\n",
      "Batch 4, Loss: 1646.307861328125, average loss 16.727863159179687\n",
      "Batch 5, Loss: 1656.340576171875, average loss 16.6949716796875\n",
      "Batch 6, Loss: 1606.0750732421875, average loss 16.589268188476563\n",
      "Batch 7, Loss: 1662.93212890625, average loss 16.594990059988838\n",
      "Batch 8, Loss: 1447.724609375, average loss 16.330272064208984\n",
      "Batch 9, Loss: 1578.365966796875, average loss 16.269537353515624\n",
      "Batch 10, Loss: 1650.98974609375, average loss 16.293573364257814\n",
      "Total loss: 16267.83447265625\n",
      "Validation loss: 16.26783447265625\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: \n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 86, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1559.671630859375, average loss 15.59671630859375\n",
      "Batch 2, Loss: 1734.7801513671875, average loss 16.472258911132812\n",
      "Batch 3, Loss: 1641.82861328125, average loss 16.454267985026043\n",
      "Batch 4, Loss: 1651.3687744140625, average loss 16.469122924804687\n",
      "Batch 5, Loss: 1601.6075439453125, average loss 16.378513427734376\n",
      "Batch 6, Loss: 1658.79638671875, average loss 16.413421834309897\n",
      "Batch 7, Loss: 1442.22900390625, average loss 16.12897443498884\n",
      "Batch 8, Loss: 1571.944091796875, average loss 16.07778274536133\n",
      "Batch 9, Loss: 1640.3812255859375, average loss 16.114008246527778\n",
      "Batch 10, Loss: 1724.6966552734375, average loss 16.22730407714844\n",
      "Total loss: 16166.588623046875\n",
      "Validation loss: 16.166588623046874\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: \n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 87, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1725.997314453125, average loss 17.25997314453125\n",
      "Batch 2, Loss: 1631.88427734375, average loss 16.789407958984373\n",
      "Batch 3, Loss: 1641.731689453125, average loss 16.665377604166668\n",
      "Batch 4, Loss: 1589.94091796875, average loss 16.473885498046876\n",
      "Batch 5, Loss: 1654.3427734375, average loss 16.4877939453125\n",
      "Batch 6, Loss: 1435.810302734375, average loss 16.132845458984374\n",
      "Batch 7, Loss: 1559.3165283203125, average loss 16.055748291015625\n",
      "Batch 8, Loss: 1625.0361328125, average loss 16.080074920654297\n",
      "Batch 9, Loss: 1702.490966796875, average loss 16.185056559244792\n",
      "Batch 10, Loss: 1547.99658203125, average loss 16.11454748535156\n",
      "Total loss: 15971.394653320312\n",
      "Validation loss: 15.971394653320312\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: \n",
      "\tInput: 9303634\tPrediction: \n",
      "Epoch 88, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1609.5179443359375, average loss 16.095179443359374\n",
      "Batch 2, Loss: 1616.528076171875, average loss 16.130230102539063\n",
      "Batch 3, Loss: 1581.6240234375, average loss 16.02556681315104\n",
      "Batch 4, Loss: 1630.24365234375, average loss 16.094784240722657\n",
      "Batch 5, Loss: 1415.8564453125, average loss 15.707540283203125\n",
      "Batch 6, Loss: 1532.060791015625, average loss 15.643051554361978\n",
      "Batch 7, Loss: 1602.30419921875, average loss 15.697335902622768\n",
      "Batch 8, Loss: 1693.852783203125, average loss 15.852484893798827\n",
      "Batch 9, Loss: 1513.119873046875, average loss 15.772341986762152\n",
      "Batch 10, Loss: 1701.946533203125, average loss 15.897054321289062\n",
      "Total loss: 15837.303833007812\n",
      "Validation loss: 15.837303833007812\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 2\n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 89, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1599.0908203125, average loss 15.990908203125\n",
      "Batch 2, Loss: 1569.5572509765625, average loss 15.843240356445312\n",
      "Batch 3, Loss: 1620.19873046875, average loss 15.962822672526041\n",
      "Batch 4, Loss: 1422.0029296875, average loss 15.527124328613281\n",
      "Batch 5, Loss: 1567.7862548828125, average loss 15.55727197265625\n",
      "Batch 6, Loss: 1606.844482421875, average loss 15.642467447916667\n",
      "Batch 7, Loss: 1686.6658935546875, average loss 15.817351946149554\n",
      "Batch 8, Loss: 1540.2508544921875, average loss 15.765496520996093\n",
      "Batch 9, Loss: 1700.3388671875, average loss 15.903040093315973\n",
      "Batch 10, Loss: 1614.7841796875, average loss 15.927520263671875\n",
      "Total loss: 16860.707275390625\n",
      "Validation loss: 16.860707275390624\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: \n",
      "\tInput: 6167476\tPrediction: \n",
      "Epoch 90, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1565.9588623046875, average loss 15.659588623046876\n",
      "Batch 2, Loss: 1620.5067138671875, average loss 15.932327880859376\n",
      "Batch 3, Loss: 1406.0643310546875, average loss 15.308433024088542\n",
      "Batch 4, Loss: 1526.3428955078125, average loss 15.297182006835937\n",
      "Batch 5, Loss: 1596.3397216796875, average loss 15.430425048828125\n",
      "Batch 6, Loss: 1685.6353759765625, average loss 15.668079833984375\n",
      "Batch 7, Loss: 1511.205810546875, average loss 15.588648158482142\n",
      "Batch 8, Loss: 1689.630615234375, average loss 15.752105407714843\n",
      "Batch 9, Loss: 1590.628173828125, average loss 15.76923611111111\n",
      "Batch 10, Loss: 1595.834716796875, average loss 15.788147216796874\n",
      "Total loss: 17804.169799804688\n",
      "Validation loss: 17.804169799804686\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 2\n",
      "\tInput: 21151199\tPrediction: 2\n",
      "Epoch 91, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1614.58642578125, average loss 16.1458642578125\n",
      "Batch 2, Loss: 1397.97412109375, average loss 15.062802734375\n",
      "Batch 3, Loss: 1521.26318359375, average loss 15.112745768229166\n",
      "Batch 4, Loss: 1593.043212890625, average loss 15.317167358398438\n",
      "Batch 5, Loss: 1679.7408447265625, average loss 15.613215576171875\n",
      "Batch 6, Loss: 1508.937744140625, average loss 15.525909220377605\n",
      "Batch 7, Loss: 1688.382568359375, average loss 15.719897286551339\n",
      "Batch 8, Loss: 1588.785400390625, average loss 15.740891876220703\n",
      "Batch 9, Loss: 1593.5390625, average loss 15.762502848307292\n",
      "Batch 10, Loss: 1560.3349609375, average loss 15.746587524414062\n",
      "Total loss: 18099.224731445312\n",
      "Validation loss: 18.09922473144531\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 8\n",
      "\tInput: 76206\tPrediction: 8\n",
      "Epoch 92, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1393.9345703125, average loss 13.939345703125\n",
      "Batch 2, Loss: 1519.3756103515625, average loss 14.566550903320312\n",
      "Batch 3, Loss: 1588.552490234375, average loss 15.006208902994791\n",
      "Batch 4, Loss: 1677.25048828125, average loss 15.447782897949219\n",
      "Batch 5, Loss: 1505.228759765625, average loss 15.368683837890625\n",
      "Batch 6, Loss: 1686.7130126953125, average loss 15.618424886067709\n",
      "Batch 7, Loss: 1588.241943359375, average loss 15.656138392857143\n",
      "Batch 8, Loss: 1591.8428955078125, average loss 15.688924713134766\n",
      "Batch 9, Loss: 1559.1805419921875, average loss 15.678133680555556\n",
      "Batch 10, Loss: 1611.572998046875, average loss 15.721893310546875\n",
      "Total loss: 15731.460205078125\n",
      "Validation loss: 15.731460205078125\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 3\n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 93, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1520.3382568359375, average loss 15.203382568359375\n",
      "Batch 2, Loss: 1586.907470703125, average loss 15.536228637695313\n",
      "Batch 3, Loss: 1677.9462890625, average loss 15.95064005533854\n",
      "Batch 4, Loss: 1507.2244873046875, average loss 15.731041259765625\n",
      "Batch 5, Loss: 1683.5606689453125, average loss 15.951954345703125\n",
      "Batch 6, Loss: 1584.5875244140625, average loss 15.934274495442708\n",
      "Batch 7, Loss: 1589.9443359375, average loss 15.929298618861607\n",
      "Batch 8, Loss: 1559.2900390625, average loss 15.887248840332031\n",
      "Batch 9, Loss: 1609.767822265625, average loss 15.9106298828125\n",
      "Batch 10, Loss: 1394.196533203125, average loss 15.713763427734374\n",
      "Total loss: 15710.555786132812\n",
      "Validation loss: 15.710555786132813\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 94, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1588.51904296875, average loss 15.8851904296875\n",
      "Batch 2, Loss: 1678.115234375, average loss 16.33317138671875\n",
      "Batch 3, Loss: 1506.65625, average loss 15.910968424479167\n",
      "Batch 4, Loss: 1682.755126953125, average loss 16.140114135742188\n",
      "Batch 5, Loss: 1585.115966796875, average loss 16.0823232421875\n",
      "Batch 6, Loss: 1591.5823974609375, average loss 16.054573364257813\n",
      "Batch 7, Loss: 1559.60546875, average loss 15.989070696149554\n",
      "Batch 8, Loss: 1610.737548828125, average loss 16.003858795166014\n",
      "Batch 9, Loss: 1392.628173828125, average loss 15.773016899956597\n",
      "Batch 10, Loss: 1519.921875, average loss 15.715637084960937\n",
      "Total loss: 15711.205810546875\n",
      "Validation loss: 15.711205810546875\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: 8\n",
      "Epoch 95, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1678.4168701171875, average loss 16.784168701171875\n",
      "Batch 2, Loss: 1504.1097412109375, average loss 15.912633056640624\n",
      "Batch 3, Loss: 1684.708740234375, average loss 16.224117838541666\n",
      "Batch 4, Loss: 1584.4698486328125, average loss 16.12926300048828\n",
      "Batch 5, Loss: 1590.8363037109375, average loss 16.0850830078125\n",
      "Batch 6, Loss: 1560.780517578125, average loss 16.005536702473957\n",
      "Batch 7, Loss: 1608.13818359375, average loss 16.016371721540178\n",
      "Batch 8, Loss: 1392.750732421875, average loss 15.755263671875\n",
      "Batch 9, Loss: 1516.701171875, average loss 15.68990234375\n",
      "Batch 10, Loss: 1588.87451171875, average loss 15.70978662109375\n",
      "Total loss: 15699.568237304688\n",
      "Validation loss: 15.699568237304687\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 3\n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 96, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1504.244384765625, average loss 15.04244384765625\n",
      "Batch 2, Loss: 1683.6298828125, average loss 15.939371337890625\n",
      "Batch 3, Loss: 1584.0458984375, average loss 15.906400553385417\n",
      "Batch 4, Loss: 1592.485107421875, average loss 15.91101318359375\n",
      "Batch 5, Loss: 1559.4539794921875, average loss 15.847718505859374\n",
      "Batch 6, Loss: 1609.37646484375, average loss 15.888726196289063\n",
      "Batch 7, Loss: 1392.8856201171875, average loss 15.608744768415178\n",
      "Batch 8, Loss: 1518.4066162109375, average loss 15.555659942626953\n",
      "Batch 9, Loss: 1584.771484375, average loss 15.58811048719618\n",
      "Batch 10, Loss: 1677.971435546875, average loss 15.707270874023438\n",
      "Total loss: 15693.072265625\n",
      "Validation loss: 15.693072265625\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 3\n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 97, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1682.33984375, average loss 16.8233984375\n",
      "Batch 2, Loss: 1582.2698974609375, average loss 16.32304870605469\n",
      "Batch 3, Loss: 1590.6082763671875, average loss 16.18406005859375\n",
      "Batch 4, Loss: 1558.1884765625, average loss 16.033516235351563\n",
      "Batch 5, Loss: 1610.085693359375, average loss 16.046984375\n",
      "Batch 6, Loss: 1392.1025390625, average loss 15.692657877604166\n",
      "Batch 7, Loss: 1518.479248046875, average loss 15.620105678013394\n",
      "Batch 8, Loss: 1586.282470703125, average loss 15.650445556640625\n",
      "Batch 9, Loss: 1676.153564453125, average loss 15.773900010850694\n",
      "Batch 10, Loss: 1506.06689453125, average loss 15.702576904296874\n",
      "Total loss: 15690.891967773438\n",
      "Validation loss: 15.690891967773437\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 7\n",
      "\tInput: 9303634\tPrediction: 7\n",
      "Epoch 98, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1580.42822265625, average loss 15.8042822265625\n",
      "Batch 2, Loss: 1588.449951171875, average loss 15.844390869140625\n",
      "Batch 3, Loss: 1558.4232177734375, average loss 15.757671305338542\n",
      "Batch 4, Loss: 1608.139892578125, average loss 15.838603210449218\n",
      "Batch 5, Loss: 1390.67724609375, average loss 15.452237060546874\n",
      "Batch 6, Loss: 1516.1148681640625, average loss 15.403722330729167\n",
      "Batch 7, Loss: 1587.1083984375, average loss 15.47048828125\n",
      "Batch 8, Loss: 1677.3216552734375, average loss 15.633329315185547\n",
      "Batch 9, Loss: 1503.249267578125, average loss 15.56656968858507\n",
      "Batch 10, Loss: 1684.059814453125, average loss 15.693972534179688\n",
      "Total loss: 15697.68994140625\n",
      "Validation loss: 15.69768994140625\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 3\n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 99, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1591.4088134765625, average loss 15.914088134765626\n",
      "Batch 2, Loss: 1560.4044189453125, average loss 15.759066162109375\n",
      "Batch 3, Loss: 1607.9830322265625, average loss 15.865987548828125\n",
      "Batch 4, Loss: 1390.730224609375, average loss 15.376316223144531\n",
      "Batch 5, Loss: 1516.0955810546875, average loss 15.333244140625\n",
      "Batch 6, Loss: 1585.41259765625, average loss 15.420057779947916\n",
      "Batch 7, Loss: 1678.776123046875, average loss 15.615443987165179\n",
      "Batch 8, Loss: 1501.302490234375, average loss 15.5401416015625\n",
      "Batch 9, Loss: 1681.2723388671875, average loss 15.681539577907985\n",
      "Batch 10, Loss: 1584.853271484375, average loss 15.698238891601562\n",
      "Total loss: 15692.28857421875\n",
      "Validation loss: 15.69228857421875\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 3\n",
      "\tInput: 6167476\tPrediction: 3\n",
      "Epoch 100, lr=[0.0025]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1559.1363525390625, average loss 15.591363525390625\n",
      "Batch 2, Loss: 1608.1163330078125, average loss 15.836263427734375\n",
      "Batch 3, Loss: 1389.323486328125, average loss 15.188587239583333\n",
      "Batch 4, Loss: 1515.119140625, average loss 15.17923828125\n",
      "Batch 5, Loss: 1584.9351806640625, average loss 15.313260986328125\n",
      "Batch 6, Loss: 1677.0943603515625, average loss 15.556208089192708\n",
      "Batch 7, Loss: 1504.1751708984375, average loss 15.482714320591517\n",
      "Batch 8, Loss: 1682.450927734375, average loss 15.650438690185547\n",
      "Batch 9, Loss: 1580.41162109375, average loss 15.667513970269097\n",
      "Batch 10, Loss: 1589.8502197265625, average loss 15.69061279296875\n",
      "Total loss: 15682.71142578125\n",
      "Validation loss: 15.68271142578125\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 3\n",
      "\tInput: 21151199\tPrediction: 3\n",
      "Epoch 101, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1606.3521728515625, average loss 16.063521728515624\n",
      "Batch 2, Loss: 1389.4683837890625, average loss 14.979102783203125\n",
      "Batch 3, Loss: 1514.537841796875, average loss 15.034527994791667\n",
      "Batch 4, Loss: 1585.135498046875, average loss 15.238734741210937\n",
      "Batch 5, Loss: 1676.00146484375, average loss 15.54299072265625\n",
      "Batch 6, Loss: 1502.61376953125, average loss 15.45684855143229\n",
      "Batch 7, Loss: 1680.4873046875, average loss 15.649423479352679\n",
      "Batch 8, Loss: 1581.291259765625, average loss 15.669859619140626\n",
      "Batch 9, Loss: 1588.854248046875, average loss 15.69415771484375\n",
      "Batch 10, Loss: 1558.4375, average loss 15.683179443359375\n",
      "Total loss: 15681.295654296875\n",
      "Validation loss: 15.681295654296875\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: \n",
      "Epoch 102, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1388.62939453125, average loss 13.8862939453125\n",
      "Batch 2, Loss: 1515.16259765625, average loss 14.5189599609375\n",
      "Batch 3, Loss: 1584.765625, average loss 14.961858723958333\n",
      "Batch 4, Loss: 1675.689453125, average loss 15.41061767578125\n",
      "Batch 5, Loss: 1502.360107421875, average loss 15.33321435546875\n",
      "Batch 6, Loss: 1679.9154052734375, average loss 15.577537638346355\n",
      "Batch 7, Loss: 1582.08935546875, average loss 15.612302769252231\n",
      "Batch 8, Loss: 1588.39697265625, average loss 15.646261138916016\n",
      "Batch 9, Loss: 1558.098876953125, average loss 15.639008653428819\n",
      "Batch 10, Loss: 1607.734130859375, average loss 15.682841918945313\n",
      "Total loss: 15678.621948242188\n",
      "Validation loss: 15.678621948242187\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 3\n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 103, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1515.3271484375, average loss 15.153271484375\n",
      "Batch 2, Loss: 1583.6156005859375, average loss 15.494713745117188\n",
      "Batch 3, Loss: 1675.249755859375, average loss 15.913975016276042\n",
      "Batch 4, Loss: 1503.4962158203125, average loss 15.694221801757813\n",
      "Batch 5, Loss: 1679.4300537109375, average loss 15.914237548828124\n",
      "Batch 6, Loss: 1580.592529296875, average loss 15.896185506184896\n",
      "Batch 7, Loss: 1587.659912109375, average loss 15.893387451171876\n",
      "Batch 8, Loss: 1556.944091796875, average loss 15.852894134521485\n",
      "Batch 9, Loss: 1607.1199951171875, average loss 15.877150336371528\n",
      "Batch 10, Loss: 1390.602294921875, average loss 15.68003759765625\n",
      "Total loss: 15676.037231445312\n",
      "Validation loss: 15.676037231445312\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 3\n",
      "\tInput: 94164\tPrediction: \n",
      "Epoch 104, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1583.26953125, average loss 15.8326953125\n",
      "Batch 2, Loss: 1675.864013671875, average loss 16.295667724609373\n",
      "Batch 3, Loss: 1502.939208984375, average loss 15.873575846354166\n",
      "Batch 4, Loss: 1679.200439453125, average loss 16.103182983398437\n",
      "Batch 5, Loss: 1580.4725341796875, average loss 16.043491455078126\n",
      "Batch 6, Loss: 1587.7705078125, average loss 16.015860392252605\n",
      "Batch 7, Loss: 1556.9718017578125, average loss 15.952125767299107\n",
      "Batch 8, Loss: 1606.3525390625, average loss 15.966050720214843\n",
      "Batch 9, Loss: 1388.850830078125, average loss 15.73521267361111\n",
      "Batch 10, Loss: 1515.8807373046875, average loss 15.677572143554688\n",
      "Total loss: 15670.733032226562\n",
      "Validation loss: 15.670733032226563\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: 3\n",
      "Epoch 105, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1674.604248046875, average loss 16.74604248046875\n",
      "Batch 2, Loss: 1501.71435546875, average loss 15.881593017578124\n",
      "Batch 3, Loss: 1679.8447265625, average loss 16.187211100260416\n",
      "Batch 4, Loss: 1580.632080078125, average loss 16.091988525390626\n",
      "Batch 5, Loss: 1587.977294921875, average loss 16.04954541015625\n",
      "Batch 6, Loss: 1557.72265625, average loss 15.97082560221354\n",
      "Batch 7, Loss: 1606.05517578125, average loss 15.983643624441964\n",
      "Batch 8, Loss: 1388.1329345703125, average loss 15.72085433959961\n",
      "Batch 9, Loss: 1513.1004638671875, average loss 15.655315483940972\n",
      "Batch 10, Loss: 1584.3856201171875, average loss 15.674169555664063\n",
      "Total loss: 15670.226806640625\n",
      "Validation loss: 15.670226806640626\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 8\n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 106, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1501.1319580078125, average loss 15.011319580078125\n",
      "Batch 2, Loss: 1679.4691162109375, average loss 15.90300537109375\n",
      "Batch 3, Loss: 1580.14501953125, average loss 15.869153645833334\n",
      "Batch 4, Loss: 1587.132568359375, average loss 15.869696655273437\n",
      "Batch 5, Loss: 1558.1224365234375, average loss 15.812002197265626\n",
      "Batch 6, Loss: 1604.069091796875, average loss 15.85011698404948\n",
      "Batch 7, Loss: 1388.3333740234375, average loss 15.56914794921875\n",
      "Batch 8, Loss: 1513.1463623046875, average loss 15.514437408447266\n",
      "Batch 9, Loss: 1583.13525390625, average loss 15.549650200737847\n",
      "Batch 10, Loss: 1674.7099609375, average loss 15.669395141601562\n",
      "Total loss: 15675.590454101562\n",
      "Validation loss: 15.675590454101563\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 3\n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 107, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1679.2669677734375, average loss 16.792669677734374\n",
      "Batch 2, Loss: 1578.55078125, average loss 16.289088745117187\n",
      "Batch 3, Loss: 1586.5126953125, average loss 16.147768147786458\n",
      "Batch 4, Loss: 1557.509521484375, average loss 16.00459991455078\n",
      "Batch 5, Loss: 1604.487548828125, average loss 16.012655029296877\n",
      "Batch 6, Loss: 1389.052490234375, average loss 15.658966674804688\n",
      "Batch 7, Loss: 1512.5863037109375, average loss 15.582809012276785\n",
      "Batch 8, Loss: 1581.56787109375, average loss 15.611917724609375\n",
      "Batch 9, Loss: 1874.95458984375, average loss 15.960543077256945\n",
      "Batch 10, Loss: 1542.078125, average loss 15.90656689453125\n",
      "Total loss: 16358.424438476562\n",
      "Validation loss: 16.35842443847656\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 3\n",
      "\tInput: 9303634\tPrediction: 3\n",
      "Epoch 108, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1631.48388671875, average loss 16.3148388671875\n",
      "Batch 2, Loss: 1607.951904296875, average loss 16.197178955078126\n",
      "Batch 3, Loss: 1594.869873046875, average loss 16.114352213541668\n",
      "Batch 4, Loss: 1656.4619140625, average loss 16.2269189453125\n",
      "Batch 5, Loss: 1406.9970703125, average loss 15.795529296875\n",
      "Batch 6, Loss: 1538.0433349609375, average loss 15.726346638997397\n",
      "Batch 7, Loss: 1618.5931396484375, average loss 15.792001604352679\n",
      "Batch 8, Loss: 1683.603271484375, average loss 15.922505493164062\n",
      "Batch 9, Loss: 1512.1865234375, average loss 15.833545464409722\n",
      "Batch 10, Loss: 1699.115478515625, average loss 15.949306396484374\n",
      "Total loss: 15852.953735351562\n",
      "Validation loss: 15.852953735351562\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 5\n",
      "\tInput: 11002\tPrediction: \n",
      "Epoch 109, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1600.8355712890625, average loss 16.008355712890626\n",
      "Batch 2, Loss: 1570.7437744140625, average loss 15.857896728515625\n",
      "Batch 3, Loss: 1616.038330078125, average loss 15.9587255859375\n",
      "Batch 4, Loss: 1398.6781005859375, average loss 15.465739440917968\n",
      "Batch 5, Loss: 1521.074951171875, average loss 15.414741455078126\n",
      "Batch 6, Loss: 1592.1187744140625, average loss 15.499149169921875\n",
      "Batch 7, Loss: 1678.3746337890625, average loss 15.682663051060269\n",
      "Batch 8, Loss: 1504.810791015625, average loss 15.603343658447265\n",
      "Batch 9, Loss: 1681.4046630859375, average loss 15.7378662109375\n",
      "Batch 10, Loss: 1587.215576171875, average loss 15.751295166015625\n",
      "Total loss: 15715.539794921875\n",
      "Validation loss: 15.715539794921876\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 3\n",
      "\tInput: 6167476\tPrediction: 3\n",
      "Epoch 110, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1560.4083251953125, average loss 15.604083251953124\n",
      "Batch 2, Loss: 1609.560302734375, average loss 15.849843139648437\n",
      "Batch 3, Loss: 1389.5977783203125, average loss 15.1985546875\n",
      "Batch 4, Loss: 1516.39794921875, average loss 15.189910888671875\n",
      "Batch 5, Loss: 1584.373291015625, average loss 15.32067529296875\n",
      "Batch 6, Loss: 1679.54443359375, average loss 15.566470133463541\n",
      "Batch 7, Loss: 1502.4058837890625, average loss 15.488982805524554\n",
      "Batch 8, Loss: 1681.0008544921875, average loss 15.654111022949218\n",
      "Batch 9, Loss: 1582.96826171875, average loss 15.673618977864583\n",
      "Batch 10, Loss: 1591.2437744140625, average loss 15.697500854492187\n",
      "Total loss: 16302.130737304688\n",
      "Validation loss: 16.302130737304687\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: \n",
      "\tInput: 21151199\tPrediction: \n",
      "Epoch 111, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1663.4476318359375, average loss 16.634476318359376\n",
      "Batch 2, Loss: 1391.23583984375, average loss 15.273417358398438\n",
      "Batch 3, Loss: 1517.71044921875, average loss 15.241313069661459\n",
      "Batch 4, Loss: 1590.1021728515625, average loss 15.406240234375\n",
      "Batch 5, Loss: 1676.2557373046875, average loss 15.677503662109375\n",
      "Batch 6, Loss: 1504.443115234375, average loss 15.571991577148438\n",
      "Batch 7, Loss: 1679.781982421875, average loss 15.747109898158483\n",
      "Batch 8, Loss: 1583.041259765625, average loss 15.757522735595703\n",
      "Batch 9, Loss: 1590.5999755859375, average loss 15.774020182291666\n",
      "Batch 10, Loss: 1561.367919921875, average loss 15.757986083984376\n",
      "Total loss: 15683.780151367188\n",
      "Validation loss: 15.683780151367188\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: 3\n",
      "Epoch 112, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1388.427001953125, average loss 13.88427001953125\n",
      "Batch 2, Loss: 1512.9244384765625, average loss 14.506757202148437\n",
      "Batch 3, Loss: 1583.554931640625, average loss 14.949687906901042\n",
      "Batch 4, Loss: 1674.481689453125, average loss 15.398470153808594\n",
      "Batch 5, Loss: 1505.1728515625, average loss 15.329121826171875\n",
      "Batch 6, Loss: 1680.552978515625, average loss 15.575189819335938\n",
      "Batch 7, Loss: 1581.3385009765625, average loss 15.609217703683036\n",
      "Batch 8, Loss: 1588.489990234375, average loss 15.643677978515624\n",
      "Batch 9, Loss: 1556.357421875, average loss 15.634777560763888\n",
      "Batch 10, Loss: 1607.491943359375, average loss 15.678791748046875\n",
      "Total loss: 15677.246826171875\n",
      "Validation loss: 15.677246826171874\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 3\n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 113, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1514.8525390625, average loss 15.148525390625\n",
      "Batch 2, Loss: 1584.926025390625, average loss 15.498892822265624\n",
      "Batch 3, Loss: 1675.7830810546875, average loss 15.918538818359375\n",
      "Batch 4, Loss: 1503.3646240234375, average loss 15.697315673828125\n",
      "Batch 5, Loss: 1683.573974609375, average loss 15.92500048828125\n",
      "Batch 6, Loss: 1581.09033203125, average loss 15.905984293619792\n",
      "Batch 7, Loss: 1588.91796875, average loss 15.90358363560268\n",
      "Batch 8, Loss: 1556.298583984375, average loss 15.861008911132812\n",
      "Batch 9, Loss: 1607.12109375, average loss 15.884364691840277\n",
      "Batch 10, Loss: 1389.6376953125, average loss 15.68556591796875\n",
      "Total loss: 15671.841674804688\n",
      "Validation loss: 15.671841674804687\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 3\n",
      "\tInput: 94164\tPrediction: 3\n",
      "Epoch 114, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1583.11376953125, average loss 15.8311376953125\n",
      "Batch 2, Loss: 1679.243896484375, average loss 16.311788330078127\n",
      "Batch 3, Loss: 1503.704833984375, average loss 15.886875\n",
      "Batch 4, Loss: 1677.48828125, average loss 16.108876953125\n",
      "Batch 5, Loss: 1578.880615234375, average loss 16.04486279296875\n",
      "Batch 6, Loss: 1589.7119140625, average loss 16.020238850911458\n",
      "Batch 7, Loss: 1556.737548828125, average loss 15.955544084821428\n",
      "Batch 8, Loss: 1607.494140625, average loss 15.97046875\n",
      "Batch 9, Loss: 1388.8658447265625, average loss 15.739156494140625\n",
      "Batch 10, Loss: 1513.75048828125, average loss 15.678991333007813\n",
      "Total loss: 15669.678466796875\n",
      "Validation loss: 15.669678466796874\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: 3\n",
      "Epoch 115, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1675.50927734375, average loss 16.7550927734375\n",
      "Batch 2, Loss: 1501.957275390625, average loss 15.887332763671875\n",
      "Batch 3, Loss: 1677.2578125, average loss 16.18241455078125\n",
      "Batch 4, Loss: 1578.4371337890625, average loss 16.082903747558593\n",
      "Batch 5, Loss: 1586.7435302734375, average loss 16.03981005859375\n",
      "Batch 6, Loss: 1556.2989501953125, average loss 15.960339965820312\n",
      "Batch 7, Loss: 1606.063720703125, average loss 15.97466814313616\n",
      "Batch 8, Loss: 1388.08544921875, average loss 15.712941436767577\n",
      "Batch 9, Loss: 1512.087646484375, average loss 15.647156439887153\n",
      "Batch 10, Loss: 1583.6876220703125, average loss 15.66612841796875\n",
      "Total loss: 15662.249877929688\n",
      "Validation loss: 15.662249877929687\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 7\n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 116, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1501.63818359375, average loss 15.0163818359375\n",
      "Batch 2, Loss: 1677.728759765625, average loss 15.896834716796874\n",
      "Batch 3, Loss: 1578.727294921875, average loss 15.860314127604166\n",
      "Batch 4, Loss: 1587.873046875, average loss 15.864918212890625\n",
      "Batch 5, Loss: 1557.373046875, average loss 15.8066806640625\n",
      "Batch 6, Loss: 1604.9215087890625, average loss 15.847103068033855\n",
      "Batch 7, Loss: 1387.708740234375, average loss 15.565672258649554\n",
      "Batch 8, Loss: 1511.750244140625, average loss 15.50965103149414\n",
      "Batch 9, Loss: 1582.1739501953125, average loss 15.544327528211806\n",
      "Batch 10, Loss: 1674.376708984375, average loss 15.664271484375\n",
      "Total loss: 15658.691650390625\n",
      "Validation loss: 15.658691650390624\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 3\n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 117, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1677.99609375, average loss 16.7799609375\n",
      "Batch 2, Loss: 1577.683837890625, average loss 16.278399658203124\n",
      "Batch 3, Loss: 1585.860107421875, average loss 16.138466796875\n",
      "Batch 4, Loss: 1555.380615234375, average loss 15.992301635742187\n",
      "Batch 5, Loss: 1603.712890625, average loss 16.00126708984375\n",
      "Batch 6, Loss: 1388.400146484375, average loss 15.648389485677084\n",
      "Batch 7, Loss: 1512.179443359375, average loss 15.57316162109375\n",
      "Batch 8, Loss: 1581.68017578125, average loss 15.603616638183594\n",
      "Batch 9, Loss: 1675.031982421875, average loss 15.73102810329861\n",
      "Batch 10, Loss: 1502.4217529296875, average loss 15.660347045898437\n",
      "Total loss: 15656.390625\n",
      "Validation loss: 15.656390625\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 3\n",
      "\tInput: 9303634\tPrediction: 3\n",
      "Epoch 118, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1576.1019287109375, average loss 15.761019287109375\n",
      "Batch 2, Loss: 1587.977294921875, average loss 15.820396118164062\n",
      "Batch 3, Loss: 1555.1392822265625, average loss 15.730728352864583\n",
      "Batch 4, Loss: 1604.6805419921875, average loss 15.809747619628906\n",
      "Batch 5, Loss: 1389.177490234375, average loss 15.426153076171875\n",
      "Batch 6, Loss: 1511.95458984375, average loss 15.375051879882813\n",
      "Batch 7, Loss: 1581.734130859375, average loss 15.438236083984375\n",
      "Batch 8, Loss: 1675.4783935546875, average loss 15.602804565429688\n",
      "Batch 9, Loss: 1500.421630859375, average loss 15.536294759114583\n",
      "Batch 10, Loss: 1677.40380859375, average loss 15.660069091796874\n",
      "Total loss: 15643.514282226562\n",
      "Validation loss: 15.643514282226562\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 3\n",
      "\tInput: 11002\tPrediction: 5\n",
      "Epoch 119, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1586.04931640625, average loss 15.8604931640625\n",
      "Batch 2, Loss: 1554.4222412109375, average loss 15.702357788085937\n",
      "Batch 3, Loss: 1603.5335693359375, average loss 15.813350423177083\n",
      "Batch 4, Loss: 1385.738525390625, average loss 15.324359130859374\n",
      "Batch 5, Loss: 1511.44091796875, average loss 15.282369140625\n",
      "Batch 6, Loss: 1580.20166015625, average loss 15.36897705078125\n",
      "Batch 7, Loss: 1672.861572265625, average loss 15.563211146763393\n",
      "Batch 8, Loss: 1497.740234375, average loss 15.48998504638672\n",
      "Batch 9, Loss: 1675.159423828125, average loss 15.630163845486111\n",
      "Batch 10, Loss: 1579.491943359375, average loss 15.646639404296875\n",
      "Total loss: 15645.955444335938\n",
      "Validation loss: 15.645955444335938\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 3\n",
      "\tInput: 6167476\tPrediction: 3\n",
      "Epoch 120, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1554.5352783203125, average loss 15.545352783203125\n",
      "Batch 2, Loss: 1603.7255859375, average loss 15.791304321289063\n",
      "Batch 3, Loss: 1386.0107421875, average loss 15.147572021484375\n",
      "Batch 4, Loss: 1509.943359375, average loss 15.13553741455078\n",
      "Batch 5, Loss: 1579.76611328125, average loss 15.267962158203124\n",
      "Batch 6, Loss: 1675.186279296875, average loss 15.515278930664062\n",
      "Batch 7, Loss: 1498.6363525390625, average loss 15.439719587053572\n",
      "Batch 8, Loss: 1674.955322265625, average loss 15.603448791503906\n",
      "Batch 9, Loss: 1576.8193359375, average loss 15.621753743489583\n",
      "Batch 10, Loss: 1587.289794921875, average loss 15.6468681640625\n",
      "Total loss: 15632.956176757812\n",
      "Validation loss: 15.632956176757812\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 3\n",
      "\tInput: 21151199\tPrediction: 3\n",
      "Epoch 121, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1603.172119140625, average loss 16.03172119140625\n",
      "Batch 2, Loss: 1385.063232421875, average loss 14.9411767578125\n",
      "Batch 3, Loss: 1512.2296142578125, average loss 15.001549886067709\n",
      "Batch 4, Loss: 1582.3828125, average loss 15.207119445800782\n",
      "Batch 5, Loss: 1674.78369140625, average loss 15.515262939453125\n",
      "Batch 6, Loss: 1499.771728515625, average loss 15.429005330403646\n",
      "Batch 7, Loss: 1678.23046875, average loss 15.62233380998884\n",
      "Batch 8, Loss: 1578.6702880859375, average loss 15.642879943847657\n",
      "Batch 9, Loss: 1588.6685791015625, average loss 15.669969482421875\n",
      "Batch 10, Loss: 1553.57958984375, average loss 15.656552124023438\n",
      "Total loss: 15630.37841796875\n",
      "Validation loss: 15.63037841796875\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: 3\n",
      "Epoch 122, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1386.2220458984375, average loss 13.862220458984375\n",
      "Batch 2, Loss: 1511.50146484375, average loss 14.488617553710938\n",
      "Batch 3, Loss: 1580.9404296875, average loss 14.928879801432291\n",
      "Batch 4, Loss: 1673.107421875, average loss 15.379428405761718\n",
      "Batch 5, Loss: 1499.946533203125, average loss 15.303435791015625\n",
      "Batch 6, Loss: 1674.9617919921875, average loss 15.544466145833333\n",
      "Batch 7, Loss: 1575.330810546875, average loss 15.574300711495535\n",
      "Batch 8, Loss: 1587.8507080078125, average loss 15.61232650756836\n",
      "Batch 9, Loss: 1551.396728515625, average loss 15.601397705078124\n",
      "Batch 10, Loss: 1601.95556640625, average loss 15.643213500976563\n",
      "Total loss: 15620.950805664062\n",
      "Validation loss: 15.620950805664062\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 3\n",
      "\tInput: 925\tPrediction: \n",
      "Epoch 123, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1511.270751953125, average loss 15.11270751953125\n",
      "Batch 2, Loss: 1578.9500732421875, average loss 15.451104125976563\n",
      "Batch 3, Loss: 1671.956298828125, average loss 15.873923746744792\n",
      "Batch 4, Loss: 1499.8321533203125, average loss 15.655023193359375\n",
      "Batch 5, Loss: 1673.75341796875, average loss 15.871525390625\n",
      "Batch 6, Loss: 1574.2509765625, average loss 15.850022786458334\n",
      "Batch 7, Loss: 1584.910400390625, average loss 15.849891531808035\n",
      "Batch 8, Loss: 1549.86767578125, average loss 15.805989685058593\n",
      "Batch 9, Loss: 1599.630615234375, average loss 15.827135959201389\n",
      "Batch 10, Loss: 1386.53173828125, average loss 15.6309541015625\n",
      "Total loss: 15613.70458984375\n",
      "Validation loss: 15.61370458984375\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: \n",
      "\tInput: 94164\tPrediction: 3\n",
      "Epoch 124, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1577.8826904296875, average loss 15.778826904296874\n",
      "Batch 2, Loss: 1677.106689453125, average loss 16.27494689941406\n",
      "Batch 3, Loss: 1498.3837890625, average loss 15.844577229817709\n",
      "Batch 4, Loss: 1672.5323486328125, average loss 16.064763793945314\n",
      "Batch 5, Loss: 1574.1966552734375, average loss 16.000204345703125\n",
      "Batch 6, Loss: 1584.4908447265625, average loss 15.97432169596354\n",
      "Batch 7, Loss: 1547.366943359375, average loss 15.90279994419643\n",
      "Batch 8, Loss: 1598.83203125, average loss 15.913489990234375\n",
      "Batch 9, Loss: 1386.134521484375, average loss 15.685473904079862\n",
      "Batch 10, Loss: 1516.78759765625, average loss 15.633714111328125\n",
      "Total loss: 15595.913452148438\n",
      "Validation loss: 15.595913452148437\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: \n",
      "\tInput: 64009172\tPrediction: 3\n",
      "Epoch 125, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1672.309326171875, average loss 16.72309326171875\n",
      "Batch 2, Loss: 1496.232177734375, average loss 15.84270751953125\n",
      "Batch 3, Loss: 1672.8704833984375, average loss 16.13803995768229\n",
      "Batch 4, Loss: 1577.510498046875, average loss 16.047306213378906\n",
      "Batch 5, Loss: 1588.3035888671875, average loss 16.0144521484375\n",
      "Batch 6, Loss: 1546.833740234375, average loss 15.923433024088542\n",
      "Batch 7, Loss: 1594.806640625, average loss 15.926952078683035\n",
      "Batch 8, Loss: 1385.3310546875, average loss 15.66774688720703\n",
      "Batch 9, Loss: 1513.5491943359375, average loss 15.608607449001736\n",
      "Batch 10, Loss: 1580.083984375, average loss 15.627830688476562\n",
      "Total loss: 15565.978881835938\n",
      "Validation loss: 15.565978881835937\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 7\n",
      "\tInput: 6824\tPrediction: \n",
      "Epoch 126, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1494.9007568359375, average loss 14.949007568359375\n",
      "Batch 2, Loss: 1667.50830078125, average loss 15.812045288085937\n",
      "Batch 3, Loss: 1564.8216552734375, average loss 15.757435709635416\n",
      "Batch 4, Loss: 1580.5299072265625, average loss 15.769401550292969\n",
      "Batch 5, Loss: 1545.11962890625, average loss 15.705760498046875\n",
      "Batch 6, Loss: 1594.5355224609375, average loss 15.745692952473958\n",
      "Batch 7, Loss: 1380.23583984375, average loss 15.46807373046875\n",
      "Batch 8, Loss: 1507.0509033203125, average loss 15.418378143310546\n",
      "Batch 9, Loss: 1570.77685546875, average loss 15.450532633463542\n",
      "Batch 10, Loss: 1657.35009765625, average loss 15.562829467773437\n",
      "Total loss: 15565.340942382812\n",
      "Validation loss: 15.565340942382813\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 3\n",
      "\tInput: 135\tPrediction: \n",
      "Epoch 127, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1661.8880615234375, average loss 16.618880615234374\n",
      "Batch 2, Loss: 1560.844482421875, average loss 16.113662719726562\n",
      "Batch 3, Loss: 1576.0245361328125, average loss 15.99585693359375\n",
      "Batch 4, Loss: 1538.094482421875, average loss 15.84212890625\n",
      "Batch 5, Loss: 1588.12353515625, average loss 15.8499501953125\n",
      "Batch 6, Loss: 1377.8304443359375, average loss 15.504675903320312\n",
      "Batch 7, Loss: 1504.195068359375, average loss 15.438572300502232\n",
      "Batch 8, Loss: 1571.6177978515625, average loss 15.473273010253907\n",
      "Batch 9, Loss: 1653.645751953125, average loss 15.591404622395833\n",
      "Batch 10, Loss: 1487.963623046875, average loss 15.520227783203126\n",
      "Total loss: 15568.95703125\n",
      "Validation loss: 15.56895703125\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 7\n",
      "\tInput: 9303634\tPrediction: 3\n",
      "Epoch 128, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1555.826171875, average loss 15.55826171875\n",
      "Batch 2, Loss: 1574.912353515625, average loss 15.653692626953125\n",
      "Batch 3, Loss: 1535.18994140625, average loss 15.553094889322917\n",
      "Batch 4, Loss: 1584.215087890625, average loss 15.62535888671875\n",
      "Batch 5, Loss: 1375.2415771484375, average loss 15.250770263671875\n",
      "Batch 6, Loss: 1501.738525390625, average loss 15.21187276204427\n",
      "Batch 7, Loss: 1563.7021484375, average loss 15.272608293805803\n",
      "Batch 8, Loss: 1648.62353515625, average loss 15.42431167602539\n",
      "Batch 9, Loss: 1479.250244140625, average loss 15.354110649956597\n",
      "Batch 10, Loss: 1660.447265625, average loss 15.479146850585938\n",
      "Total loss: 15615.034545898438\n",
      "Validation loss: 15.615034545898437\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 8\n",
      "\tInput: 11002\tPrediction: 8\n",
      "Epoch 129, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1573.271240234375, average loss 15.73271240234375\n",
      "Batch 2, Loss: 1531.8575439453125, average loss 15.525643920898437\n",
      "Batch 3, Loss: 1579.058349609375, average loss 15.613957112630208\n",
      "Batch 4, Loss: 1368.465576171875, average loss 15.131631774902344\n",
      "Batch 5, Loss: 1492.6466064453125, average loss 15.0905986328125\n",
      "Batch 6, Loss: 1565.693115234375, average loss 15.184987386067709\n",
      "Batch 7, Loss: 1645.4271240234375, average loss 15.36631365094866\n",
      "Batch 8, Loss: 1472.73828125, average loss 15.286447296142578\n",
      "Batch 9, Loss: 1651.464599609375, average loss 15.422913818359374\n",
      "Batch 10, Loss: 1555.543701171875, average loss 15.436166137695313\n",
      "Total loss: 15765.462646484375\n",
      "Validation loss: 15.765462646484375\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 8\n",
      "\tInput: 6167476\tPrediction: 8\n",
      "Epoch 130, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1535.415283203125, average loss 15.35415283203125\n",
      "Batch 2, Loss: 1581.1710205078125, average loss 15.582931518554688\n",
      "Batch 3, Loss: 1366.755615234375, average loss 14.944473063151042\n",
      "Batch 4, Loss: 1486.961669921875, average loss 14.92575897216797\n",
      "Batch 5, Loss: 1555.299072265625, average loss 15.051205322265625\n",
      "Batch 6, Loss: 1651.97412109375, average loss 15.295961303710937\n",
      "Batch 7, Loss: 1470.9599609375, average loss 15.212195347377232\n",
      "Batch 8, Loss: 1639.68359375, average loss 15.360275421142578\n",
      "Batch 9, Loss: 1544.721923828125, average loss 15.369935845269097\n",
      "Batch 10, Loss: 1560.0584716796875, average loss 15.393000732421875\n",
      "Total loss: 16162.80517578125\n",
      "Validation loss: 16.16280517578125\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 3\n",
      "\tInput: 21151199\tPrediction: 3\n",
      "Epoch 131, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1571.26513671875, average loss 15.7126513671875\n",
      "Batch 2, Loss: 1360.639404296875, average loss 14.659522705078125\n",
      "Batch 3, Loss: 1479.754150390625, average loss 14.705528971354166\n",
      "Batch 4, Loss: 1551.302978515625, average loss 14.907404174804688\n",
      "Batch 5, Loss: 1632.89794921875, average loss 15.19171923828125\n",
      "Batch 6, Loss: 1465.7216796875, average loss 15.102635498046874\n",
      "Batch 7, Loss: 1634.200927734375, average loss 15.279688895089286\n",
      "Batch 8, Loss: 1538.6246337890625, average loss 15.293008575439453\n",
      "Batch 9, Loss: 1544.81005859375, average loss 15.310241021050347\n",
      "Batch 10, Loss: 1511.3623046875, average loss 15.290579223632813\n",
      "Total loss: 21225.258911132812\n",
      "Validation loss: 21.225258911132812\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 28\n",
      "\tInput: 76206\tPrediction: 28\n",
      "Epoch 132, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1354.782958984375, average loss 13.54782958984375\n",
      "Batch 2, Loss: 1479.566162109375, average loss 14.17174560546875\n",
      "Batch 3, Loss: 1544.0853271484375, average loss 14.594781494140625\n",
      "Batch 4, Loss: 1628.7232666015625, average loss 15.017894287109375\n",
      "Batch 5, Loss: 1459.7874755859375, average loss 14.933890380859374\n",
      "Batch 6, Loss: 1633.79736328125, average loss 15.167904256184896\n",
      "Batch 7, Loss: 1537.5655517578125, average loss 15.1975830078125\n",
      "Batch 8, Loss: 1536.958251953125, average loss 15.219082946777343\n",
      "Batch 9, Loss: 1503.58984375, average loss 15.198729112413194\n",
      "Batch 10, Loss: 1559.0467529296875, average loss 15.237902954101562\n",
      "Total loss: 20475.58056640625\n",
      "Validation loss: 20.47558056640625\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 98\n",
      "\tInput: 925\tPrediction: 93\n",
      "Epoch 133, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1478.269287109375, average loss 14.78269287109375\n",
      "Batch 2, Loss: 1535.931396484375, average loss 15.07100341796875\n",
      "Batch 3, Loss: 1611.745361328125, average loss 15.419820149739584\n",
      "Batch 4, Loss: 1452.5009765625, average loss 15.196117553710938\n",
      "Batch 5, Loss: 1623.216064453125, average loss 15.403326171875\n",
      "Batch 6, Loss: 1533.880615234375, average loss 15.392572835286458\n",
      "Batch 7, Loss: 1525.909912109375, average loss 15.373505161830357\n",
      "Batch 8, Loss: 1492.9083251953125, average loss 15.317952423095702\n",
      "Batch 9, Loss: 1544.205322265625, average loss 15.331741400824653\n",
      "Batch 10, Loss: 1349.658447265625, average loss 15.148225708007812\n",
      "Total loss: 19775.099731445312\n",
      "Validation loss: 19.775099731445312\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 77\n",
      "\tInput: 94164\tPrediction: 777\n",
      "Epoch 134, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1535.55078125, average loss 15.3555078125\n",
      "Batch 2, Loss: 1609.446533203125, average loss 15.724986572265625\n",
      "Batch 3, Loss: 1445.711669921875, average loss 15.30236328125\n",
      "Batch 4, Loss: 1614.9248046875, average loss 15.51408447265625\n",
      "Batch 5, Loss: 1525.54638671875, average loss 15.4623603515625\n",
      "Batch 6, Loss: 1534.2626953125, average loss 15.44240478515625\n",
      "Batch 7, Loss: 1493.6085205078125, average loss 15.37007341657366\n",
      "Batch 8, Loss: 1551.16015625, average loss 15.387764434814454\n",
      "Batch 9, Loss: 1339.092529296875, average loss 15.16589341905382\n",
      "Batch 10, Loss: 1466.8770751953125, average loss 15.11618115234375\n",
      "Total loss: 17217.857543945312\n",
      "Validation loss: 17.217857543945314\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 77\n",
      "\tInput: 64009172\tPrediction: 7\n",
      "Epoch 135, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1605.382568359375, average loss 16.05382568359375\n",
      "Batch 2, Loss: 1435.385498046875, average loss 15.20384033203125\n",
      "Batch 3, Loss: 1608.597412109375, average loss 15.497884928385417\n",
      "Batch 4, Loss: 1520.6737060546875, average loss 15.42509796142578\n",
      "Batch 5, Loss: 1514.6875, average loss 15.369453369140626\n",
      "Batch 6, Loss: 1484.0517578125, average loss 15.281297403971354\n",
      "Batch 7, Loss: 1531.251708984375, average loss 15.285757359095982\n",
      "Batch 8, Loss: 1334.116943359375, average loss 15.042683868408202\n",
      "Batch 9, Loss: 1462.299072265625, average loss 14.996051296657987\n",
      "Batch 10, Loss: 1503.041259765625, average loss 14.999487426757813\n",
      "Total loss: 21566.383178710938\n",
      "Validation loss: 21.566383178710936\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 8\n",
      "\tInput: 6824\tPrediction: 8\n",
      "Epoch 136, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1429.7314453125, average loss 14.297314453125\n",
      "Batch 2, Loss: 1602.7646484375, average loss 15.16248046875\n",
      "Batch 3, Loss: 1509.977783203125, average loss 15.14157958984375\n",
      "Batch 4, Loss: 1504.42529296875, average loss 15.117247924804687\n",
      "Batch 5, Loss: 1473.9356689453125, average loss 15.041669677734374\n",
      "Batch 6, Loss: 1528.721923828125, average loss 15.082594604492188\n",
      "Batch 7, Loss: 1321.1226806640625, average loss 14.81525634765625\n",
      "Batch 8, Loss: 1437.35888671875, average loss 14.760047912597656\n",
      "Batch 9, Loss: 1486.99365234375, average loss 14.772257758246528\n",
      "Batch 10, Loss: 1571.26806640625, average loss 14.866300048828125\n",
      "Total loss: 21274.798950195312\n",
      "Validation loss: 21.27479895019531\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 28\n",
      "\tInput: 135\tPrediction: 28\n",
      "Epoch 137, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1593.3759765625, average loss 15.933759765625\n",
      "Batch 2, Loss: 1494.68994140625, average loss 15.44032958984375\n",
      "Batch 3, Loss: 1482.20947265625, average loss 15.234251302083333\n",
      "Batch 4, Loss: 1460.964111328125, average loss 15.078098754882813\n",
      "Batch 5, Loss: 1508.3170166015625, average loss 15.079113037109375\n",
      "Batch 6, Loss: 1316.69140625, average loss 14.760413208007812\n",
      "Batch 7, Loss: 1422.123046875, average loss 14.683387102399553\n",
      "Batch 8, Loss: 1475.9862060546875, average loss 14.692946472167968\n",
      "Batch 9, Loss: 1562.266357421875, average loss 14.796248372395834\n",
      "Batch 10, Loss: 1414.8084716796875, average loss 14.731432006835938\n",
      "Total loss: 14636.018798828125\n",
      "Validation loss: 14.636018798828125\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 7\n",
      "\tInput: 9303634\tPrediction: 5\n",
      "Epoch 138, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1478.34130859375, average loss 14.7834130859375\n",
      "Batch 2, Loss: 1474.663330078125, average loss 14.765023193359376\n",
      "Batch 3, Loss: 1447.8416748046875, average loss 14.669487711588541\n",
      "Batch 4, Loss: 1500.5908203125, average loss 14.753592834472656\n",
      "Batch 5, Loss: 1305.15771484375, average loss 14.413189697265626\n",
      "Batch 6, Loss: 1412.3900146484375, average loss 14.364974772135417\n",
      "Batch 7, Loss: 1472.5458984375, average loss 14.416472516741072\n",
      "Batch 8, Loss: 1541.338623046875, average loss 14.541086730957032\n",
      "Batch 9, Loss: 1417.7242431640625, average loss 14.500659586588542\n",
      "Batch 10, Loss: 1564.605712890625, average loss 14.615199340820313\n",
      "Total loss: 16546.536376953125\n",
      "Validation loss: 16.546536376953124\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 77\n",
      "\tInput: 11002\tPrediction: 7\n",
      "Epoch 139, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1467.94580078125, average loss 14.6794580078125\n",
      "Batch 2, Loss: 1451.01806640625, average loss 14.5948193359375\n",
      "Batch 3, Loss: 1485.9376220703125, average loss 14.683004964192708\n",
      "Batch 4, Loss: 1295.458251953125, average loss 14.250899353027343\n",
      "Batch 5, Loss: 1404.199462890625, average loss 14.209118408203125\n",
      "Batch 6, Loss: 1448.930419921875, average loss 14.255816040039063\n",
      "Batch 7, Loss: 1533.11865234375, average loss 14.409440394810268\n",
      "Batch 8, Loss: 1392.80029296875, average loss 14.349260711669922\n",
      "Batch 9, Loss: 1551.6395263671875, average loss 14.478942328559027\n",
      "Batch 10, Loss: 1453.825927734375, average loss 14.4848740234375\n",
      "Total loss: 20105.569702148438\n",
      "Validation loss: 20.105569702148436\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 73\n",
      "\tInput: 6167476\tPrediction: 77\n",
      "Epoch 140, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1416.52197265625, average loss 14.1652197265625\n",
      "Batch 2, Loss: 1479.111083984375, average loss 14.478165283203126\n",
      "Batch 3, Loss: 1280.0216064453125, average loss 13.918848876953126\n",
      "Batch 4, Loss: 1416.4869384765625, average loss 13.98035400390625\n",
      "Batch 5, Loss: 1438.5501708984375, average loss 14.061383544921876\n",
      "Batch 6, Loss: 1525.43310546875, average loss 14.260208129882812\n",
      "Batch 7, Loss: 1394.6005859375, average loss 14.21532209123884\n",
      "Batch 8, Loss: 1542.13330078125, average loss 14.366073455810547\n",
      "Batch 9, Loss: 1468.57958984375, average loss 14.401598171657986\n",
      "Batch 10, Loss: 1434.93359375, average loss 14.396371948242187\n",
      "Total loss: 19793.607788085938\n",
      "Validation loss: 19.793607788085936\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 71\n",
      "\tInput: 21151199\tPrediction: 77\n",
      "Epoch 141, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1469.4703369140625, average loss 14.694703369140624\n",
      "Batch 2, Loss: 1271.59619140625, average loss 13.705332641601563\n",
      "Batch 3, Loss: 1376.637451171875, average loss 13.725679931640625\n",
      "Batch 4, Loss: 1447.060302734375, average loss 13.911910705566406\n",
      "Batch 5, Loss: 1512.0732421875, average loss 14.153675048828125\n",
      "Batch 6, Loss: 1371.997802734375, average loss 14.081392211914062\n",
      "Batch 7, Loss: 1539.8587646484375, average loss 14.26956298828125\n",
      "Batch 8, Loss: 1432.1591796875, average loss 14.27606658935547\n",
      "Batch 9, Loss: 1424.61376953125, average loss 14.272741156684027\n",
      "Batch 10, Loss: 1407.31005859375, average loss 14.252777099609375\n",
      "Total loss: 14270.665161132812\n",
      "Validation loss: 14.270665161132813\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: 74\n",
      "Epoch 142, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1255.18408203125, average loss 12.5518408203125\n",
      "Batch 2, Loss: 1361.4931640625, average loss 13.08338623046875\n",
      "Batch 3, Loss: 1409.44482421875, average loss 13.420406901041666\n",
      "Batch 4, Loss: 1492.7603759765625, average loss 13.797206115722656\n",
      "Batch 5, Loss: 1358.31396484375, average loss 13.754392822265626\n",
      "Batch 6, Loss: 1503.796142578125, average loss 13.968320922851563\n",
      "Batch 7, Loss: 1412.78173828125, average loss 13.99110613141741\n",
      "Batch 8, Loss: 1410.921142578125, average loss 14.005869293212891\n",
      "Batch 9, Loss: 1373.280029296875, average loss 13.975528293185764\n",
      "Batch 10, Loss: 1431.9884033203125, average loss 14.0099638671875\n",
      "Total loss: 14075.781860351562\n",
      "Validation loss: 14.075781860351562\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 4\n",
      "\tInput: 925\tPrediction: 7\n",
      "Epoch 143, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1341.778076171875, average loss 13.41778076171875\n",
      "Batch 2, Loss: 1395.88232421875, average loss 13.688302001953126\n",
      "Batch 3, Loss: 1477.2611083984375, average loss 14.049738362630208\n",
      "Batch 4, Loss: 1345.73046875, average loss 13.901629943847656\n",
      "Batch 5, Loss: 1493.3145751953125, average loss 14.10793310546875\n",
      "Batch 6, Loss: 1394.2449951171875, average loss 14.080352579752605\n",
      "Batch 7, Loss: 1395.630615234375, average loss 14.06263166155134\n",
      "Batch 8, Loss: 1362.396484375, average loss 14.007798309326171\n",
      "Batch 9, Loss: 1402.74560546875, average loss 14.009982503255209\n",
      "Batch 10, Loss: 1222.7191162109375, average loss 13.831703369140625\n",
      "Total loss: 14270.236206054688\n",
      "Validation loss: 14.270236206054687\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 17\n",
      "\tInput: 94164\tPrediction: 174\n",
      "Epoch 144, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1377.65771484375, average loss 13.7765771484375\n",
      "Batch 2, Loss: 1460.148193359375, average loss 14.189029541015625\n",
      "Batch 3, Loss: 1314.65087890625, average loss 13.841522623697916\n",
      "Batch 4, Loss: 1475.5281982421875, average loss 14.069962463378907\n",
      "Batch 5, Loss: 1371.258544921875, average loss 13.998487060546875\n",
      "Batch 6, Loss: 1386.6982421875, average loss 13.97656962076823\n",
      "Batch 7, Loss: 1349.50439453125, average loss 13.907780238560267\n",
      "Batch 8, Loss: 1387.204345703125, average loss 13.903313140869141\n",
      "Batch 9, Loss: 1204.8935546875, average loss 13.697271185980902\n",
      "Batch 10, Loss: 1308.37939453125, average loss 13.635923461914063\n",
      "Total loss: 14928.740234375\n",
      "Validation loss: 14.928740234375\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 17\n",
      "\tInput: 64009172\tPrediction: 47474\n",
      "Epoch 145, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1445.8427734375, average loss 14.458427734375\n",
      "Batch 2, Loss: 1301.54443359375, average loss 13.73693603515625\n",
      "Batch 3, Loss: 1462.851806640625, average loss 14.034130045572917\n",
      "Batch 4, Loss: 1345.391845703125, average loss 13.8890771484375\n",
      "Batch 5, Loss: 1354.004638671875, average loss 13.81927099609375\n",
      "Batch 6, Loss: 1320.5791015625, average loss 13.717024332682291\n",
      "Batch 7, Loss: 1364.6005859375, average loss 13.706878836495536\n",
      "Batch 8, Loss: 1175.2095947265625, average loss 13.462530975341798\n",
      "Batch 9, Loss: 1284.8819580078125, average loss 13.3943408203125\n",
      "Batch 10, Loss: 1331.447998046875, average loss 13.386354736328125\n",
      "Total loss: 13436.419799804688\n",
      "Validation loss: 13.436419799804687\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 87\n",
      "\tInput: 6824\tPrediction: 0\n",
      "Epoch 146, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1270.425537109375, average loss 12.70425537109375\n",
      "Batch 2, Loss: 1422.635009765625, average loss 13.465302734375\n",
      "Batch 3, Loss: 1315.233154296875, average loss 13.36097900390625\n",
      "Batch 4, Loss: 1333.5948486328125, average loss 13.354721374511719\n",
      "Batch 5, Loss: 1288.361572265625, average loss 13.260500244140625\n",
      "Batch 6, Loss: 1335.064208984375, average loss 13.275523885091145\n",
      "Batch 7, Loss: 1149.671875, average loss 13.021408865792411\n",
      "Batch 8, Loss: 1257.64453125, average loss 12.96578842163086\n",
      "Batch 9, Loss: 1307.5196533203125, average loss 12.977944878472222\n",
      "Batch 10, Loss: 1379.03125, average loss 13.059181640625\n",
      "Total loss: 15726.181274414062\n",
      "Validation loss: 15.726181274414062\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 87\n",
      "\tInput: 135\tPrediction: 777\n",
      "Epoch 147, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1390.416015625, average loss 13.90416015625\n",
      "Batch 2, Loss: 1280.48193359375, average loss 13.35448974609375\n",
      "Batch 3, Loss: 1300.663818359375, average loss 13.238539225260416\n",
      "Batch 4, Loss: 1268.936279296875, average loss 13.1012451171875\n",
      "Batch 5, Loss: 1303.58154296875, average loss 13.0881591796875\n",
      "Batch 6, Loss: 1131.326171875, average loss 12.792342936197917\n",
      "Batch 7, Loss: 1228.935791015625, average loss 12.72048793247768\n",
      "Batch 8, Loss: 1270.7098388671875, average loss 12.718814239501953\n",
      "Batch 9, Loss: 1355.22607421875, average loss 12.811419406467014\n",
      "Batch 10, Loss: 1198.05615234375, average loss 12.728333618164063\n",
      "Total loss: 17374.297119140625\n",
      "Validation loss: 17.374297119140625\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 744\n",
      "\tInput: 9303634\tPrediction: 207\n",
      "Epoch 148, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1274.3837890625, average loss 12.743837890625\n",
      "Batch 2, Loss: 1308.74267578125, average loss 12.91563232421875\n",
      "Batch 3, Loss: 1242.2020263671875, average loss 12.751094970703125\n",
      "Batch 4, Loss: 1280.6063232421875, average loss 12.764837036132812\n",
      "Batch 5, Loss: 1097.5460205078125, average loss 12.406961669921875\n",
      "Batch 6, Loss: 1191.82177734375, average loss 12.325504353841145\n",
      "Batch 7, Loss: 1230.6923828125, average loss 12.322849993024553\n",
      "Batch 8, Loss: 1299.34765625, average loss 12.406678314208984\n",
      "Batch 9, Loss: 1165.9781494140625, average loss 12.323689778645834\n",
      "Batch 10, Loss: 1344.53076171875, average loss 12.4358515625\n",
      "Total loss: 12135.014526367188\n",
      "Validation loss: 12.135014526367188\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 2848\n",
      "\tInput: 11002\tPrediction: 714\n",
      "Epoch 149, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1247.50634765625, average loss 12.4750634765625\n",
      "Batch 2, Loss: 1192.4227294921875, average loss 12.199645385742187\n",
      "Batch 3, Loss: 1229.7535400390625, average loss 12.232275390625\n",
      "Batch 4, Loss: 1065.73193359375, average loss 11.838536376953124\n",
      "Batch 5, Loss: 1161.903076171875, average loss 11.79463525390625\n",
      "Batch 6, Loss: 1221.6026611328125, average loss 11.864867146809896\n",
      "Batch 7, Loss: 1259.131591796875, average loss 11.968645542689732\n",
      "Batch 8, Loss: 1124.1796875, average loss 11.877789459228516\n",
      "Batch 9, Loss: 1296.2330322265625, average loss 11.998293999565972\n",
      "Batch 10, Loss: 1210.27880859375, average loss 12.008743408203125\n",
      "Total loss: 14422.175170898438\n",
      "Validation loss: 14.422175170898438\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 744\n",
      "\tInput: 6167476\tPrediction: 7447\n",
      "Epoch 150, lr=[0.00125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1164.7044677734375, average loss 11.647044677734375\n",
      "Batch 2, Loss: 1192.143310546875, average loss 11.784238891601563\n",
      "Batch 3, Loss: 1035.5062255859375, average loss 11.3078466796875\n",
      "Batch 4, Loss: 1119.0224609375, average loss 11.278441162109376\n",
      "Batch 5, Loss: 1163.388671875, average loss 11.3495302734375\n",
      "Batch 6, Loss: 1213.86865234375, average loss 11.481056315104167\n",
      "Batch 7, Loss: 1094.797607421875, average loss 11.404901994977678\n",
      "Batch 8, Loss: 1261.15966796875, average loss 11.555738830566407\n",
      "Batch 9, Loss: 1196.0914306640625, average loss 11.600758327907986\n",
      "Batch 10, Loss: 1159.3681640625, average loss 11.600050659179688\n",
      "Total loss: 14215.515380859375\n",
      "Validation loss: 14.215515380859374\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 74908\n",
      "\tInput: 21151199\tPrediction: 77744\n",
      "Epoch 151, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1247.007080078125, average loss 12.47007080078125\n",
      "Batch 2, Loss: 1004.1357421875, average loss 11.255714111328125\n",
      "Batch 3, Loss: 1092.121337890625, average loss 11.1442138671875\n",
      "Batch 4, Loss: 1176.50146484375, average loss 11.2994140625\n",
      "Batch 5, Loss: 1168.9130859375, average loss 11.377357421875\n",
      "Batch 6, Loss: 1054.710693359375, average loss 11.238982340494792\n",
      "Batch 7, Loss: 1256.386962890625, average loss 11.428251953125\n",
      "Batch 8, Loss: 1114.191650390625, average loss 11.392460021972656\n",
      "Batch 9, Loss: 1151.47998046875, average loss 11.406053331163195\n",
      "Batch 10, Loss: 1101.3499755859375, average loss 11.366797973632812\n",
      "Total loss: 11091.903442382812\n",
      "Validation loss: 11.091903442382813\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: \n",
      "\tInput: 76206\tPrediction: 7040\n",
      "Epoch 152, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 973.7652587890625, average loss 9.737652587890626\n",
      "Batch 2, Loss: 1062.517578125, average loss 10.181414184570313\n",
      "Batch 3, Loss: 1110.7950439453125, average loss 10.490259602864583\n",
      "Batch 4, Loss: 1161.5615234375, average loss 10.771598510742187\n",
      "Batch 5, Loss: 1076.2576904296875, average loss 10.769794189453124\n",
      "Batch 6, Loss: 1219.6827392578125, average loss 11.007633056640625\n",
      "Batch 7, Loss: 1088.3883056640625, average loss 10.989954485212053\n",
      "Batch 8, Loss: 1105.736572265625, average loss 10.998380889892578\n",
      "Batch 9, Loss: 1098.339599609375, average loss 10.996715901692708\n",
      "Batch 10, Loss: 1087.2232666015625, average loss 10.984267578125\n",
      "Total loss: 11432.68310546875\n",
      "Validation loss: 11.43268310546875\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 9242048\n",
      "\tInput: 925\tPrediction: 09\n",
      "Epoch 153, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1011.92919921875, average loss 10.1192919921875\n",
      "Batch 2, Loss: 1072.4249267578125, average loss 10.421770629882813\n",
      "Batch 3, Loss: 1115.40576171875, average loss 10.665866292317709\n",
      "Batch 4, Loss: 1030.06640625, average loss 10.574565734863281\n",
      "Batch 5, Loss: 1162.635986328125, average loss 10.784924560546875\n",
      "Batch 6, Loss: 1066.7943115234375, average loss 10.765427652994791\n",
      "Batch 7, Loss: 1075.7734375, average loss 10.76432861328125\n",
      "Batch 8, Loss: 1037.045654296875, average loss 10.715094604492187\n",
      "Batch 9, Loss: 1061.972412109375, average loss 10.704497884114584\n",
      "Batch 10, Loss: 919.58056640625, average loss 10.553628662109375\n",
      "Total loss: 10597.430908203125\n",
      "Validation loss: 10.597430908203124\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 907227\n",
      "\tInput: 94164\tPrediction: 40744\n",
      "Epoch 154, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1039.6507568359375, average loss 10.396507568359375\n",
      "Batch 2, Loss: 1107.02587890625, average loss 10.733383178710938\n",
      "Batch 3, Loss: 985.9898681640625, average loss 10.4422216796875\n",
      "Batch 4, Loss: 1145.7015380859375, average loss 10.695920104980468\n",
      "Batch 5, Loss: 1040.2587890625, average loss 10.637253662109375\n",
      "Batch 6, Loss: 1058.684326171875, average loss 10.628851928710937\n",
      "Batch 7, Loss: 1034.1597900390625, average loss 10.587815638950893\n",
      "Batch 8, Loss: 1044.9949951171875, average loss 10.570582427978515\n",
      "Batch 9, Loss: 907.8038330078125, average loss 10.404744194878472\n",
      "Batch 10, Loss: 962.094970703125, average loss 10.32636474609375\n",
      "Total loss: 10083.474914550781\n",
      "Validation loss: 10.08347491455078\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 04\n",
      "\tInput: 64009172\tPrediction: 00004\n",
      "Epoch 155, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1061.174560546875, average loss 10.61174560546875\n",
      "Batch 2, Loss: 955.052978515625, average loss 10.0811376953125\n",
      "Batch 3, Loss: 1094.069580078125, average loss 10.367657063802083\n",
      "Batch 4, Loss: 1003.0165405273438, average loss 10.283284149169923\n",
      "Batch 5, Loss: 1037.9088134765625, average loss 10.302444946289063\n",
      "Batch 6, Loss: 990.7081298828125, average loss 10.236551005045573\n",
      "Batch 7, Loss: 1012.313720703125, average loss 10.22034903390067\n",
      "Batch 8, Loss: 862.583740234375, average loss 10.021035079956055\n",
      "Batch 9, Loss: 932.184326171875, average loss 9.94334710015191\n",
      "Batch 10, Loss: 988.5611572265625, average loss 9.93757354736328\n",
      "Total loss: 9878.08642578125\n",
      "Validation loss: 9.87808642578125\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 98716476\n",
      "\tInput: 6824\tPrediction: 980\n",
      "Epoch 156, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 951.67529296875, average loss 9.5167529296875\n",
      "Batch 2, Loss: 1072.53759765625, average loss 10.121064453125\n",
      "Batch 3, Loss: 972.8305053710938, average loss 9.990144653320312\n",
      "Batch 4, Loss: 1005.5701904296875, average loss 10.006533966064453\n",
      "Batch 5, Loss: 975.9151611328125, average loss 9.957057495117187\n",
      "Batch 6, Loss: 978.638916015625, average loss 9.928612772623698\n",
      "Batch 7, Loss: 852.02001953125, average loss 9.727410975864956\n",
      "Batch 8, Loss: 918.8560791015625, average loss 9.660054702758789\n",
      "Batch 9, Loss: 957.808837890625, average loss 9.65094733344184\n",
      "Batch 10, Loss: 1005.7730712890625, average loss 9.691625671386719\n",
      "Total loss: 9618.727416992188\n",
      "Validation loss: 9.618727416992188\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9888881\n",
      "\tInput: 135\tPrediction: 488\n",
      "Epoch 157, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 1071.4189453125, average loss 10.714189453125\n",
      "Batch 2, Loss: 969.248046875, average loss 10.2033349609375\n",
      "Batch 3, Loss: 989.3040771484375, average loss 10.099903564453125\n",
      "Batch 4, Loss: 996.171875, average loss 10.065357360839844\n",
      "Batch 5, Loss: 952.7149658203125, average loss 9.9577158203125\n",
      "Batch 6, Loss: 871.6083984375, average loss 9.750777180989584\n",
      "Batch 7, Loss: 890.8582763671875, average loss 9.630463692801339\n",
      "Batch 8, Loss: 960.8914794921875, average loss 9.627770080566407\n",
      "Batch 9, Loss: 1004.65185546875, average loss 9.674297688802083\n",
      "Batch 10, Loss: 882.7255859375, average loss 9.589593505859375\n",
      "Total loss: 11578.5634765625\n",
      "Validation loss: 11.5785634765625\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 603800\n",
      "\tInput: 9303634\tPrediction: 989890\n",
      "Epoch 158, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 957.6126708984375, average loss 9.576126708984376\n",
      "Batch 2, Loss: 976.0595703125, average loss 9.668361206054687\n",
      "Batch 3, Loss: 926.565673828125, average loss 9.534126383463542\n",
      "Batch 4, Loss: 962.911376953125, average loss 9.55787322998047\n",
      "Batch 5, Loss: 826.9365234375, average loss 9.300171630859374\n",
      "Batch 6, Loss: 874.772705078125, average loss 9.208097534179688\n",
      "Batch 7, Loss: 925.62646484375, average loss 9.214978550502233\n",
      "Batch 8, Loss: 962.3738403320312, average loss 9.266073532104492\n",
      "Batch 9, Loss: 874.7401733398438, average loss 9.208443332248264\n",
      "Batch 10, Loss: 993.2879638671875, average loss 9.280886962890625\n",
      "Total loss: 9396.10498046875\n",
      "Validation loss: 9.39610498046875\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 2188374480\n",
      "\tInput: 11002\tPrediction: 77240\n",
      "Epoch 159, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 941.0372314453125, average loss 9.410372314453125\n",
      "Batch 2, Loss: 912.3861083984375, average loss 9.26711669921875\n",
      "Batch 3, Loss: 916.7359008789062, average loss 9.233864135742188\n",
      "Batch 4, Loss: 794.3369140625, average loss 8.911240386962891\n",
      "Batch 5, Loss: 844.2078857421875, average loss 8.817408081054687\n",
      "Batch 6, Loss: 908.3074951171875, average loss 8.861685892740885\n",
      "Batch 7, Loss: 930.1954345703125, average loss 8.924581386021206\n",
      "Batch 8, Loss: 867.51611328125, average loss 8.893403854370117\n",
      "Batch 9, Loss: 984.0247802734375, average loss 8.998608737521701\n",
      "Batch 10, Loss: 898.8582153320312, average loss 8.997606079101562\n",
      "Total loss: 9029.177001953125\n",
      "Validation loss: 9.029177001953125\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 91245984\n",
      "\tInput: 6167476\tPrediction: 9794409\n",
      "Epoch 160, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 876.4566650390625, average loss 8.764566650390625\n",
      "Batch 2, Loss: 900.521240234375, average loss 8.884889526367187\n",
      "Batch 3, Loss: 782.9456787109375, average loss 8.53307861328125\n",
      "Batch 4, Loss: 839.4453125, average loss 8.498422241210937\n",
      "Batch 5, Loss: 865.3521728515625, average loss 8.529442138671875\n",
      "Batch 6, Loss: 944.24658203125, average loss 8.681612752278646\n",
      "Batch 7, Loss: 844.7374267578125, average loss 8.648150111607142\n",
      "Batch 8, Loss: 946.6937255859375, average loss 8.750498504638673\n",
      "Batch 9, Loss: 875.40869140625, average loss 8.750897216796876\n",
      "Batch 10, Loss: 897.1129150390625, average loss 8.77292041015625\n",
      "Total loss: 8696.719360351562\n",
      "Validation loss: 8.696719360351562\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 498338\n",
      "\tInput: 21151199\tPrediction: 943400\n",
      "Epoch 161, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 879.873291015625, average loss 8.79873291015625\n",
      "Batch 2, Loss: 762.2250366210938, average loss 8.210491638183594\n",
      "Batch 3, Loss: 816.1685791015625, average loss 8.194223022460937\n",
      "Batch 4, Loss: 867.7923583984375, average loss 8.315148162841798\n",
      "Batch 5, Loss: 910.7667236328125, average loss 8.473651977539063\n",
      "Batch 6, Loss: 837.6546630859375, average loss 8.457467753092448\n",
      "Batch 7, Loss: 940.38232421875, average loss 8.59266139439174\n",
      "Batch 8, Loss: 857.5855102539062, average loss 8.590560607910156\n",
      "Batch 9, Loss: 884.3411254882812, average loss 8.618655124240451\n",
      "Batch 10, Loss: 832.7425537109375, average loss 8.589532165527343\n",
      "Total loss: 12258.846923828125\n",
      "Validation loss: 12.258846923828125\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 01\n",
      "\tInput: 76206\tPrediction: 472002\n",
      "Epoch 162, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 739.081298828125, average loss 7.39081298828125\n",
      "Batch 2, Loss: 804.1319580078125, average loss 7.716066284179687\n",
      "Batch 3, Loss: 847.7567138671875, average loss 7.96989990234375\n",
      "Batch 4, Loss: 896.785888671875, average loss 8.2193896484375\n",
      "Batch 5, Loss: 789.0697631835938, average loss 8.153651245117187\n",
      "Batch 6, Loss: 934.8505249023438, average loss 8.352793579101563\n",
      "Batch 7, Loss: 830.62841796875, average loss 8.346149379185269\n",
      "Batch 8, Loss: 874.6535034179688, average loss 8.39619758605957\n",
      "Batch 9, Loss: 787.1319580078125, average loss 8.337877807617188\n",
      "Batch 10, Loss: 850.0936279296875, average loss 8.354183654785157\n",
      "Total loss: 9416.855224609375\n",
      "Validation loss: 9.416855224609375\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8930800819\n",
      "\tInput: 925\tPrediction: 909\n",
      "Epoch 163, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 778.1812744140625, average loss 7.781812744140625\n",
      "Batch 2, Loss: 826.2826538085938, average loss 8.022319641113281\n",
      "Batch 3, Loss: 855.579833984375, average loss 8.200145874023438\n",
      "Batch 4, Loss: 797.5453491210938, average loss 8.143972778320313\n",
      "Batch 5, Loss: 887.2064208984375, average loss 8.289591064453125\n",
      "Batch 6, Loss: 832.5687866210938, average loss 8.295607198079427\n",
      "Batch 7, Loss: 862.4502563476562, average loss 8.342592250279019\n",
      "Batch 8, Loss: 806.2103881835938, average loss 8.307531204223633\n",
      "Batch 9, Loss: 819.3228759765625, average loss 8.294830932617188\n",
      "Batch 10, Loss: 710.947265625, average loss 8.176295104980468\n",
      "Total loss: 10567.244445800781\n",
      "Validation loss: 10.567244445800782\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 807394\n",
      "\tInput: 94164\tPrediction: 9099\n",
      "Epoch 164, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 823.5196533203125, average loss 8.235196533203125\n",
      "Batch 2, Loss: 846.4027099609375, average loss 8.34961181640625\n",
      "Batch 3, Loss: 792.869140625, average loss 8.209305013020833\n",
      "Batch 4, Loss: 879.9990844726562, average loss 8.356976470947266\n",
      "Batch 5, Loss: 816.1049194335938, average loss 8.317791015625\n",
      "Batch 6, Loss: 847.017822265625, average loss 8.343188883463542\n",
      "Batch 7, Loss: 780.1773071289062, average loss 8.265843767438616\n",
      "Batch 8, Loss: 838.75341796875, average loss 8.281055068969726\n",
      "Batch 9, Loss: 699.9249267578125, average loss 8.138632202148438\n",
      "Batch 10, Loss: 792.0675048828125, average loss 8.116836486816407\n",
      "Total loss: 7771.050720214844\n",
      "Validation loss: 7.771050720214844\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 204909444\n",
      "Epoch 165, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 827.5764770507812, average loss 8.275764770507813\n",
      "Batch 2, Loss: 822.716064453125, average loss 8.251462707519531\n",
      "Batch 3, Loss: 887.1226806640625, average loss 8.458050740559896\n",
      "Batch 4, Loss: 793.63037109375, average loss 8.327613983154297\n",
      "Batch 5, Loss: 864.7342529296875, average loss 8.391559692382813\n",
      "Batch 6, Loss: 772.4340209960938, average loss 8.2803564453125\n",
      "Batch 7, Loss: 810.2134399414062, average loss 8.254896153041294\n",
      "Batch 8, Loss: 732.8133544921875, average loss 8.139050827026367\n",
      "Batch 9, Loss: 739.2697143554688, average loss 8.056122639973958\n",
      "Batch 10, Loss: 778.2612915039062, average loss 8.028771667480468\n",
      "Total loss: 7706.993347167969\n",
      "Validation loss: 7.706993347167969\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 088410276\n",
      "\tInput: 6824\tPrediction: 6600\n",
      "Epoch 166, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 745.08740234375, average loss 7.4508740234375\n",
      "Batch 2, Loss: 862.5023803710938, average loss 8.037948913574219\n",
      "Batch 3, Loss: 789.9351196289062, average loss 7.991749674479166\n",
      "Batch 4, Loss: 788.7461547851562, average loss 7.965677642822266\n",
      "Batch 5, Loss: 777.1550903320312, average loss 7.926852294921875\n",
      "Batch 6, Loss: 777.8931884765625, average loss 7.9021988932291665\n",
      "Batch 7, Loss: 663.0248413085938, average loss 7.720491681780134\n",
      "Batch 8, Loss: 738.9288330078125, average loss 7.679091262817383\n",
      "Batch 9, Loss: 762.6697998046875, average loss 7.673269788953993\n",
      "Batch 10, Loss: 796.872314453125, average loss 7.7028151245117185\n",
      "Total loss: 7880.695251464844\n",
      "Validation loss: 7.880695251464844\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9538637\n",
      "\tInput: 135\tPrediction: 135\n",
      "Epoch 167, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 849.4435424804688, average loss 8.494435424804687\n",
      "Batch 2, Loss: 761.362548828125, average loss 8.054030456542968\n",
      "Batch 3, Loss: 798.678466796875, average loss 8.031615193684896\n",
      "Batch 4, Loss: 728.41552734375, average loss 7.844750213623047\n",
      "Batch 5, Loss: 782.774169921875, average loss 7.841348510742187\n",
      "Batch 6, Loss: 681.7161865234375, average loss 7.670650736490885\n",
      "Batch 7, Loss: 706.5648193359375, average loss 7.5842218017578125\n",
      "Batch 8, Loss: 767.6771240234375, average loss 7.5957904815673825\n",
      "Batch 9, Loss: 786.8280029296875, average loss 7.626067097981771\n",
      "Batch 10, Loss: 725.7152709960938, average loss 7.589175659179688\n",
      "Total loss: 7722.44677734375\n",
      "Validation loss: 7.72244677734375\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909540\n",
      "\tInput: 9303634\tPrediction: 0949994\n",
      "Epoch 168, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 755.745361328125, average loss 7.55745361328125\n",
      "Batch 2, Loss: 797.5980224609375, average loss 7.7667169189453125\n",
      "Batch 3, Loss: 790.2078247070312, average loss 7.811837361653645\n",
      "Batch 4, Loss: 749.1416015625, average loss 7.731732025146484\n",
      "Batch 5, Loss: 693.4827880859375, average loss 7.572351196289063\n",
      "Batch 6, Loss: 711.116943359375, average loss 7.495487569173177\n",
      "Batch 7, Loss: 733.6629638671875, average loss 7.4727935791015625\n",
      "Batch 8, Loss: 793.6029663085938, average loss 7.53069808959961\n",
      "Batch 9, Loss: 699.2760009765625, average loss 7.4709271918402775\n",
      "Batch 10, Loss: 817.2135009765625, average loss 7.541047973632812\n",
      "Total loss: 7559.9107666015625\n",
      "Validation loss: 7.559910766601562\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7216564480\n",
      "\tInput: 11002\tPrediction: 14990\n",
      "Epoch 169, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 797.93701171875, average loss 7.9793701171875\n",
      "Batch 2, Loss: 723.8854370117188, average loss 7.609112243652344\n",
      "Batch 3, Loss: 752.304931640625, average loss 7.580424601236979\n",
      "Batch 4, Loss: 664.1639404296875, average loss 7.3457283020019535\n",
      "Batch 5, Loss: 687.6727905273438, average loss 7.25192822265625\n",
      "Batch 6, Loss: 735.0760498046875, average loss 7.268400268554688\n",
      "Batch 7, Loss: 768.6300659179688, average loss 7.3281003243582585\n",
      "Batch 8, Loss: 695.8983764648438, average loss 7.2819607543945315\n",
      "Batch 9, Loss: 811.387451171875, average loss 7.374395616319444\n",
      "Batch 10, Loss: 731.0718383789062, average loss 7.368027893066406\n",
      "Total loss: 7251.7431640625\n",
      "Validation loss: 7.2517431640625\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 949459674\n",
      "\tInput: 6167476\tPrediction: 61944079\n",
      "Epoch 170, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 703.0518798828125, average loss 7.030518798828125\n",
      "Batch 2, Loss: 766.418701171875, average loss 7.347352905273437\n",
      "Batch 3, Loss: 646.1141357421875, average loss 7.051949055989583\n",
      "Batch 4, Loss: 695.8348388671875, average loss 7.028548889160156\n",
      "Batch 5, Loss: 722.8626708984375, average loss 7.068564453125\n",
      "Batch 6, Loss: 747.5126953125, average loss 7.136324869791666\n",
      "Batch 7, Loss: 738.0459594726562, average loss 7.171201259068081\n",
      "Batch 8, Loss: 822.0112915039062, average loss 7.3023152160644536\n",
      "Batch 9, Loss: 718.7001953125, average loss 7.289502631293403\n",
      "Batch 10, Loss: 736.323486328125, average loss 7.296875854492187\n",
      "Total loss: 8517.452514648438\n",
      "Validation loss: 8.517452514648438\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 422698\n",
      "\tInput: 21151199\tPrediction: 21491742\n",
      "Epoch 171, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 742.376708984375, average loss 7.42376708984375\n",
      "Batch 2, Loss: 633.727783203125, average loss 6.8805224609375\n",
      "Batch 3, Loss: 673.8436279296875, average loss 6.833160400390625\n",
      "Batch 4, Loss: 721.657470703125, average loss 6.929013977050781\n",
      "Batch 5, Loss: 728.9730224609375, average loss 7.0011572265625\n",
      "Batch 6, Loss: 673.9822998046875, average loss 6.957601521809896\n",
      "Batch 7, Loss: 794.4503784179688, average loss 7.098587559291294\n",
      "Batch 8, Loss: 684.410888671875, average loss 7.066777725219726\n",
      "Batch 9, Loss: 747.8585815429688, average loss 7.1125341796875\n",
      "Batch 10, Loss: 691.344970703125, average loss 7.092625732421875\n",
      "Total loss: 7055.353210449219\n",
      "Validation loss: 7.055353210449219\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 01\n",
      "\tInput: 76206\tPrediction: 2764749\n",
      "Epoch 172, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 632.866455078125, average loss 6.32866455078125\n",
      "Batch 2, Loss: 659.52734375, average loss 6.461968994140625\n",
      "Batch 3, Loss: 692.3385009765625, average loss 6.6157743326822915\n",
      "Batch 4, Loss: 725.10791015625, average loss 6.774600524902343\n",
      "Batch 5, Loss: 679.828125, average loss 6.779336669921875\n",
      "Batch 6, Loss: 777.7490234375, average loss 6.9456955973307295\n",
      "Batch 7, Loss: 704.4877319335938, average loss 6.959864414760045\n",
      "Batch 8, Loss: 750.6568603515625, average loss 7.0282024383544925\n",
      "Batch 9, Loss: 692.2146606445312, average loss 7.01641845703125\n",
      "Batch 10, Loss: 698.26123046875, average loss 7.013037841796875\n",
      "Total loss: 6891.399353027344\n",
      "Validation loss: 6.891399353027344\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600510\n",
      "\tInput: 925\tPrediction: 209\n",
      "Epoch 173, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 633.0931396484375, average loss 6.330931396484375\n",
      "Batch 2, Loss: 685.4344482421875, average loss 6.592637939453125\n",
      "Batch 3, Loss: 721.8087158203125, average loss 6.801121012369792\n",
      "Batch 4, Loss: 655.0596313476562, average loss 6.738489837646484\n",
      "Batch 5, Loss: 752.639892578125, average loss 6.896071655273437\n",
      "Batch 6, Loss: 673.05322265625, average loss 6.868481750488281\n",
      "Batch 7, Loss: 708.866455078125, average loss 6.8999364362444195\n",
      "Batch 8, Loss: 694.390625, average loss 6.905432662963867\n",
      "Batch 9, Loss: 712.2039794921875, average loss 6.929500122070312\n",
      "Batch 10, Loss: 604.88818359375, average loss 6.841438293457031\n",
      "Total loss: 7056.398986816406\n",
      "Validation loss: 7.056398986816406\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607907\n",
      "\tInput: 94164\tPrediction: 60050\n",
      "Epoch 174, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 664.9061279296875, average loss 6.649061279296875\n",
      "Batch 2, Loss: 724.172119140625, average loss 6.945391235351562\n",
      "Batch 3, Loss: 651.8496704101562, average loss 6.803093058268229\n",
      "Batch 4, Loss: 732.0550537109375, average loss 6.932457427978516\n",
      "Batch 5, Loss: 668.0206298828125, average loss 6.882007202148437\n",
      "Batch 6, Loss: 717.753662109375, average loss 6.93126210530599\n",
      "Batch 7, Loss: 666.7283935546875, average loss 6.893550938197545\n",
      "Batch 8, Loss: 685.7670288085938, average loss 6.889065856933594\n",
      "Batch 9, Loss: 596.8936157226562, average loss 6.786829223632813\n",
      "Batch 10, Loss: 656.0842895507812, average loss 6.764230590820312\n",
      "Total loss: 7242.516296386719\n",
      "Validation loss: 7.242516296386719\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 90900000\n",
      "Epoch 175, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 717.9759521484375, average loss 7.179759521484375\n",
      "Batch 2, Loss: 664.6900634765625, average loss 6.913330078125\n",
      "Batch 3, Loss: 748.64453125, average loss 7.104368489583333\n",
      "Batch 4, Loss: 661.7587890625, average loss 6.98267333984375\n",
      "Batch 5, Loss: 709.240966796875, average loss 7.00462060546875\n",
      "Batch 6, Loss: 688.2809448242188, average loss 6.98431874593099\n",
      "Batch 7, Loss: 671.5298461914062, average loss 6.945887276785714\n",
      "Batch 8, Loss: 604.458984375, average loss 6.83322509765625\n",
      "Batch 9, Loss: 625.439208984375, average loss 6.768910319010416\n",
      "Batch 10, Loss: 682.38671875, average loss 6.774406005859375\n",
      "Total loss: 9802.462280273438\n",
      "Validation loss: 9.802462280273437\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 938170348\n",
      "\tInput: 6824\tPrediction: 776800\n",
      "Epoch 176, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 621.2478637695312, average loss 6.2124786376953125\n",
      "Batch 2, Loss: 722.59521484375, average loss 6.719215393066406\n",
      "Batch 3, Loss: 669.28173828125, average loss 6.710416056315104\n",
      "Batch 4, Loss: 692.3865966796875, average loss 6.763778533935547\n",
      "Batch 5, Loss: 633.4114990234375, average loss 6.677845825195313\n",
      "Batch 6, Loss: 657.3212890625, average loss 6.660407002766927\n",
      "Batch 7, Loss: 591.4292602539062, average loss 6.553819231305804\n",
      "Batch 8, Loss: 600.5275268554688, average loss 6.485251235961914\n",
      "Batch 9, Loss: 638.0770263671875, average loss 6.473642239040799\n",
      "Batch 10, Loss: 701.0255126953125, average loss 6.527303527832031\n",
      "Total loss: 6706.072509765625\n",
      "Validation loss: 6.706072509765625\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 67368831\n",
      "\tInput: 135\tPrediction: 163\n",
      "Epoch 177, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 727.9168701171875, average loss 7.279168701171875\n",
      "Batch 2, Loss: 653.458740234375, average loss 6.906878051757812\n",
      "Batch 3, Loss: 662.6864013671875, average loss 6.8135400390625\n",
      "Batch 4, Loss: 638.635986328125, average loss 6.706744995117187\n",
      "Batch 5, Loss: 658.8561401367188, average loss 6.683108276367188\n",
      "Batch 6, Loss: 573.4520874023438, average loss 6.525010375976563\n",
      "Batch 7, Loss: 603.4154663085938, average loss 6.454888131277902\n",
      "Batch 8, Loss: 651.9940185546875, average loss 6.463019638061524\n",
      "Batch 9, Loss: 677.587158203125, average loss 6.49778096516927\n",
      "Batch 10, Loss: 618.083251953125, average loss 6.4660861206054685\n",
      "Total loss: 6274.562438964844\n",
      "Validation loss: 6.274562438964844\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909540\n",
      "\tInput: 9303634\tPrediction: 2929990\n",
      "Epoch 178, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 648.012451171875, average loss 6.48012451171875\n",
      "Batch 2, Loss: 661.4032592773438, average loss 6.547078552246094\n",
      "Batch 3, Loss: 621.228271484375, average loss 6.435479939778646\n",
      "Batch 4, Loss: 649.6404418945312, average loss 6.450711059570312\n",
      "Batch 5, Loss: 562.9695434570312, average loss 6.286507934570312\n",
      "Batch 6, Loss: 601.937744140625, average loss 6.241986185709635\n",
      "Batch 7, Loss: 642.70703125, average loss 6.2684267752511165\n",
      "Batch 8, Loss: 652.961669921875, average loss 6.30107551574707\n",
      "Batch 9, Loss: 592.0730590820312, average loss 6.258814968532986\n",
      "Batch 10, Loss: 704.9696044921875, average loss 6.337903076171875\n",
      "Total loss: 6401.9345703125\n",
      "Validation loss: 6.4019345703125\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7216664460\n",
      "\tInput: 11002\tPrediction: 14200\n",
      "Epoch 179, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 675.0972290039062, average loss 6.750972290039062\n",
      "Batch 2, Loss: 605.0533447265625, average loss 6.400752868652344\n",
      "Batch 3, Loss: 625.957763671875, average loss 6.353694458007812\n",
      "Batch 4, Loss: 550.1175537109375, average loss 6.140564727783203\n",
      "Batch 5, Loss: 582.4940185546875, average loss 6.077439819335938\n",
      "Batch 6, Loss: 605.7462158203125, average loss 6.074110209147135\n",
      "Batch 7, Loss: 638.9369506835938, average loss 6.119147251674107\n",
      "Batch 8, Loss: 569.7359619140625, average loss 6.066423797607422\n",
      "Batch 9, Loss: 709.3348388671875, average loss 6.180526529947917\n",
      "Batch 10, Loss: 603.5694580078125, average loss 6.166043334960937\n",
      "Total loss: 6402.673889160156\n",
      "Validation loss: 6.402673889160156\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94246934\n",
      "\tInput: 6167476\tPrediction: 2347479\n",
      "Epoch 180, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 603.3670654296875, average loss 6.033670654296875\n",
      "Batch 2, Loss: 614.206298828125, average loss 6.0878668212890625\n",
      "Batch 3, Loss: 542.9519653320312, average loss 5.868417765299479\n",
      "Batch 4, Loss: 574.1368408203125, average loss 5.8366554260253904\n",
      "Batch 5, Loss: 623.607666015625, average loss 5.916539672851562\n",
      "Batch 6, Loss: 637.8984985351562, average loss 5.993613891601562\n",
      "Batch 7, Loss: 576.95166015625, average loss 5.961599993024554\n",
      "Batch 8, Loss: 685.2655029296875, average loss 6.072981872558594\n",
      "Batch 9, Loss: 610.618896484375, average loss 6.076671549479166\n",
      "Batch 10, Loss: 645.927734375, average loss 6.11493212890625\n",
      "Total loss: 5984.615417480469\n",
      "Validation loss: 5.984615417480469\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 128368\n",
      "\tInput: 21151199\tPrediction: 21164409\n",
      "Epoch 181, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 595.0140380859375, average loss 5.950140380859375\n",
      "Batch 2, Loss: 541.3165283203125, average loss 5.68165283203125\n",
      "Batch 3, Loss: 565.8467407226562, average loss 5.673924357096354\n",
      "Batch 4, Loss: 624.2366943359375, average loss 5.816035003662109\n",
      "Batch 5, Loss: 630.3051147460938, average loss 5.913438232421875\n",
      "Batch 6, Loss: 578.9400634765625, average loss 5.892765299479167\n",
      "Batch 7, Loss: 700.8825073242188, average loss 6.052202410016741\n",
      "Batch 8, Loss: 607.47705078125, average loss 6.055023422241211\n",
      "Batch 9, Loss: 639.5509033203125, average loss 6.092855156792535\n",
      "Batch 10, Loss: 578.6668090820312, average loss 6.0622364501953125\n",
      "Total loss: 6319.129699707031\n",
      "Validation loss: 6.319129699707031\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 276299\n",
      "Epoch 182, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 530.5359497070312, average loss 5.305359497070312\n",
      "Batch 2, Loss: 556.89013671875, average loss 5.437130432128907\n",
      "Batch 3, Loss: 589.371337890625, average loss 5.589324747721355\n",
      "Batch 4, Loss: 594.9251708984375, average loss 5.67930648803711\n",
      "Batch 5, Loss: 574.1641235351562, average loss 5.6917734375\n",
      "Batch 6, Loss: 662.2869873046875, average loss 5.846956176757812\n",
      "Batch 7, Loss: 596.2532958984375, average loss 5.863467145647322\n",
      "Batch 8, Loss: 647.9217529296875, average loss 5.940435943603515\n",
      "Batch 9, Loss: 551.8317260742188, average loss 5.893533867730035\n",
      "Batch 10, Loss: 616.7081298828125, average loss 5.920888610839844\n",
      "Total loss: 6266.276062011719\n",
      "Validation loss: 6.266276062011719\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 06\n",
      "Epoch 183, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 539.8861083984375, average loss 5.398861083984375\n",
      "Batch 2, Loss: 581.9014892578125, average loss 5.60893798828125\n",
      "Batch 3, Loss: 594.9654541015625, average loss 5.722510172526042\n",
      "Batch 4, Loss: 547.006103515625, average loss 5.659397888183594\n",
      "Batch 5, Loss: 636.8323364257812, average loss 5.801182983398437\n",
      "Batch 6, Loss: 557.8554077148438, average loss 5.764078165690104\n",
      "Batch 7, Loss: 608.525634765625, average loss 5.809960763113839\n",
      "Batch 8, Loss: 530.2158203125, average loss 5.746485443115234\n",
      "Batch 9, Loss: 577.8111572265625, average loss 5.749999457465278\n",
      "Batch 10, Loss: 521.9812622070312, average loss 5.696980773925781\n",
      "Total loss: 5774.215637207031\n",
      "Validation loss: 5.774215637207031\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607907\n",
      "\tInput: 94164\tPrediction: 102490\n",
      "Epoch 184, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 565.7545776367188, average loss 5.657545776367187\n",
      "Batch 2, Loss: 602.1715698242188, average loss 5.839630737304687\n",
      "Batch 3, Loss: 534.7940063476562, average loss 5.6757338460286455\n",
      "Batch 4, Loss: 636.6734619140625, average loss 5.848484039306641\n",
      "Batch 5, Loss: 581.549560546875, average loss 5.8418863525390625\n",
      "Batch 6, Loss: 588.17578125, average loss 5.848531595865885\n",
      "Batch 7, Loss: 561.7611694335938, average loss 5.815543038504464\n",
      "Batch 8, Loss: 592.3675537109375, average loss 5.829059600830078\n",
      "Batch 9, Loss: 529.4803466796875, average loss 5.769697808159722\n",
      "Batch 10, Loss: 548.8228759765625, average loss 5.741550903320313\n",
      "Total loss: 6189.357238769531\n",
      "Validation loss: 6.1893572387695315\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 1427291470\n",
      "Epoch 185, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 610.6388549804688, average loss 6.106388549804688\n",
      "Batch 2, Loss: 514.042236328125, average loss 5.623405456542969\n",
      "Batch 3, Loss: 628.8472900390625, average loss 5.845094604492187\n",
      "Batch 4, Loss: 567.698486328125, average loss 5.803067169189453\n",
      "Batch 5, Loss: 577.1976318359375, average loss 5.796848999023437\n",
      "Batch 6, Loss: 553.31591796875, average loss 5.752900695800781\n",
      "Batch 7, Loss: 582.22314453125, average loss 5.76280508858817\n",
      "Batch 8, Loss: 495.0614929199219, average loss 5.6612813186645505\n",
      "Batch 9, Loss: 550.7999877929688, average loss 5.644250047471788\n",
      "Batch 10, Loss: 540.5706176757812, average loss 5.620395660400391\n",
      "Total loss: 5730.4073486328125\n",
      "Validation loss: 5.730407348632813\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172478\n",
      "\tInput: 6824\tPrediction: 6609\n",
      "Epoch 186, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 541.29248046875, average loss 5.4129248046875\n",
      "Batch 2, Loss: 630.3914794921875, average loss 5.858419799804688\n",
      "Batch 3, Loss: 555.1973876953125, average loss 5.756271158854167\n",
      "Batch 4, Loss: 609.7677001953125, average loss 5.8416226196289065\n",
      "Batch 5, Loss: 529.7598876953125, average loss 5.73281787109375\n",
      "Batch 6, Loss: 573.850830078125, average loss 5.733766276041667\n",
      "Batch 7, Loss: 519.8948974609375, average loss 5.657363804408482\n",
      "Batch 8, Loss: 536.231201171875, average loss 5.6204823303222655\n",
      "Batch 9, Loss: 521.5086669921875, average loss 5.575438368055556\n",
      "Batch 10, Loss: 579.833740234375, average loss 5.597728271484375\n",
      "Total loss: 5393.566650390625\n",
      "Validation loss: 5.393566650390625\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9338531\n",
      "\tInput: 135\tPrediction: 468\n",
      "Epoch 187, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 646.90625, average loss 6.4690625\n",
      "Batch 2, Loss: 575.6821899414062, average loss 6.112942199707032\n",
      "Batch 3, Loss: 626.9207153320312, average loss 6.165030517578125\n",
      "Batch 4, Loss: 535.4298706054688, average loss 5.962347564697265\n",
      "Batch 5, Loss: 555.8095092773438, average loss 5.8814970703125\n",
      "Batch 6, Loss: 534.0220336914062, average loss 5.791284281412761\n",
      "Batch 7, Loss: 511.2862243652344, average loss 5.694366847446987\n",
      "Batch 8, Loss: 528.800048828125, average loss 5.64357105255127\n",
      "Batch 9, Loss: 550.4439697265625, average loss 5.628112013075087\n",
      "Batch 10, Loss: 505.752685546875, average loss 5.571053497314453\n",
      "Total loss: 5782.591491699219\n",
      "Validation loss: 5.782591491699219\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909540\n",
      "\tInput: 9303634\tPrediction: 23923990\n",
      "Epoch 188, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 542.8286743164062, average loss 5.428286743164063\n",
      "Batch 2, Loss: 594.7846069335938, average loss 5.68806640625\n",
      "Batch 3, Loss: 506.553466796875, average loss 5.480555826822917\n",
      "Batch 4, Loss: 501.1260986328125, average loss 5.363232116699219\n",
      "Batch 5, Loss: 468.2120361328125, average loss 5.227009765625\n",
      "Batch 6, Loss: 510.6904296875, average loss 5.2069921875\n",
      "Batch 7, Loss: 526.778564453125, average loss 5.2156769670758925\n",
      "Batch 8, Loss: 562.4739379882812, average loss 5.266809768676758\n",
      "Batch 9, Loss: 491.8610534667969, average loss 5.228120964898004\n",
      "Batch 10, Loss: 581.391357421875, average loss 5.286700225830078\n",
      "Total loss: 5326.599273681641\n",
      "Validation loss: 5.326599273681641\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7015664480\n",
      "\tInput: 11002\tPrediction: 24000\n",
      "Epoch 189, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 570.2653198242188, average loss 5.702653198242188\n",
      "Batch 2, Loss: 525.4056396484375, average loss 5.478354797363282\n",
      "Batch 3, Loss: 546.3926391601562, average loss 5.473545328776042\n",
      "Batch 4, Loss: 451.52642822265625, average loss 5.2339750671386716\n",
      "Batch 5, Loss: 482.70513916015625, average loss 5.15259033203125\n",
      "Batch 6, Loss: 529.2835693359375, average loss 5.17596455891927\n",
      "Batch 7, Loss: 537.5589599609375, average loss 5.204482421875\n",
      "Batch 8, Loss: 491.2005615234375, average loss 5.167922821044922\n",
      "Batch 9, Loss: 554.52099609375, average loss 5.20984361436632\n",
      "Batch 10, Loss: 516.6572265625, average loss 5.2055164794921875\n",
      "Total loss: 5725.500732421875\n",
      "Validation loss: 5.725500732421875\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94246934\n",
      "\tInput: 6167476\tPrediction: 4941947472\n",
      "Epoch 190, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 492.093017578125, average loss 4.92093017578125\n",
      "Batch 2, Loss: 527.34765625, average loss 5.097203369140625\n",
      "Batch 3, Loss: 466.17401123046875, average loss 4.952048950195312\n",
      "Batch 4, Loss: 465.4054260253906, average loss 4.877550277709961\n",
      "Batch 5, Loss: 472.72406005859375, average loss 4.847488342285156\n",
      "Batch 6, Loss: 531.9425048828125, average loss 4.926144460042318\n",
      "Batch 7, Loss: 489.60369873046875, average loss 4.921843392508371\n",
      "Batch 8, Loss: 555.3709106445312, average loss 5.000826606750488\n",
      "Batch 9, Loss: 499.0281677246094, average loss 4.9996549479166665\n",
      "Batch 10, Loss: 534.508056640625, average loss 5.034197509765625\n",
      "Total loss: 5085.938659667969\n",
      "Validation loss: 5.085938659667969\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 426368\n",
      "\tInput: 21151199\tPrediction: 046499\n",
      "Epoch 191, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 495.7705078125, average loss 4.957705078125\n",
      "Batch 2, Loss: 442.54913330078125, average loss 4.6915982055664065\n",
      "Batch 3, Loss: 463.0168151855469, average loss 4.671121520996094\n",
      "Batch 4, Loss: 485.894775390625, average loss 4.718078079223633\n",
      "Batch 5, Loss: 510.95111083984375, average loss 4.7963646850585935\n",
      "Batch 6, Loss: 458.1147766113281, average loss 4.760495198567709\n",
      "Batch 7, Loss: 536.7232055664062, average loss 4.847171892438616\n",
      "Batch 8, Loss: 487.5419616699219, average loss 4.850702857971191\n",
      "Batch 9, Loss: 516.2749633789062, average loss 4.885374721950955\n",
      "Batch 10, Loss: 471.08514404296875, average loss 4.867922393798828\n",
      "Total loss: 4994.967224121094\n",
      "Validation loss: 4.994967224121094\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 479009\n",
      "Epoch 192, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 431.750244140625, average loss 4.31750244140625\n",
      "Batch 2, Loss: 451.12872314453125, average loss 4.414394836425782\n",
      "Batch 3, Loss: 468.0791931152344, average loss 4.503193868001302\n",
      "Batch 4, Loss: 517.213623046875, average loss 4.670429458618164\n",
      "Batch 5, Loss: 465.95361328125, average loss 4.668250793457031\n",
      "Batch 6, Loss: 558.205322265625, average loss 4.820551198323567\n",
      "Batch 7, Loss: 464.8188781738281, average loss 4.795927995954241\n",
      "Batch 8, Loss: 525.107177734375, average loss 4.852820968627929\n",
      "Batch 9, Loss: 455.61883544921875, average loss 4.819861789279514\n",
      "Batch 10, Loss: 500.61474609375, average loss 4.838490356445313\n",
      "Total loss: 4725.4459228515625\n",
      "Validation loss: 4.725445922851563\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 59606006812\n",
      "\tInput: 925\tPrediction: 206\n",
      "Epoch 193, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 452.03717041015625, average loss 4.520371704101563\n",
      "Batch 2, Loss: 469.3166809082031, average loss 4.606769256591797\n",
      "Batch 3, Loss: 491.459716796875, average loss 4.709378560384114\n",
      "Batch 4, Loss: 449.20977783203125, average loss 4.6550583648681645\n",
      "Batch 5, Loss: 532.3771362304688, average loss 4.7888009643554685\n",
      "Batch 6, Loss: 471.538818359375, average loss 4.776565500895182\n",
      "Batch 7, Loss: 504.6361083984375, average loss 4.815107727050782\n",
      "Batch 8, Loss: 444.094970703125, average loss 4.76833797454834\n",
      "Batch 9, Loss: 492.9527587890625, average loss 4.786247931586372\n",
      "Batch 10, Loss: 416.256103515625, average loss 4.72387924194336\n",
      "Total loss: 4812.9208984375\n",
      "Validation loss: 4.8129208984375\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 307927\n",
      "\tInput: 94164\tPrediction: 074194\n",
      "Epoch 194, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 462.32122802734375, average loss 4.623212280273438\n",
      "Batch 2, Loss: 480.07086181640625, average loss 4.71196044921875\n",
      "Batch 3, Loss: 437.4438781738281, average loss 4.59945322672526\n",
      "Batch 4, Loss: 497.32220458984375, average loss 4.692895431518554\n",
      "Batch 5, Loss: 461.897216796875, average loss 4.678110778808594\n",
      "Batch 6, Loss: 487.4293212890625, average loss 4.710807851155599\n",
      "Batch 7, Loss: 448.7300720214844, average loss 4.678878261021206\n",
      "Batch 8, Loss: 484.9673767089844, average loss 4.700227699279785\n",
      "Batch 9, Loss: 430.42620849609375, average loss 4.6562315199110245\n",
      "Batch 10, Loss: 433.1333312988281, average loss 4.62374169921875\n",
      "Total loss: 4700.427093505859\n",
      "Validation loss: 4.700427093505859\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 94023472\n",
      "Epoch 195, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 479.2406311035156, average loss 4.7924063110351565\n",
      "Batch 2, Loss: 444.06640625, average loss 4.616535186767578\n",
      "Batch 3, Loss: 509.91668701171875, average loss 4.777412414550781\n",
      "Batch 4, Loss: 455.7132568359375, average loss 4.72234245300293\n",
      "Batch 5, Loss: 468.70123291015625, average loss 4.715276428222657\n",
      "Batch 6, Loss: 444.45452880859375, average loss 4.670154571533203\n",
      "Batch 7, Loss: 472.3020935058594, average loss 4.677706909179688\n",
      "Batch 8, Loss: 416.5921325683594, average loss 4.613733711242676\n",
      "Batch 9, Loss: 408.68212890625, average loss 4.55518788655599\n",
      "Batch 10, Loss: 435.886962890625, average loss 4.5355560607910155\n",
      "Total loss: 4728.632598876953\n",
      "Validation loss: 4.7286325988769535\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172478\n",
      "\tInput: 6824\tPrediction: 6804\n",
      "Epoch 196, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 417.5054931640625, average loss 4.175054931640625\n",
      "Batch 2, Loss: 522.7899169921875, average loss 4.70147705078125\n",
      "Batch 3, Loss: 448.6533508300781, average loss 4.629829203287761\n",
      "Batch 4, Loss: 484.81243896484375, average loss 4.684402999877929\n",
      "Batch 5, Loss: 415.0604248046875, average loss 4.577643249511719\n",
      "Batch 6, Loss: 444.58074951171875, average loss 4.555670623779297\n",
      "Batch 7, Loss: 385.35272216796875, average loss 4.455364423479352\n",
      "Batch 8, Loss: 401.50592041015625, average loss 4.4003262710571285\n",
      "Batch 9, Loss: 422.2332763671875, average loss 4.38054921468099\n",
      "Batch 10, Loss: 471.44952392578125, average loss 4.413943817138672\n",
      "Total loss: 25523.05224609375\n",
      "Validation loss: 25.52305224609375\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 7793988317\n",
      "\tInput: 135\tPrediction: 777465777\n",
      "Epoch 197, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 505.52899169921875, average loss 5.055289916992187\n",
      "Batch 2, Loss: 456.4853515625, average loss 4.8100717163085935\n",
      "Batch 3, Loss: 502.8149108886719, average loss 4.882764180501302\n",
      "Batch 4, Loss: 408.17840576171875, average loss 4.682519149780274\n",
      "Batch 5, Loss: 467.5098876953125, average loss 4.681035095214844\n",
      "Batch 6, Loss: 390.25299072265625, average loss 4.55128423055013\n",
      "Batch 7, Loss: 397.9906921386719, average loss 4.469658900669643\n",
      "Batch 8, Loss: 431.087890625, average loss 4.449811401367188\n",
      "Batch 9, Loss: 454.947021484375, average loss 4.460884602864583\n",
      "Batch 10, Loss: 383.14166259765625, average loss 4.397937805175781\n",
      "Total loss: 4479.733825683594\n",
      "Validation loss: 4.479733825683594\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909540\n",
      "\tInput: 9303634\tPrediction: 0629904\n",
      "Epoch 198, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 431.2418212890625, average loss 4.312418212890625\n",
      "Batch 2, Loss: 443.70941162109375, average loss 4.3747561645507815\n",
      "Batch 3, Loss: 397.4601745605469, average loss 4.241371358235677\n",
      "Batch 4, Loss: 428.7822265625, average loss 4.252984085083008\n",
      "Batch 5, Loss: 376.78387451171875, average loss 4.155955017089844\n",
      "Batch 6, Loss: 386.8777160644531, average loss 4.108092041015625\n",
      "Batch 7, Loss: 405.4709167480469, average loss 4.100465916224889\n",
      "Batch 8, Loss: 427.9323425292969, average loss 4.122823104858399\n",
      "Batch 9, Loss: 368.842529296875, average loss 4.074556681315104\n",
      "Batch 10, Loss: 444.52392578125, average loss 4.111624938964844\n",
      "Total loss: 4230.638092041016\n",
      "Validation loss: 4.230638092041016\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7216664460\n",
      "\tInput: 11002\tPrediction: 14120\n",
      "Epoch 199, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 452.41693115234375, average loss 4.524169311523438\n",
      "Batch 2, Loss: 396.9427490234375, average loss 4.246798400878906\n",
      "Batch 3, Loss: 436.57769775390625, average loss 4.286457926432291\n",
      "Batch 4, Loss: 380.633544921875, average loss 4.166427307128906\n",
      "Batch 5, Loss: 394.9633483886719, average loss 4.1230685424804685\n",
      "Batch 6, Loss: 380.49200439453125, average loss 4.07004379272461\n",
      "Batch 7, Loss: 426.4054870605469, average loss 4.097759660993304\n",
      "Batch 8, Loss: 372.89215087890625, average loss 4.0516548919677735\n",
      "Batch 9, Loss: 474.5325622558594, average loss 4.128729417588976\n",
      "Batch 10, Loss: 402.5410461425781, average loss 4.118397521972656\n",
      "Total loss: 4109.8199462890625\n",
      "Validation loss: 4.109819946289062\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94246934\n",
      "\tInput: 6167476\tPrediction: 64947479\n",
      "Epoch 200, lr=[0.000625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 386.5640869140625, average loss 3.865640869140625\n",
      "Batch 2, Loss: 449.0728759765625, average loss 4.178184814453125\n",
      "Batch 3, Loss: 375.8454895019531, average loss 4.038274841308594\n",
      "Batch 4, Loss: 374.29522705078125, average loss 3.9644441986083985\n",
      "Batch 5, Loss: 393.4630126953125, average loss 3.9584813842773436\n",
      "Batch 6, Loss: 412.51947021484375, average loss 3.9862669372558592\n",
      "Batch 7, Loss: 355.2005615234375, average loss 3.9242296055385046\n",
      "Batch 8, Loss: 451.8251953125, average loss 3.9984823989868166\n",
      "Batch 9, Loss: 399.0171203613281, average loss 3.9975589328342016\n",
      "Batch 10, Loss: 413.3868713378906, average loss 4.011189910888672\n",
      "Total loss: 3856.9993896484375\n",
      "Validation loss: 3.8569993896484376\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 045499\n",
      "Epoch 201, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 396.1397705078125, average loss 3.961397705078125\n",
      "Batch 2, Loss: 354.1080322265625, average loss 3.751239013671875\n",
      "Batch 3, Loss: 355.098388671875, average loss 3.6844873046875\n",
      "Batch 4, Loss: 367.31207275390625, average loss 3.6816456604003904\n",
      "Batch 5, Loss: 373.0229187011719, average loss 3.6913623657226564\n",
      "Batch 6, Loss: 362.68402099609375, average loss 3.680608673095703\n",
      "Batch 7, Loss: 423.70367431640625, average loss 3.7600983973911832\n",
      "Batch 8, Loss: 411.8571472167969, average loss 3.8049075317382814\n",
      "Batch 9, Loss: 416.4012451171875, average loss 3.8448080783420138\n",
      "Batch 10, Loss: 366.09185791015625, average loss 3.8264191284179687\n",
      "Total loss: 3816.4999389648438\n",
      "Validation loss: 3.816499938964844\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 479009\n",
      "Epoch 202, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 343.1246032714844, average loss 3.431246032714844\n",
      "Batch 2, Loss: 357.6126708984375, average loss 3.5036863708496093\n",
      "Batch 3, Loss: 358.75726318359375, average loss 3.531648457845052\n",
      "Batch 4, Loss: 391.0655212402344, average loss 3.626400146484375\n",
      "Batch 5, Loss: 330.0123596191406, average loss 3.561144836425781\n",
      "Batch 6, Loss: 426.7251281738281, average loss 3.6788292439778645\n",
      "Batch 7, Loss: 394.40997314453125, average loss 3.7167250279017856\n",
      "Batch 8, Loss: 394.47027587890625, average loss 3.7452222442626955\n",
      "Batch 9, Loss: 328.37640380859375, average loss 3.6939491102430555\n",
      "Batch 10, Loss: 374.4652404785156, average loss 3.6990194396972655\n",
      "Total loss: 3891.357421875\n",
      "Validation loss: 3.891357421875\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 16945\n",
      "Epoch 203, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 355.130859375, average loss 3.55130859375\n",
      "Batch 2, Loss: 340.33935546875, average loss 3.47735107421875\n",
      "Batch 3, Loss: 407.99627685546875, average loss 3.6782216389973956\n",
      "Batch 4, Loss: 361.5411376953125, average loss 3.662519073486328\n",
      "Batch 5, Loss: 414.19281005859375, average loss 3.75840087890625\n",
      "Batch 6, Loss: 365.36114501953125, average loss 3.740935974121094\n",
      "Batch 7, Loss: 405.2652587890625, average loss 3.7854669189453123\n",
      "Batch 8, Loss: 382.8865966796875, average loss 3.790891799926758\n",
      "Batch 9, Loss: 385.57391357421875, average loss 3.7980970594618055\n",
      "Batch 10, Loss: 324.1947326660156, average loss 3.742482086181641\n",
      "Total loss: 4553.484283447266\n",
      "Validation loss: 4.553484283447266\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 7607927\n",
      "\tInput: 94164\tPrediction: 090494\n",
      "Epoch 204, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 356.22601318359375, average loss 3.5622601318359375\n",
      "Batch 2, Loss: 389.5107727050781, average loss 3.7286839294433594\n",
      "Batch 3, Loss: 338.487060546875, average loss 3.6140794881184894\n",
      "Batch 4, Loss: 399.6812744140625, average loss 3.7097628021240237\n",
      "Batch 5, Loss: 364.1353759765625, average loss 3.6960809936523438\n",
      "Batch 6, Loss: 410.3530578613281, average loss 3.7639892578125\n",
      "Batch 7, Loss: 316.1992492675781, average loss 3.677989719935826\n",
      "Batch 8, Loss: 382.53753662109375, average loss 3.6964129257202147\n",
      "Batch 9, Loss: 332.998046875, average loss 3.65569820827908\n",
      "Batch 10, Loss: 314.1055908203125, average loss 3.6042339782714845\n",
      "Total loss: 3585.828399658203\n",
      "Validation loss: 3.585828399658203\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 69404074\n",
      "Epoch 205, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 377.5420837402344, average loss 3.775420837402344\n",
      "Batch 2, Loss: 332.35040283203125, average loss 3.5494624328613282\n",
      "Batch 3, Loss: 406.8193664550781, average loss 3.722372843424479\n",
      "Batch 4, Loss: 365.8720703125, average loss 3.706459808349609\n",
      "Batch 5, Loss: 380.1065979003906, average loss 3.7253810424804685\n",
      "Batch 6, Loss: 350.67864990234375, average loss 3.6889486185709637\n",
      "Batch 7, Loss: 357.6746520996094, average loss 3.672919747488839\n",
      "Batch 8, Loss: 326.576904296875, average loss 3.622025909423828\n",
      "Batch 9, Loss: 305.204833984375, average loss 3.558695068359375\n",
      "Batch 10, Loss: 336.133056640625, average loss 3.5389586181640627\n",
      "Total loss: 3594.97900390625\n",
      "Validation loss: 3.59497900390625\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 265172473\n",
      "\tInput: 6824\tPrediction: 656890\n",
      "Epoch 206, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 327.43817138671875, average loss 3.2743817138671876\n",
      "Batch 2, Loss: 400.440673828125, average loss 3.6393942260742187\n",
      "Batch 3, Loss: 350.7345886230469, average loss 3.595378112792969\n",
      "Batch 4, Loss: 393.7647399902344, average loss 3.6809454345703125\n",
      "Batch 5, Loss: 349.00390625, average loss 3.64276416015625\n",
      "Batch 6, Loss: 355.057373046875, average loss 3.6273990885416665\n",
      "Batch 7, Loss: 323.51025390625, average loss 3.5713567243303572\n",
      "Batch 8, Loss: 337.06683349609375, average loss 3.5462706756591795\n",
      "Batch 9, Loss: 344.1479187011719, average loss 3.534627176920573\n",
      "Batch 10, Loss: 363.43939208984375, average loss 3.5446038513183593\n",
      "Total loss: 3581.924774169922\n",
      "Validation loss: 3.581924774169922\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9338331\n",
      "\tInput: 135\tPrediction: 109\n",
      "Epoch 207, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 386.45794677734375, average loss 3.8645794677734373\n",
      "Batch 2, Loss: 340.14764404296875, average loss 3.6330279541015624\n",
      "Batch 3, Loss: 368.69146728515625, average loss 3.650990193684896\n",
      "Batch 4, Loss: 311.4158630371094, average loss 3.5167823028564453\n",
      "Batch 5, Loss: 353.2853698730469, average loss 3.51999658203125\n",
      "Batch 6, Loss: 319.208251953125, average loss 3.46534423828125\n",
      "Batch 7, Loss: 340.318359375, average loss 3.456464146205357\n",
      "Batch 8, Loss: 342.7006530761719, average loss 3.452781944274902\n",
      "Batch 9, Loss: 357.1106262207031, average loss 3.4659290907118057\n",
      "Batch 10, Loss: 337.5081481933594, average loss 3.4568443298339844\n",
      "Total loss: 3485.7334594726562\n",
      "Validation loss: 3.485733459472656\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909540\n",
      "\tInput: 9303634\tPrediction: 66231904\n",
      "Epoch 208, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 370.39434814453125, average loss 3.7039434814453127\n",
      "Batch 2, Loss: 365.9082946777344, average loss 3.681513214111328\n",
      "Batch 3, Loss: 310.66241455078125, average loss 3.4898835245768227\n",
      "Batch 4, Loss: 359.83062744140625, average loss 3.516989212036133\n",
      "Batch 5, Loss: 313.6724548339844, average loss 3.440936279296875\n",
      "Batch 6, Loss: 306.226806640625, average loss 3.377824910481771\n",
      "Batch 7, Loss: 320.82830810546875, average loss 3.3536046491350446\n",
      "Batch 8, Loss: 371.26458740234375, average loss 3.398484802246094\n",
      "Batch 9, Loss: 308.55706787109375, average loss 3.363716566297743\n",
      "Batch 10, Loss: 386.37164306640625, average loss 3.413716552734375\n",
      "Total loss: 3476.0926513671875\n",
      "Validation loss: 3.4760926513671877\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7215664450\n",
      "\tInput: 11002\tPrediction: 4000\n",
      "Epoch 209, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 392.7865295410156, average loss 3.927865295410156\n",
      "Batch 2, Loss: 302.5361328125, average loss 3.476613311767578\n",
      "Batch 3, Loss: 377.6611328125, average loss 3.5766126505533853\n",
      "Batch 4, Loss: 290.1920166015625, average loss 3.4079395294189454\n",
      "Batch 5, Loss: 332.17864990234375, average loss 3.3907089233398438\n",
      "Batch 6, Loss: 316.1678466796875, average loss 3.3525371805826825\n",
      "Batch 7, Loss: 357.9545593261719, average loss 3.3849669538225444\n",
      "Batch 8, Loss: 301.5542297363281, average loss 3.3387888717651366\n",
      "Batch 9, Loss: 374.6329345703125, average loss 3.3840711466471354\n",
      "Batch 10, Loss: 332.0039367675781, average loss 3.37766796875\n",
      "Total loss: 3297.3534240722656\n",
      "Validation loss: 3.2973534240722655\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94246934\n",
      "\tInput: 6167476\tPrediction: 16497473\n",
      "Epoch 210, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 306.06591796875, average loss 3.0606591796875\n",
      "Batch 2, Loss: 359.9240417480469, average loss 3.3299497985839843\n",
      "Batch 3, Loss: 299.8200378417969, average loss 3.219366658528646\n",
      "Batch 4, Loss: 318.484375, average loss 3.2107359313964845\n",
      "Batch 5, Loss: 331.7897644042969, average loss 3.2321682739257813\n",
      "Batch 6, Loss: 349.529296875, average loss 3.2760223897298175\n",
      "Batch 7, Loss: 299.0657043457031, average loss 3.235255911690848\n",
      "Batch 8, Loss: 405.5255432128906, average loss 3.3377558517456056\n",
      "Batch 9, Loss: 311.96746826171875, average loss 3.3135246107313368\n",
      "Batch 10, Loss: 375.8876647949219, average loss 3.358059814453125\n",
      "Total loss: 3332.0865783691406\n",
      "Validation loss: 3.3320865783691405\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 944840099\n",
      "Epoch 211, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 323.23162841796875, average loss 3.2323162841796873\n",
      "Batch 2, Loss: 300.7062683105469, average loss 3.119689483642578\n",
      "Batch 3, Loss: 301.6353759765625, average loss 3.0852442423502606\n",
      "Batch 4, Loss: 305.2223815917969, average loss 3.0769891357421875\n",
      "Batch 5, Loss: 333.896728515625, average loss 3.129384765625\n",
      "Batch 6, Loss: 325.12347412109375, average loss 3.1496930948893227\n",
      "Batch 7, Loss: 348.5879211425781, average loss 3.19771968296596\n",
      "Batch 8, Loss: 315.6016845703125, average loss 3.1925068283081055\n",
      "Batch 9, Loss: 385.63262939453125, average loss 3.2662645467122395\n",
      "Batch 10, Loss: 295.44915771484375, average loss 3.2350872497558596\n",
      "Total loss: 9999.644836425781\n",
      "Validation loss: 9.999644836425782\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 777217\n",
      "\tInput: 76206\tPrediction: 7727904097\n",
      "Epoch 212, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 312.5273742675781, average loss 3.1252737426757813\n",
      "Batch 2, Loss: 287.38555908203125, average loss 2.999564666748047\n",
      "Batch 3, Loss: 294.93804931640625, average loss 2.9828366088867186\n",
      "Batch 4, Loss: 340.94610595703125, average loss 3.089492721557617\n",
      "Batch 5, Loss: 318.35125732421875, average loss 3.108296691894531\n",
      "Batch 6, Loss: 360.89459228515625, average loss 3.1917382303873696\n",
      "Batch 7, Loss: 324.03411865234375, average loss 3.1986815098353794\n",
      "Batch 8, Loss: 384.682373046875, average loss 3.2796992874145507\n",
      "Batch 9, Loss: 308.8029479980469, average loss 3.2584026421440972\n",
      "Batch 10, Loss: 343.86822509765625, average loss 3.2764306030273436\n",
      "Total loss: 3303.8995666503906\n",
      "Validation loss: 3.3038995666503905\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600512\n",
      "\tInput: 925\tPrediction: 6909\n",
      "Epoch 213, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 287.2027587890625, average loss 2.872027587890625\n",
      "Batch 2, Loss: 305.55963134765625, average loss 2.963811950683594\n",
      "Batch 3, Loss: 375.485595703125, average loss 3.2274932861328125\n",
      "Batch 4, Loss: 309.048828125, average loss 3.1932420349121093\n",
      "Batch 5, Loss: 383.60821533203125, average loss 3.32181005859375\n",
      "Batch 6, Loss: 345.8379821777344, average loss 3.3445716857910157\n",
      "Batch 7, Loss: 366.0696105957031, average loss 3.3897323172433036\n",
      "Batch 8, Loss: 318.1278076171875, average loss 3.363675537109375\n",
      "Batch 9, Loss: 326.9613037109375, average loss 3.353224148220486\n",
      "Batch 10, Loss: 294.37799072265625, average loss 3.312279724121094\n",
      "Total loss: 4220.154296875\n",
      "Validation loss: 4.220154296875\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 1607927\n",
      "\tInput: 94164\tPrediction: 014494\n",
      "Epoch 214, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 274.80712890625, average loss 2.7480712890625\n",
      "Batch 2, Loss: 309.39483642578125, average loss 2.921009826660156\n",
      "Batch 3, Loss: 300.7650146484375, average loss 2.949889933268229\n",
      "Batch 4, Loss: 378.12677001953125, average loss 3.157734375\n",
      "Batch 5, Loss: 327.881103515625, average loss 3.18194970703125\n",
      "Batch 6, Loss: 377.80908203125, average loss 3.2813065592447916\n",
      "Batch 7, Loss: 309.98681640625, average loss 3.2553867885044645\n",
      "Batch 8, Loss: 321.0126037597656, average loss 3.2497291946411133\n",
      "Batch 9, Loss: 291.4759521484375, average loss 3.2125103420681422\n",
      "Batch 10, Loss: 287.14324951171875, average loss 3.178402557373047\n",
      "Total loss: 3416.465545654297\n",
      "Validation loss: 3.416465545654297\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 64429474\n",
      "Epoch 215, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 308.5116271972656, average loss 3.085116271972656\n",
      "Batch 2, Loss: 291.8698425292969, average loss 3.0019073486328125\n",
      "Batch 3, Loss: 358.467529296875, average loss 3.196163330078125\n",
      "Batch 4, Loss: 320.3953552246094, average loss 3.198110885620117\n",
      "Batch 5, Loss: 335.3780517578125, average loss 3.2292448120117188\n",
      "Batch 6, Loss: 280.3027038574219, average loss 3.158208516438802\n",
      "Batch 7, Loss: 324.6383056640625, average loss 3.170804879324777\n",
      "Batch 8, Loss: 272.8378601074219, average loss 3.115501594543457\n",
      "Batch 9, Loss: 249.20175170898438, average loss 3.0462255859375\n",
      "Batch 10, Loss: 280.49896240234375, average loss 3.0221019897460937\n",
      "Total loss: 3385.3887939453125\n",
      "Validation loss: 3.3853887939453124\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172475\n",
      "\tInput: 6824\tPrediction: 18604\n",
      "Epoch 216, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 283.67889404296875, average loss 2.8367889404296873\n",
      "Batch 2, Loss: 338.8843078613281, average loss 3.1128160095214845\n",
      "Batch 3, Loss: 318.1016540527344, average loss 3.1355495198567707\n",
      "Batch 4, Loss: 345.66558837890625, average loss 3.215826110839844\n",
      "Batch 5, Loss: 272.5130615234375, average loss 3.11768701171875\n",
      "Batch 6, Loss: 298.0972595214844, average loss 3.094901275634766\n",
      "Batch 7, Loss: 281.1925354003906, average loss 3.0544761439732144\n",
      "Batch 8, Loss: 268.2149658203125, average loss 3.007935333251953\n",
      "Batch 9, Loss: 307.409423828125, average loss 3.015286322699653\n",
      "Batch 10, Loss: 312.8202209472656, average loss 3.026577911376953\n",
      "Total loss: 3039.218994140625\n",
      "Validation loss: 3.039218994140625\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9338531\n",
      "\tInput: 135\tPrediction: 4969\n",
      "Epoch 217, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 339.08575439453125, average loss 3.3908575439453124\n",
      "Batch 2, Loss: 323.470458984375, average loss 3.3127810668945314\n",
      "Batch 3, Loss: 338.4033203125, average loss 3.336531778971354\n",
      "Batch 4, Loss: 287.7536315917969, average loss 3.221782913208008\n",
      "Batch 5, Loss: 310.92169189453125, average loss 3.1992697143554687\n",
      "Batch 6, Loss: 291.0608825683594, average loss 3.1511595662434897\n",
      "Batch 7, Loss: 269.4969482421875, average loss 3.085989554268973\n",
      "Batch 8, Loss: 282.8570556640625, average loss 3.05381217956543\n",
      "Batch 9, Loss: 293.98846435546875, average loss 3.041153564453125\n",
      "Batch 10, Loss: 279.8327941894531, average loss 3.0168710021972656\n",
      "Total loss: 3053.7308044433594\n",
      "Validation loss: 3.0537308044433593\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 496469964\n",
      "Epoch 218, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 304.12322998046875, average loss 3.0412322998046877\n",
      "Batch 2, Loss: 325.65826416015625, average loss 3.148907470703125\n",
      "Batch 3, Loss: 283.68475341796875, average loss 3.0448874918619793\n",
      "Batch 4, Loss: 299.84271240234375, average loss 3.033272399902344\n",
      "Batch 5, Loss: 275.590576171875, average loss 2.977799072265625\n",
      "Batch 6, Loss: 261.3688049316406, average loss 2.9171139017740884\n",
      "Batch 7, Loss: 286.5540771484375, average loss 2.9097463117327007\n",
      "Batch 8, Loss: 287.9412841796875, average loss 2.9059546279907225\n",
      "Batch 9, Loss: 271.74664306640625, average loss 2.885011494954427\n",
      "Batch 10, Loss: 333.8774108886719, average loss 2.930387756347656\n",
      "Total loss: 2840.3844451904297\n",
      "Validation loss: 2.8403844451904297\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7218664450\n",
      "\tInput: 11002\tPrediction: 240104\n",
      "Epoch 219, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 323.0291442871094, average loss 3.230291442871094\n",
      "Batch 2, Loss: 259.9978332519531, average loss 2.9151348876953125\n",
      "Batch 3, Loss: 295.1441955566406, average loss 2.927237243652344\n",
      "Batch 4, Loss: 256.47772216796875, average loss 2.8366222381591797\n",
      "Batch 5, Loss: 273.7890319824219, average loss 2.8168758544921877\n",
      "Batch 6, Loss: 287.64166259765625, average loss 2.82679931640625\n",
      "Batch 7, Loss: 291.0031433105469, average loss 2.838689618791853\n",
      "Batch 8, Loss: 278.23175048828125, average loss 2.8316431045532227\n",
      "Batch 9, Loss: 311.9248962402344, average loss 2.8635993109809026\n",
      "Batch 10, Loss: 276.7847900390625, average loss 2.854024169921875\n",
      "Total loss: 2917.5027923583984\n",
      "Validation loss: 2.9175027923583983\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 94947479\n",
      "Epoch 220, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 255.07505798339844, average loss 2.5507505798339842\n",
      "Batch 2, Loss: 266.8558349609375, average loss 2.6096544647216797\n",
      "Batch 3, Loss: 258.2159423828125, average loss 2.600489451090495\n",
      "Batch 4, Loss: 262.50213623046875, average loss 2.606622428894043\n",
      "Batch 5, Loss: 265.4216613769531, average loss 2.6161412658691408\n",
      "Batch 6, Loss: 297.051025390625, average loss 2.6752027638753257\n",
      "Batch 7, Loss: 274.28753662109375, average loss 2.6848702784946985\n",
      "Batch 8, Loss: 320.7919921875, average loss 2.7502514839172365\n",
      "Batch 9, Loss: 300.0087890625, average loss 2.7780110846625434\n",
      "Batch 10, Loss: 332.42584228515625, average loss 2.8326358184814455\n",
      "Total loss: 2769.1234436035156\n",
      "Validation loss: 2.7691234436035157\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 04454669\n",
      "Epoch 221, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 277.6862487792969, average loss 2.7768624877929686\n",
      "Batch 2, Loss: 271.4483337402344, average loss 2.7456729125976564\n",
      "Batch 3, Loss: 244.90982055664062, average loss 2.646814676920573\n",
      "Batch 4, Loss: 262.89898681640625, average loss 2.6423584747314455\n",
      "Batch 5, Loss: 271.02642822265625, average loss 2.655939636230469\n",
      "Batch 6, Loss: 264.0675048828125, average loss 2.653395538330078\n",
      "Batch 7, Loss: 335.345703125, average loss 2.753404323032924\n",
      "Batch 8, Loss: 285.6880798339844, average loss 2.766338882446289\n",
      "Batch 9, Loss: 314.19586181640625, average loss 2.808074408637153\n",
      "Batch 10, Loss: 256.4622497558594, average loss 2.783729217529297\n",
      "Total loss: 3045.5235900878906\n",
      "Validation loss: 3.0455235900878908\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 27941419\n",
      "Epoch 222, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 269.17822265625, average loss 2.6917822265625\n",
      "Batch 2, Loss: 267.7171630859375, average loss 2.6844769287109376\n",
      "Batch 3, Loss: 258.71099853515625, average loss 2.652021280924479\n",
      "Batch 4, Loss: 290.331298828125, average loss 2.714844207763672\n",
      "Batch 5, Loss: 269.79901123046875, average loss 2.711473388671875\n",
      "Batch 6, Loss: 332.952392578125, average loss 2.8144818115234376\n",
      "Batch 7, Loss: 265.56591796875, average loss 2.7917928641183036\n",
      "Batch 8, Loss: 300.1444091796875, average loss 2.817999267578125\n",
      "Batch 9, Loss: 247.79188537597656, average loss 2.7802125549316408\n",
      "Batch 10, Loss: 283.2418212890625, average loss 2.785433120727539\n",
      "Total loss: 2676.0125274658203\n",
      "Validation loss: 2.6760125274658204\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 1946\n",
      "Epoch 223, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 227.09677124023438, average loss 2.270967712402344\n",
      "Batch 2, Loss: 258.2977600097656, average loss 2.42697265625\n",
      "Batch 3, Loss: 288.778564453125, average loss 2.5805769856770833\n",
      "Batch 4, Loss: 237.56124877929688, average loss 2.5293358612060546\n",
      "Batch 5, Loss: 331.84771728515625, average loss 2.6871641235351564\n",
      "Batch 6, Loss: 276.62469482421875, average loss 2.7003445943196613\n",
      "Batch 7, Loss: 307.252197265625, average loss 2.7535127912248885\n",
      "Batch 8, Loss: 227.84011840820312, average loss 2.6941238403320313\n",
      "Batch 9, Loss: 275.8626403808594, average loss 2.701290791829427\n",
      "Batch 10, Loss: 268.595703125, average loss 2.6997574157714843\n",
      "Total loss: 2693.4530181884766\n",
      "Validation loss: 2.6934530181884764\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607927\n",
      "\tInput: 94164\tPrediction: 13464\n",
      "Epoch 224, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 246.837646484375, average loss 2.46837646484375\n",
      "Batch 2, Loss: 277.2347412109375, average loss 2.6203619384765626\n",
      "Batch 3, Loss: 246.30239868164062, average loss 2.567915954589844\n",
      "Batch 4, Loss: 306.3343505859375, average loss 2.6917728424072265\n",
      "Batch 5, Loss: 282.0390930175781, average loss 2.7174964599609375\n",
      "Batch 6, Loss: 281.87408447265625, average loss 2.7343705240885416\n",
      "Batch 7, Loss: 241.0013427734375, average loss 2.6880337960379466\n",
      "Batch 8, Loss: 280.58721923828125, average loss 2.7027635955810547\n",
      "Batch 9, Loss: 263.5894775390625, average loss 2.695333726671007\n",
      "Batch 10, Loss: 255.02696228027344, average loss 2.6808273162841796\n",
      "Total loss: 11449.365783691406\n",
      "Validation loss: 11.449365783691407\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 77704177\n",
      "\tInput: 64009172\tPrediction: 792043473\n",
      "Epoch 225, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 271.8307189941406, average loss 2.7183071899414064\n",
      "Batch 2, Loss: 255.25808715820312, average loss 2.6354440307617186\n",
      "Batch 3, Loss: 287.00830078125, average loss 2.713657023111979\n",
      "Batch 4, Loss: 276.6329345703125, average loss 2.7268251037597655\n",
      "Batch 5, Loss: 290.9913024902344, average loss 2.7634426879882814\n",
      "Batch 6, Loss: 244.93577575683594, average loss 2.711095199584961\n",
      "Batch 7, Loss: 257.645751953125, average loss 2.6918612452915736\n",
      "Batch 8, Loss: 250.8892059326172, average loss 2.6689900970458984\n",
      "Batch 9, Loss: 234.69378662109375, average loss 2.633206515842014\n",
      "Batch 10, Loss: 261.4478759765625, average loss 2.631333740234375\n",
      "Total loss: 2674.6576385498047\n",
      "Validation loss: 2.674657638549805\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172475\n",
      "\tInput: 6824\tPrediction: 16622\n",
      "Epoch 226, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 250.83670043945312, average loss 2.508367004394531\n",
      "Batch 2, Loss: 299.0467834472656, average loss 2.749417419433594\n",
      "Batch 3, Loss: 274.42608642578125, average loss 2.7476985677083334\n",
      "Batch 4, Loss: 297.72015380859375, average loss 2.8050743103027345\n",
      "Batch 5, Loss: 234.4110107421875, average loss 2.7128814697265624\n",
      "Batch 6, Loss: 277.08880615234375, average loss 2.7225492350260416\n",
      "Batch 7, Loss: 231.52304077148438, average loss 2.664360831124442\n",
      "Batch 8, Loss: 234.0677947998047, average loss 2.6239004707336426\n",
      "Batch 9, Loss: 232.5935516357422, average loss 2.590793253580729\n",
      "Batch 10, Loss: 267.0539855957031, average loss 2.5987679138183593\n",
      "Total loss: 3241.420684814453\n",
      "Validation loss: 3.2414206848144533\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9338531\n",
      "\tInput: 135\tPrediction: 4965\n",
      "Epoch 227, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 279.40869140625, average loss 2.7940869140625\n",
      "Batch 2, Loss: 246.1366729736328, average loss 2.627726821899414\n",
      "Batch 3, Loss: 290.83319091796875, average loss 2.7212618509928386\n",
      "Batch 4, Loss: 229.08599853515625, average loss 2.6136613845825196\n",
      "Batch 5, Loss: 269.46875, average loss 2.6298666076660155\n",
      "Batch 6, Loss: 244.43069458007812, average loss 2.598939997355143\n",
      "Batch 7, Loss: 223.8531494140625, average loss 2.5474530683244976\n",
      "Batch 8, Loss: 233.3367919921875, average loss 2.52069242477417\n",
      "Batch 9, Loss: 252.10928344726562, average loss 2.5207369147406684\n",
      "Batch 10, Loss: 251.12139892578125, average loss 2.519784622192383\n",
      "Total loss: 2649.820755004883\n",
      "Validation loss: 2.6498207550048827\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 133296654\n",
      "Epoch 228, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 262.43243408203125, average loss 2.6243243408203125\n",
      "Batch 2, Loss: 305.241943359375, average loss 2.8383718872070314\n",
      "Batch 3, Loss: 236.32069396972656, average loss 2.6799835713704425\n",
      "Batch 4, Loss: 250.47474670410156, average loss 2.636174545288086\n",
      "Batch 5, Loss: 249.1844940185547, average loss 2.6073086242675783\n",
      "Batch 6, Loss: 217.50473022460938, average loss 2.5352650705973305\n",
      "Batch 7, Loss: 221.263916015625, average loss 2.4891756548200337\n",
      "Batch 8, Loss: 241.79644775390625, average loss 2.480274257659912\n",
      "Batch 9, Loss: 252.16049194335938, average loss 2.4848665534125436\n",
      "Batch 10, Loss: 282.9632873535156, average loss 2.5193431854248045\n",
      "Total loss: 5878.218078613281\n",
      "Validation loss: 5.878218078613282\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7215664430\n",
      "\tInput: 11002\tPrediction: 7412417\n",
      "Epoch 229, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 307.51226806640625, average loss 3.0751226806640624\n",
      "Batch 2, Loss: 242.015869140625, average loss 2.7476406860351563\n",
      "Batch 3, Loss: 264.0953063964844, average loss 2.712078145345052\n",
      "Batch 4, Loss: 233.57122802734375, average loss 2.6179866790771484\n",
      "Batch 5, Loss: 216.30291748046875, average loss 2.526995178222656\n",
      "Batch 6, Loss: 236.01321411132812, average loss 2.4991846720377606\n",
      "Batch 7, Loss: 246.99131774902344, average loss 2.4950030299595425\n",
      "Batch 8, Loss: 235.97171020507812, average loss 2.4780922889709474\n",
      "Batch 9, Loss: 282.3398742675781, average loss 2.516459672715929\n",
      "Batch 10, Loss: 245.504150390625, average loss 2.510317855834961\n",
      "Total loss: 2478.8694915771484\n",
      "Validation loss: 2.4788694915771483\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 191737773\n",
      "Epoch 230, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 226.55848693847656, average loss 2.2655848693847656\n",
      "Batch 2, Loss: 238.9241485595703, average loss 2.3274131774902345\n",
      "Batch 3, Loss: 233.73216247558594, average loss 2.330715993245443\n",
      "Batch 4, Loss: 215.644775390625, average loss 2.2871489334106445\n",
      "Batch 5, Loss: 226.5531005859375, average loss 2.2828253479003906\n",
      "Batch 6, Loss: 238.62452697753906, average loss 2.300062001546224\n",
      "Batch 7, Loss: 216.38601684570312, average loss 2.2806045968191966\n",
      "Batch 8, Loss: 268.9757080078125, average loss 2.3317486572265627\n",
      "Batch 9, Loss: 240.3744659423828, average loss 2.3397482130262586\n",
      "Batch 10, Loss: 258.90386962890625, average loss 2.364677261352539\n",
      "Total loss: 2346.5109100341797\n",
      "Validation loss: 2.34651091003418\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 345499\n",
      "Epoch 231, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 236.18702697753906, average loss 2.3618702697753906\n",
      "Batch 2, Loss: 217.31381225585938, average loss 2.267504196166992\n",
      "Batch 3, Loss: 214.0229949951172, average loss 2.2250794474283855\n",
      "Batch 4, Loss: 228.4092559814453, average loss 2.239832725524902\n",
      "Batch 5, Loss: 230.4428253173828, average loss 2.2527518310546877\n",
      "Batch 6, Loss: 222.9429473876953, average loss 2.248864771525065\n",
      "Batch 7, Loss: 269.05474853515625, average loss 2.3119623020717075\n",
      "Batch 8, Loss: 235.3217315673828, average loss 2.3171191787719727\n",
      "Batch 9, Loss: 281.3404541015625, average loss 2.372261996799045\n",
      "Batch 10, Loss: 222.2513427734375, average loss 2.3572871398925783\n",
      "Total loss: 2496.48486328125\n",
      "Validation loss: 2.49648486328125\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 434019\n",
      "Epoch 232, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 208.60238647460938, average loss 2.086023864746094\n",
      "Batch 2, Loss: 212.4652862548828, average loss 2.1053383636474607\n",
      "Batch 3, Loss: 205.42410278320312, average loss 2.088305918375651\n",
      "Batch 4, Loss: 219.55081176757812, average loss 2.1151064682006835\n",
      "Batch 5, Loss: 208.54017639160156, average loss 2.10916552734375\n",
      "Batch 6, Loss: 256.827392578125, average loss 2.18568359375\n",
      "Batch 7, Loss: 242.19357299804688, average loss 2.2194338989257814\n",
      "Batch 8, Loss: 284.112548828125, average loss 2.297145347595215\n",
      "Batch 9, Loss: 206.14859008789062, average loss 2.270960964626736\n",
      "Batch 10, Loss: 225.6095428466797, average loss 2.269474411010742\n",
      "Total loss: 2675.123809814453\n",
      "Validation loss: 2.675123809814453\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 6945\n",
      "Epoch 233, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 208.7795867919922, average loss 2.087795867919922\n",
      "Batch 2, Loss: 215.07044982910156, average loss 2.1192501831054686\n",
      "Batch 3, Loss: 214.17672729492188, average loss 2.126755879720052\n",
      "Batch 4, Loss: 232.64788818359375, average loss 2.1766866302490233\n",
      "Batch 5, Loss: 288.9428405761719, average loss 2.3192349853515624\n",
      "Batch 6, Loss: 224.08395385742188, average loss 2.3061690775553387\n",
      "Batch 7, Loss: 262.6539611816406, average loss 2.351936296735491\n",
      "Batch 8, Loss: 225.9435577392578, average loss 2.340373706817627\n",
      "Batch 9, Loss: 263.58251953125, average loss 2.373201649983724\n",
      "Batch 10, Loss: 203.9399871826172, average loss 2.339821472167969\n",
      "Total loss: 2277.3416900634766\n",
      "Validation loss: 2.2773416900634764\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 007927\n",
      "\tInput: 94164\tPrediction: 13264\n",
      "Epoch 234, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 181.31431579589844, average loss 1.8131431579589843\n",
      "Batch 2, Loss: 206.4713134765625, average loss 1.9389281463623047\n",
      "Batch 3, Loss: 220.5972900390625, average loss 2.027943064371745\n",
      "Batch 4, Loss: 242.28634643554688, average loss 2.1266731643676757\n",
      "Batch 5, Loss: 248.777099609375, average loss 2.1988927307128905\n",
      "Batch 6, Loss: 252.1090087890625, average loss 2.252592290242513\n",
      "Batch 7, Loss: 201.21588134765625, average loss 2.21824465070452\n",
      "Batch 8, Loss: 221.04336547851562, average loss 2.2172682762145994\n",
      "Batch 9, Loss: 186.01187133789062, average loss 2.177584991455078\n",
      "Batch 10, Loss: 191.3314208984375, average loss 2.151157913208008\n",
      "Total loss: 5965.780303955078\n",
      "Validation loss: 5.9657803039550785\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 7041\n",
      "\tInput: 64009172\tPrediction: 749003472\n",
      "Epoch 235, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 212.322509765625, average loss 2.12322509765625\n",
      "Batch 2, Loss: 196.13229370117188, average loss 2.0422740173339844\n",
      "Batch 3, Loss: 242.35818481445312, average loss 2.1693766276041666\n",
      "Batch 4, Loss: 231.94554138183594, average loss 2.206896324157715\n",
      "Batch 5, Loss: 239.335205078125, average loss 2.244187469482422\n",
      "Batch 6, Loss: 198.8685760498047, average loss 2.2016038513183593\n",
      "Batch 7, Loss: 224.22349548339844, average loss 2.2074082946777343\n",
      "Batch 8, Loss: 194.40476989746094, average loss 2.174488220214844\n",
      "Batch 9, Loss: 194.61672973632812, average loss 2.1491192287868923\n",
      "Batch 10, Loss: 192.162353515625, average loss 2.126369659423828\n",
      "Total loss: 2180.6529846191406\n",
      "Validation loss: 2.1806529846191407\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172275\n",
      "\tInput: 6824\tPrediction: 66544\n",
      "Epoch 236, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 193.8372802734375, average loss 1.938372802734375\n",
      "Batch 2, Loss: 225.63760375976562, average loss 2.0973744201660156\n",
      "Batch 3, Loss: 209.67874145507812, average loss 2.097178751627604\n",
      "Batch 4, Loss: 258.33050537109375, average loss 2.2187103271484374\n",
      "Batch 5, Loss: 167.1143798828125, average loss 2.109197021484375\n",
      "Batch 6, Loss: 209.0306396484375, average loss 2.106048583984375\n",
      "Batch 7, Loss: 188.76284790039062, average loss 2.074845711844308\n",
      "Batch 8, Loss: 167.9606475830078, average loss 2.0254408073425294\n",
      "Batch 9, Loss: 188.37924194335938, average loss 2.00970209757487\n",
      "Batch 10, Loss: 222.2127685546875, average loss 2.0309446563720703\n",
      "Total loss: 2157.4708251953125\n",
      "Validation loss: 2.1574708251953125\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9338531\n",
      "\tInput: 135\tPrediction: 265\n",
      "Epoch 237, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 225.59877014160156, average loss 2.2559877014160157\n",
      "Batch 2, Loss: 210.02064514160156, average loss 2.1780970764160155\n",
      "Batch 3, Loss: 255.45098876953125, average loss 2.3035680135091146\n",
      "Batch 4, Loss: 205.87481689453125, average loss 2.242363052368164\n",
      "Batch 5, Loss: 203.9982452392578, average loss 2.201886932373047\n",
      "Batch 6, Loss: 194.37457275390625, average loss 2.1588633982340495\n",
      "Batch 7, Loss: 194.7344512939453, average loss 2.1286464146205355\n",
      "Batch 8, Loss: 192.7516326904297, average loss 2.103505153656006\n",
      "Batch 9, Loss: 196.33282470703125, average loss 2.087929941813151\n",
      "Batch 10, Loss: 200.59097290039062, average loss 2.0797279205322265\n",
      "Total loss: 2282.454147338867\n",
      "Validation loss: 2.2824541473388673\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 1999993121\n",
      "Epoch 238, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 227.17587280273438, average loss 2.271758728027344\n",
      "Batch 2, Loss: 247.35350036621094, average loss 2.3726468658447266\n",
      "Batch 3, Loss: 184.2597198486328, average loss 2.195963643391927\n",
      "Batch 4, Loss: 213.87289428710938, average loss 2.181654968261719\n",
      "Batch 5, Loss: 180.23779296875, average loss 2.105799560546875\n",
      "Batch 6, Loss: 178.20086669921875, average loss 2.0518344116210936\n",
      "Batch 7, Loss: 212.66793823242188, average loss 2.0625265502929686\n",
      "Batch 8, Loss: 198.65386962890625, average loss 2.0530280685424804\n",
      "Batch 9, Loss: 214.10736083984375, average loss 2.0628109063042537\n",
      "Batch 10, Loss: 256.6312255859375, average loss 2.1131610412597657\n",
      "Total loss: 2133.878921508789\n",
      "Validation loss: 2.133878921508789\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7215664450\n",
      "\tInput: 11002\tPrediction: 44424\n",
      "Epoch 239, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 267.1176452636719, average loss 2.6711764526367188\n",
      "Batch 2, Loss: 174.01136779785156, average loss 2.205645065307617\n",
      "Batch 3, Loss: 205.691162109375, average loss 2.1560672505696616\n",
      "Batch 4, Loss: 209.394287109375, average loss 2.1405361557006835\n",
      "Batch 5, Loss: 185.03945922851562, average loss 2.082507843017578\n",
      "Batch 6, Loss: 196.50738525390625, average loss 2.0629355112711587\n",
      "Batch 7, Loss: 200.4582977294922, average loss 2.0545994349888392\n",
      "Batch 8, Loss: 178.78793334960938, average loss 2.021259422302246\n",
      "Batch 9, Loss: 245.17520141601562, average loss 2.0690919325086807\n",
      "Batch 10, Loss: 217.1912078857422, average loss 2.0793739471435546\n",
      "Total loss: 2111.856964111328\n",
      "Validation loss: 2.111856964111328\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 6497473\n",
      "Epoch 240, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 189.196533203125, average loss 1.89196533203125\n",
      "Batch 2, Loss: 204.9556884765625, average loss 1.9707611083984375\n",
      "Batch 3, Loss: 202.63394165039062, average loss 1.9892872111002604\n",
      "Batch 4, Loss: 181.4789276123047, average loss 1.945662727355957\n",
      "Batch 5, Loss: 180.92190551757812, average loss 1.918373992919922\n",
      "Batch 6, Loss: 209.79931640625, average loss 1.9483105214436849\n",
      "Batch 7, Loss: 209.52305603027344, average loss 1.969299098423549\n",
      "Batch 8, Loss: 275.45867919921875, average loss 2.067460060119629\n",
      "Batch 9, Loss: 209.05966186523438, average loss 2.0700307888454863\n",
      "Batch 10, Loss: 242.59808349609375, average loss 2.105625793457031\n",
      "Total loss: 2024.5081481933594\n",
      "Validation loss: 2.0245081481933593\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 2448499\n",
      "Epoch 241, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 191.20913696289062, average loss 1.9120913696289064\n",
      "Batch 2, Loss: 191.72640991210938, average loss 1.914677734375\n",
      "Batch 3, Loss: 184.44586181640625, average loss 1.8912713623046875\n",
      "Batch 4, Loss: 183.92221069335938, average loss 1.8782590484619142\n",
      "Batch 5, Loss: 186.17886352539062, average loss 1.8749649658203125\n",
      "Batch 6, Loss: 204.76974487304688, average loss 1.9037537129720052\n",
      "Batch 7, Loss: 269.0160217285156, average loss 2.016097499302455\n",
      "Batch 8, Loss: 192.97442626953125, average loss 2.0053033447265625\n",
      "Batch 9, Loss: 227.8099365234375, average loss 2.035614013671875\n",
      "Batch 10, Loss: 184.82168579101562, average loss 2.0168742980957033\n",
      "Total loss: 2309.092544555664\n",
      "Validation loss: 2.3090925445556643\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 2731419\n",
      "Epoch 242, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 182.79409790039062, average loss 1.8279409790039063\n",
      "Batch 2, Loss: 163.00234985351562, average loss 1.7289822387695313\n",
      "Batch 3, Loss: 179.19512939453125, average loss 1.749971923828125\n",
      "Batch 4, Loss: 204.867919921875, average loss 1.8246487426757811\n",
      "Batch 5, Loss: 212.41993713378906, average loss 1.884558868408203\n",
      "Batch 6, Loss: 262.11309814453125, average loss 2.0073208872477215\n",
      "Batch 7, Loss: 185.18792724609375, average loss 1.9851149422781809\n",
      "Batch 8, Loss: 223.89535522460938, average loss 2.01684476852417\n",
      "Batch 9, Loss: 165.07992553710938, average loss 1.9761730448404948\n",
      "Batch 10, Loss: 188.15740966796875, average loss 1.9667131500244142\n",
      "Total loss: 1905.3723907470703\n",
      "Validation loss: 1.9053723907470703\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 045\n",
      "Epoch 243, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 163.78311157226562, average loss 1.6378311157226562\n",
      "Batch 2, Loss: 186.40371704101562, average loss 1.7509341430664063\n",
      "Batch 3, Loss: 177.74981689453125, average loss 1.759788818359375\n",
      "Batch 4, Loss: 174.21502685546875, average loss 1.755379180908203\n",
      "Batch 5, Loss: 214.10281372070312, average loss 1.8325089721679688\n",
      "Batch 6, Loss: 195.1616668701172, average loss 1.8523602549235025\n",
      "Batch 7, Loss: 226.07302856445312, average loss 1.9106988307407924\n",
      "Batch 8, Loss: 159.88392639160156, average loss 1.8717163848876952\n",
      "Batch 9, Loss: 165.22731018066406, average loss 1.8473337978786892\n",
      "Batch 10, Loss: 183.22866821289062, average loss 1.845829086303711\n",
      "Total loss: 1964.5711059570312\n",
      "Validation loss: 1.9645711059570312\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607927\n",
      "\tInput: 94164\tPrediction: 34494\n",
      "Epoch 244, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 185.7711181640625, average loss 1.857711181640625\n",
      "Batch 2, Loss: 203.75392150878906, average loss 1.9476251983642578\n",
      "Batch 3, Loss: 170.34820556640625, average loss 1.866244150797526\n",
      "Batch 4, Loss: 251.97564697265625, average loss 2.0296222305297853\n",
      "Batch 5, Loss: 205.79092407226562, average loss 2.0352796325683595\n",
      "Batch 6, Loss: 224.21810913085938, average loss 2.0697632090250653\n",
      "Batch 7, Loss: 183.93353271484375, average loss 2.036844940185547\n",
      "Batch 8, Loss: 183.61062622070312, average loss 2.0117526054382324\n",
      "Batch 9, Loss: 166.26947021484375, average loss 1.9729683939615885\n",
      "Batch 10, Loss: 149.62240600585938, average loss 1.925293960571289\n",
      "Total loss: 1776.5138549804688\n",
      "Validation loss: 1.7765138549804687\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 91043472\n",
      "Epoch 245, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 206.68406677246094, average loss 2.0668406677246094\n",
      "Batch 2, Loss: 160.8710174560547, average loss 1.837775421142578\n",
      "Batch 3, Loss: 211.94326782226562, average loss 1.9316611735026041\n",
      "Batch 4, Loss: 188.0421905517578, average loss 1.9188513565063476\n",
      "Batch 5, Loss: 223.58187866210938, average loss 1.982244842529297\n",
      "Batch 6, Loss: 184.27577209472656, average loss 1.9589969889322916\n",
      "Batch 7, Loss: 170.18975830078125, average loss 1.9222685023716517\n",
      "Batch 8, Loss: 171.12762451171875, average loss 1.8958944702148437\n",
      "Batch 9, Loss: 161.7771759033203, average loss 1.864991946750217\n",
      "Batch 10, Loss: 200.8463592529297, average loss 1.879339111328125\n",
      "Total loss: 1804.8282470703125\n",
      "Validation loss: 1.8048282470703125\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172475\n",
      "\tInput: 6824\tPrediction: 6822\n",
      "Epoch 246, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 171.04071044921875, average loss 1.7104071044921876\n",
      "Batch 2, Loss: 212.0438690185547, average loss 1.9154228973388672\n",
      "Batch 3, Loss: 201.02928161621094, average loss 1.9470462036132812\n",
      "Batch 4, Loss: 249.45303344726562, average loss 2.083917236328125\n",
      "Batch 5, Loss: 175.47283935546875, average loss 2.0180794677734375\n",
      "Batch 6, Loss: 173.6573486328125, average loss 1.9711618041992187\n",
      "Batch 7, Loss: 165.72320556640625, average loss 1.926314697265625\n",
      "Batch 8, Loss: 199.34848022460938, average loss 1.9347109603881836\n",
      "Batch 9, Loss: 231.15869140625, average loss 1.9765860663519965\n",
      "Batch 10, Loss: 195.02195739746094, average loss 1.9739494171142578\n",
      "Total loss: 1956.733627319336\n",
      "Validation loss: 1.9567336273193359\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9558551\n",
      "\tInput: 135\tPrediction: 1495\n",
      "Epoch 247, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 225.76980590820312, average loss 2.2576980590820312\n",
      "Batch 2, Loss: 264.46807861328125, average loss 2.451189422607422\n",
      "Batch 3, Loss: 236.21632385253906, average loss 2.4215140279134113\n",
      "Batch 4, Loss: 148.51058959960938, average loss 2.1874119949340822\n",
      "Batch 5, Loss: 191.78297424316406, average loss 2.1334955444335937\n",
      "Batch 6, Loss: 158.43702697753906, average loss 2.041974665323893\n",
      "Batch 7, Loss: 169.29388427734375, average loss 1.9921124049595424\n",
      "Batch 8, Loss: 221.60772705078125, average loss 2.020108013153076\n",
      "Batch 9, Loss: 178.26821899414062, average loss 1.9937273661295574\n",
      "Batch 10, Loss: 171.3233642578125, average loss 1.965677993774414\n",
      "Total loss: 1910.4783630371094\n",
      "Validation loss: 1.9104783630371094\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 96405562\n",
      "Epoch 248, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 206.77142333984375, average loss 2.0677142333984375\n",
      "Batch 2, Loss: 244.84133911132812, average loss 2.2580638122558594\n",
      "Batch 3, Loss: 178.38076782226562, average loss 2.099978434244792\n",
      "Batch 4, Loss: 191.18124389648438, average loss 2.0529369354248046\n",
      "Batch 5, Loss: 153.45623779296875, average loss 1.9492620239257812\n",
      "Batch 6, Loss: 169.05392456054688, average loss 1.9061415608723957\n",
      "Batch 7, Loss: 197.05836486816406, average loss 1.9153475734165737\n",
      "Batch 8, Loss: 186.69210815429688, average loss 1.909294261932373\n",
      "Batch 9, Loss: 140.999755859375, average loss 1.8538168504503039\n",
      "Batch 10, Loss: 202.7606201171875, average loss 1.871195785522461\n",
      "Total loss: 2280.376495361328\n",
      "Validation loss: 2.280376495361328\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7215664450\n",
      "\tInput: 11002\tPrediction: 14003\n",
      "Epoch 249, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 224.70126342773438, average loss 2.2470126342773438\n",
      "Batch 2, Loss: 156.753662109375, average loss 1.9072746276855468\n",
      "Batch 3, Loss: 196.67724609375, average loss 1.9271072387695312\n",
      "Batch 4, Loss: 145.99191284179688, average loss 1.8103102111816407\n",
      "Batch 5, Loss: 152.83160400390625, average loss 1.753911376953125\n",
      "Batch 6, Loss: 179.1002197265625, average loss 1.7600931803385416\n",
      "Batch 7, Loss: 198.11529541015625, average loss 1.7916731480189731\n",
      "Batch 8, Loss: 166.53091430664062, average loss 1.7758776473999023\n",
      "Batch 9, Loss: 195.61456298828125, average loss 1.7959074232313368\n",
      "Batch 10, Loss: 192.7451934814453, average loss 1.8090618743896485\n",
      "Total loss: 2206.813949584961\n",
      "Validation loss: 2.206813949584961\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 64917379\n",
      "Epoch 250, lr=[0.0003125]\n",
      "-----------------------\n",
      "Batch 1, Loss: 145.88668823242188, average loss 1.4588668823242188\n",
      "Batch 2, Loss: 213.7502899169922, average loss 1.7981848907470703\n",
      "Batch 3, Loss: 163.72998046875, average loss 1.7445565287272136\n",
      "Batch 4, Loss: 143.55963134765625, average loss 1.6673164749145508\n",
      "Batch 5, Loss: 139.3649139404297, average loss 1.6125830078125\n",
      "Batch 6, Loss: 189.2474365234375, average loss 1.6592315673828124\n",
      "Batch 7, Loss: 185.81698608398438, average loss 1.6876513235909598\n",
      "Batch 8, Loss: 194.59817504882812, average loss 1.719942626953125\n",
      "Batch 9, Loss: 189.59933471679688, average loss 1.7395038180881077\n",
      "Batch 10, Loss: 227.12893676757812, average loss 1.792682373046875\n",
      "Total loss: 1742.6267852783203\n",
      "Validation loss: 1.7426267852783204\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 24494499\n",
      "Epoch 251, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 181.42747497558594, average loss 1.8142747497558593\n",
      "Batch 2, Loss: 151.7615966796875, average loss 1.6659453582763672\n",
      "Batch 3, Loss: 134.39083862304688, average loss 1.5585997009277344\n",
      "Batch 4, Loss: 141.60821533203125, average loss 1.522970314025879\n",
      "Batch 5, Loss: 171.12744140625, average loss 1.560631134033203\n",
      "Batch 6, Loss: 151.51443481445312, average loss 1.5530500030517578\n",
      "Batch 7, Loss: 189.85107421875, average loss 1.6024015372140068\n",
      "Batch 8, Loss: 165.8040771484375, average loss 1.6093564414978028\n",
      "Batch 9, Loss: 200.2115478515625, average loss 1.652996334499783\n",
      "Batch 10, Loss: 124.52714538574219, average loss 1.612223846435547\n",
      "Total loss: 1640.4641418457031\n",
      "Validation loss: 1.640464141845703\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 173209\n",
      "Epoch 252, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 157.19699096679688, average loss 1.5719699096679687\n",
      "Batch 2, Loss: 140.3152313232422, average loss 1.4875611114501952\n",
      "Batch 3, Loss: 141.25479125976562, average loss 1.4625567118326823\n",
      "Batch 4, Loss: 142.55389404296875, average loss 1.4533022689819335\n",
      "Batch 5, Loss: 138.0728759765625, average loss 1.4387875671386718\n",
      "Batch 6, Loss: 186.14358520507812, average loss 1.5092289479573568\n",
      "Batch 7, Loss: 182.27064514160156, average loss 1.554011448451451\n",
      "Batch 8, Loss: 202.33203125, average loss 1.6126750564575196\n",
      "Batch 9, Loss: 160.16110229492188, average loss 1.6114457194010416\n",
      "Batch 10, Loss: 161.08795166015625, average loss 1.6113890991210937\n",
      "Total loss: 1571.5783081054688\n",
      "Validation loss: 1.5715783081054688\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 1348\n",
      "Epoch 253, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 142.46031188964844, average loss 1.4246031188964843\n",
      "Batch 2, Loss: 136.04071044921875, average loss 1.3925051116943359\n",
      "Batch 3, Loss: 165.2032470703125, average loss 1.4790142313639323\n",
      "Batch 4, Loss: 138.07725524902344, average loss 1.454453811645508\n",
      "Batch 5, Loss: 191.1896514892578, average loss 1.5459423522949218\n",
      "Batch 6, Loss: 170.7947235107422, average loss 1.5729431660970052\n",
      "Batch 7, Loss: 197.3128662109375, average loss 1.630112522670201\n",
      "Batch 8, Loss: 135.5695343017578, average loss 1.595810375213623\n",
      "Batch 9, Loss: 182.31698608398438, average loss 1.6210725402832031\n",
      "Batch 10, Loss: 146.46726989746094, average loss 1.6054325561523437\n",
      "Total loss: 1632.5520935058594\n",
      "Validation loss: 1.6325520935058593\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607927\n",
      "\tInput: 94164\tPrediction: 231164\n",
      "Epoch 254, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 134.09906005859375, average loss 1.3409906005859376\n",
      "Batch 2, Loss: 173.73175048828125, average loss 1.539154052734375\n",
      "Batch 3, Loss: 131.91366577148438, average loss 1.465814921061198\n",
      "Batch 4, Loss: 194.99798583984375, average loss 1.5868561553955078\n",
      "Batch 5, Loss: 170.61192321777344, average loss 1.6107087707519532\n",
      "Batch 6, Loss: 212.374267578125, average loss 1.6962144215901693\n",
      "Batch 7, Loss: 140.2211456298828, average loss 1.6542139979771204\n",
      "Batch 8, Loss: 173.81703186035156, average loss 1.6647085380554199\n",
      "Batch 9, Loss: 158.97189331054688, average loss 1.6563763597276475\n",
      "Batch 10, Loss: 135.1269989013672, average loss 1.62586572265625\n",
      "Total loss: 1649.4781646728516\n",
      "Validation loss: 1.6494781646728516\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 92443472\n",
      "Epoch 255, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 171.2484588623047, average loss 1.7124845886230469\n",
      "Batch 2, Loss: 127.91160583496094, average loss 1.4958003234863282\n",
      "Batch 3, Loss: 187.72499084472656, average loss 1.622950185139974\n",
      "Batch 4, Loss: 157.9720458984375, average loss 1.6121427536010742\n",
      "Batch 5, Loss: 198.411376953125, average loss 1.6865369567871094\n",
      "Batch 6, Loss: 136.70521545410156, average loss 1.6332894897460937\n",
      "Batch 7, Loss: 178.75408935546875, average loss 1.655325404575893\n",
      "Batch 8, Loss: 148.79702758789062, average loss 1.6344060134887695\n",
      "Batch 9, Loss: 145.34930419921875, average loss 1.6143045722113716\n",
      "Batch 10, Loss: 133.45159912109375, average loss 1.586325714111328\n",
      "Total loss: 1597.7300720214844\n",
      "Validation loss: 1.5977300720214844\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172475\n",
      "\tInput: 6824\tPrediction: 952\n",
      "Epoch 256, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 157.6604461669922, average loss 1.576604461669922\n",
      "Batch 2, Loss: 200.06973266601562, average loss 1.788650894165039\n",
      "Batch 3, Loss: 159.90090942382812, average loss 1.7254369608561198\n",
      "Batch 4, Loss: 177.40733337402344, average loss 1.7375960540771485\n",
      "Batch 5, Loss: 175.93800354003906, average loss 1.741952850341797\n",
      "Batch 6, Loss: 171.61062622070312, average loss 1.7376450856526693\n",
      "Batch 7, Loss: 145.5005340576172, average loss 1.6972679792131697\n",
      "Batch 8, Loss: 119.9814453125, average loss 1.6350862884521484\n",
      "Batch 9, Loss: 133.90609741210938, average loss 1.602194586859809\n",
      "Batch 10, Loss: 166.5210723876953, average loss 1.6084962005615235\n",
      "Total loss: 1549.6290740966797\n",
      "Validation loss: 1.5496290740966796\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9358531\n",
      "\tInput: 135\tPrediction: 435\n",
      "Epoch 257, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 158.8362579345703, average loss 1.588362579345703\n",
      "Batch 2, Loss: 176.1226806640625, average loss 1.674794692993164\n",
      "Batch 3, Loss: 198.71371459960938, average loss 1.7789088439941407\n",
      "Batch 4, Loss: 122.407470703125, average loss 1.640200309753418\n",
      "Batch 5, Loss: 172.6449432373047, average loss 1.6574501342773438\n",
      "Batch 6, Loss: 148.25808715820312, average loss 1.6283052571614582\n",
      "Batch 7, Loss: 140.61306762695312, average loss 1.5965660313197545\n",
      "Batch 8, Loss: 144.43663024902344, average loss 1.5775410652160644\n",
      "Batch 9, Loss: 166.77410888671875, average loss 1.5875632900661891\n",
      "Batch 10, Loss: 143.6538543701172, average loss 1.5724608154296875\n",
      "Total loss: 1548.0720825195312\n",
      "Validation loss: 1.5480720825195313\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 96431994\n",
      "Epoch 258, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 163.07601928710938, average loss 1.6307601928710938\n",
      "Batch 2, Loss: 189.46795654296875, average loss 1.7627198791503906\n",
      "Batch 3, Loss: 139.17385864257812, average loss 1.6390594482421874\n",
      "Batch 4, Loss: 143.72900390625, average loss 1.5886170959472656\n",
      "Batch 5, Loss: 137.80978393554688, average loss 1.5465132446289063\n",
      "Batch 6, Loss: 138.28416442871094, average loss 1.51923464457194\n",
      "Batch 7, Loss: 161.65061950683594, average loss 1.533130580357143\n",
      "Batch 8, Loss: 151.87525939941406, average loss 1.5313333320617675\n",
      "Batch 9, Loss: 127.07821655273438, average loss 1.5023832024468315\n",
      "Batch 10, Loss: 181.09170532226562, average loss 1.533236587524414\n",
      "Total loss: 1558.2240295410156\n",
      "Validation loss: 1.5582240295410157\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7215664450\n",
      "\tInput: 11002\tPrediction: 1022\n",
      "Epoch 259, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 193.98483276367188, average loss 1.9398483276367187\n",
      "Batch 2, Loss: 122.89545440673828, average loss 1.5844014358520508\n",
      "Batch 3, Loss: 165.2242889404297, average loss 1.6070152537027995\n",
      "Batch 4, Loss: 135.5706024169922, average loss 1.54418794631958\n",
      "Batch 5, Loss: 135.88548278808594, average loss 1.5071213226318358\n",
      "Batch 6, Loss: 135.9724578857422, average loss 1.4825551986694336\n",
      "Batch 7, Loss: 166.04922485351562, average loss 1.5079747772216796\n",
      "Batch 8, Loss: 123.47054290771484, average loss 1.4738161087036132\n",
      "Batch 9, Loss: 185.37689208984375, average loss 1.5160330878363715\n",
      "Batch 10, Loss: 163.5264892578125, average loss 1.527956268310547\n",
      "Total loss: 1533.1293487548828\n",
      "Validation loss: 1.5331293487548827\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 6497473\n",
      "Epoch 260, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 123.49517822265625, average loss 1.2349517822265625\n",
      "Batch 2, Loss: 152.0233612060547, average loss 1.3775926971435546\n",
      "Batch 3, Loss: 145.54852294921875, average loss 1.4035568745930989\n",
      "Batch 4, Loss: 138.79910278320312, average loss 1.399665412902832\n",
      "Batch 5, Loss: 122.40864562988281, average loss 1.3645496215820312\n",
      "Batch 6, Loss: 168.5265350341797, average loss 1.4180022430419923\n",
      "Batch 7, Loss: 128.9019317626953, average loss 1.3995761108398437\n",
      "Batch 8, Loss: 174.1522979736328, average loss 1.4423194694519044\n",
      "Batch 9, Loss: 157.06736755371094, average loss 1.4565810479058159\n",
      "Batch 10, Loss: 187.05349731445312, average loss 1.4979764404296876\n",
      "Total loss: 1585.1555938720703\n",
      "Validation loss: 1.5851555938720703\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 345499\n",
      "Epoch 261, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 157.6615753173828, average loss 1.576615753173828\n",
      "Batch 2, Loss: 145.76734924316406, average loss 1.5171446228027343\n",
      "Batch 3, Loss: 109.3599853515625, average loss 1.3759630330403645\n",
      "Batch 4, Loss: 115.72810363769531, average loss 1.3212925338745116\n",
      "Batch 5, Loss: 159.96221923828125, average loss 1.376958465576172\n",
      "Batch 6, Loss: 137.59017944335938, average loss 1.3767823537190755\n",
      "Batch 7, Loss: 186.05673217773438, average loss 1.4458944920131138\n",
      "Batch 8, Loss: 163.2552490234375, average loss 1.4692267417907714\n",
      "Batch 9, Loss: 171.77816772460938, average loss 1.496843956841363\n",
      "Batch 10, Loss: 119.76445007324219, average loss 1.4669240112304687\n",
      "Total loss: 1561.586570739746\n",
      "Validation loss: 1.561586570739746\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 47903\n",
      "Epoch 262, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 140.828369140625, average loss 1.40828369140625\n",
      "Batch 2, Loss: 121.31280517578125, average loss 1.3107058715820312\n",
      "Batch 3, Loss: 138.52737426757812, average loss 1.3355618286132813\n",
      "Batch 4, Loss: 161.8175811767578, average loss 1.4062153244018554\n",
      "Batch 5, Loss: 132.89923095703125, average loss 1.390770721435547\n",
      "Batch 6, Loss: 168.96029663085938, average loss 1.4405760955810547\n",
      "Batch 7, Loss: 165.02772521972656, average loss 1.470533403669085\n",
      "Batch 8, Loss: 189.55429077148438, average loss 1.5236595916748046\n",
      "Batch 9, Loss: 133.55099487304688, average loss 1.5027540757921007\n",
      "Batch 10, Loss: 154.95635986328125, average loss 1.507435028076172\n",
      "Total loss: 1465.864616394043\n",
      "Validation loss: 1.465864616394043\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 2325\n",
      "Epoch 263, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 125.5516128540039, average loss 1.2555161285400391\n",
      "Batch 2, Loss: 137.6640167236328, average loss 1.3160781478881836\n",
      "Batch 3, Loss: 153.25357055664062, average loss 1.388230667114258\n",
      "Batch 4, Loss: 122.29763793945312, average loss 1.3469170951843261\n",
      "Batch 5, Loss: 158.41439819335938, average loss 1.3943624725341797\n",
      "Batch 6, Loss: 156.53585815429688, average loss 1.4228618240356445\n",
      "Batch 7, Loss: 193.15347290039062, average loss 1.4955293818882534\n",
      "Batch 8, Loss: 134.7285919189453, average loss 1.4769989490509032\n",
      "Batch 9, Loss: 143.2345428466797, average loss 1.4720374467637805\n",
      "Batch 10, Loss: 129.88404846191406, average loss 1.4547177505493165\n",
      "Total loss: 1472.2207870483398\n",
      "Validation loss: 1.47222078704834\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607927\n",
      "\tInput: 94164\tPrediction: 134134\n",
      "Epoch 264, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 148.55731201171875, average loss 1.4855731201171876\n",
      "Batch 2, Loss: 153.78329467773438, average loss 1.5117030334472656\n",
      "Batch 3, Loss: 140.9334716796875, average loss 1.4775802612304687\n",
      "Batch 4, Loss: 178.76954650878906, average loss 1.5551090621948243\n",
      "Batch 5, Loss: 177.64797973632812, average loss 1.5993832092285156\n",
      "Batch 6, Loss: 199.6988983154297, average loss 1.665650838216146\n",
      "Batch 7, Loss: 134.70603942871094, average loss 1.620137917654855\n",
      "Batch 8, Loss: 156.3514404296875, average loss 1.6130599784851074\n",
      "Batch 9, Loss: 140.67857360839844, average loss 1.590140618218316\n",
      "Batch 10, Loss: 126.14436340332031, average loss 1.5572709197998047\n",
      "Total loss: 1539.3793029785156\n",
      "Validation loss: 1.5393793029785157\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 94043479\n",
      "Epoch 265, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 161.0841827392578, average loss 1.6108418273925782\n",
      "Batch 2, Loss: 126.71416473388672, average loss 1.4389917373657226\n",
      "Batch 3, Loss: 162.2523956298828, average loss 1.500169143676758\n",
      "Batch 4, Loss: 160.46649169921875, average loss 1.5262930870056153\n",
      "Batch 5, Loss: 199.7535400390625, average loss 1.6205415496826172\n",
      "Batch 6, Loss: 133.10440063476562, average loss 1.5722919591267903\n",
      "Batch 7, Loss: 173.82763671875, average loss 1.5960040174211774\n",
      "Batch 8, Loss: 132.79815673828125, average loss 1.5625012111663819\n",
      "Batch 9, Loss: 122.5362548828125, average loss 1.5250413597954644\n",
      "Batch 10, Loss: 122.02604675292969, average loss 1.4945632705688476\n",
      "Total loss: 1520.1851501464844\n",
      "Validation loss: 1.5201851501464845\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172475\n",
      "\tInput: 6824\tPrediction: 66532\n",
      "Epoch 266, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 128.68917846679688, average loss 1.2868917846679688\n",
      "Batch 2, Loss: 180.0576171875, average loss 1.5437339782714843\n",
      "Batch 3, Loss: 163.32452392578125, average loss 1.573571065266927\n",
      "Batch 4, Loss: 194.94326782226562, average loss 1.6675364685058593\n",
      "Batch 5, Loss: 125.27155303955078, average loss 1.584572280883789\n",
      "Batch 6, Loss: 161.40966796875, average loss 1.5894930140177408\n",
      "Batch 7, Loss: 142.91712951660156, average loss 1.5665899113246373\n",
      "Batch 8, Loss: 130.03680419921875, average loss 1.5333121776580811\n",
      "Batch 9, Loss: 137.6596221923828, average loss 1.5158992936876086\n",
      "Batch 10, Loss: 155.89154052734375, average loss 1.5202009048461913\n",
      "Total loss: 1501.086067199707\n",
      "Validation loss: 1.501086067199707\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9338531\n",
      "\tInput: 135\tPrediction: 435\n",
      "Epoch 267, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 172.02493286132812, average loss 1.7202493286132812\n",
      "Batch 2, Loss: 165.29916381835938, average loss 1.6866204833984375\n",
      "Batch 3, Loss: 215.70843505859375, average loss 1.8434417724609375\n",
      "Batch 4, Loss: 136.30569458007812, average loss 1.7233455657958985\n",
      "Batch 5, Loss: 160.64874267578125, average loss 1.6999739379882812\n",
      "Batch 6, Loss: 130.01837158203125, average loss 1.6333422342936197\n",
      "Batch 7, Loss: 116.68498992919922, average loss 1.5667004721505302\n",
      "Batch 8, Loss: 119.69824981689453, average loss 1.5204857254028321\n",
      "Batch 9, Loss: 135.39410400390625, average loss 1.5019807603624131\n",
      "Batch 10, Loss: 128.4826202392578, average loss 1.4802653045654297\n",
      "Total loss: 1462.0526504516602\n",
      "Validation loss: 1.4620526504516602\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 19303334\n",
      "Epoch 268, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 165.61549377441406, average loss 1.6561549377441407\n",
      "Batch 2, Loss: 179.42758178710938, average loss 1.7252153778076171\n",
      "Batch 3, Loss: 116.79097747802734, average loss 1.5394468434651694\n",
      "Batch 4, Loss: 151.39019775390625, average loss 1.5330606269836426\n",
      "Batch 5, Loss: 134.4407958984375, average loss 1.495330093383789\n",
      "Batch 6, Loss: 122.4714584350586, average loss 1.4502275085449219\n",
      "Batch 7, Loss: 134.4614715576172, average loss 1.4351399666922433\n",
      "Batch 8, Loss: 145.716552734375, average loss 1.4378931617736816\n",
      "Batch 9, Loss: 126.77388000488281, average loss 1.4189871215820313\n",
      "Batch 10, Loss: 184.71353149414062, average loss 1.4618019409179688\n",
      "Total loss: 1478.7588195800781\n",
      "Validation loss: 1.478758819580078\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7215664450\n",
      "\tInput: 11002\tPrediction: 4400\n",
      "Epoch 269, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 174.2704620361328, average loss 1.7427046203613281\n",
      "Batch 2, Loss: 133.25558471679688, average loss 1.5376302337646484\n",
      "Batch 3, Loss: 156.69557189941406, average loss 1.5474053955078124\n",
      "Batch 4, Loss: 141.42080688476562, average loss 1.5141060638427735\n",
      "Batch 5, Loss: 125.66928100585938, average loss 1.4626234130859375\n",
      "Batch 6, Loss: 134.50640869140625, average loss 1.4430301920572917\n",
      "Batch 7, Loss: 141.45770263671875, average loss 1.4389654541015624\n",
      "Batch 8, Loss: 127.67960357666016, average loss 1.4186942768096924\n",
      "Batch 9, Loss: 168.68453979492188, average loss 1.4484888458251952\n",
      "Batch 10, Loss: 153.77395629882812, average loss 1.457413917541504\n",
      "Total loss: 1438.3347702026367\n",
      "Validation loss: 1.4383347702026368\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 6467473\n",
      "Epoch 270, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 118.6191177368164, average loss 1.1861911773681642\n",
      "Batch 2, Loss: 165.99630737304688, average loss 1.4230771255493164\n",
      "Batch 3, Loss: 137.30215454101562, average loss 1.4063919321695963\n",
      "Batch 4, Loss: 128.080322265625, average loss 1.3749947547912598\n",
      "Batch 5, Loss: 140.39068603515625, average loss 1.3807771759033203\n",
      "Batch 6, Loss: 152.08200073242188, average loss 1.4041176478068034\n",
      "Batch 7, Loss: 121.85624694824219, average loss 1.3776097651890347\n",
      "Batch 8, Loss: 176.16915893554688, average loss 1.4256199932098388\n",
      "Batch 9, Loss: 147.1792449951172, average loss 1.430750266181098\n",
      "Batch 10, Loss: 194.6642608642578, average loss 1.4823395004272462\n",
      "Total loss: 1472.6398162841797\n",
      "Validation loss: 1.4726398162841796\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 24454199\n",
      "Epoch 271, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 168.66897583007812, average loss 1.6866897583007812\n",
      "Batch 2, Loss: 122.81572723388672, average loss 1.4574235153198243\n",
      "Batch 3, Loss: 132.943359375, average loss 1.414760208129883\n",
      "Batch 4, Loss: 132.31094360351562, average loss 1.391847515106201\n",
      "Batch 5, Loss: 132.63552856445312, average loss 1.378749069213867\n",
      "Batch 6, Loss: 107.03471374511719, average loss 1.327348747253418\n",
      "Batch 7, Loss: 172.30152893066406, average loss 1.3838725389753068\n",
      "Batch 8, Loss: 169.17160034179688, average loss 1.4223529720306396\n",
      "Batch 9, Loss: 202.18014526367188, average loss 1.4889583587646484\n",
      "Batch 10, Loss: 114.35107421875, average loss 1.4544135971069336\n",
      "Total loss: 1407.7415084838867\n",
      "Validation loss: 1.4077415084838867\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 47909\n",
      "Epoch 272, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 132.32647705078125, average loss 1.3232647705078124\n",
      "Batch 2, Loss: 113.28955078125, average loss 1.2280801391601563\n",
      "Batch 3, Loss: 131.60488891601562, average loss 1.257403055826823\n",
      "Batch 4, Loss: 136.80117797851562, average loss 1.2850552368164063\n",
      "Batch 5, Loss: 141.40628051757812, average loss 1.3108567504882813\n",
      "Batch 6, Loss: 177.26950073242188, average loss 1.387829793294271\n",
      "Batch 7, Loss: 147.56045532226562, average loss 1.4003690447126116\n",
      "Batch 8, Loss: 182.4075927734375, average loss 1.453332405090332\n",
      "Batch 9, Loss: 138.61944580078125, average loss 1.4458726331922742\n",
      "Batch 10, Loss: 138.74459838867188, average loss 1.4400299682617188\n",
      "Total loss: 1456.2491912841797\n",
      "Validation loss: 1.4562491912841797\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 1325\n",
      "Epoch 273, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 116.69287872314453, average loss 1.1669287872314453\n",
      "Batch 2, Loss: 134.67962646484375, average loss 1.2568625259399413\n",
      "Batch 3, Loss: 136.48434448242188, average loss 1.292856165568034\n",
      "Batch 4, Loss: 115.5651626586914, average loss 1.258555030822754\n",
      "Batch 5, Loss: 169.88925170898438, average loss 1.3466225280761719\n",
      "Batch 6, Loss: 142.68063354492188, average loss 1.3599864959716796\n",
      "Batch 7, Loss: 205.0314483642578, average loss 1.458604779924665\n",
      "Batch 8, Loss: 126.29293060302734, average loss 1.4341453456878661\n",
      "Batch 9, Loss: 153.89122009277344, average loss 1.4457861073811848\n",
      "Batch 10, Loss: 125.17349243164062, average loss 1.426380989074707\n",
      "Total loss: 1434.7784729003906\n",
      "Validation loss: 1.4347784729003907\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607927\n",
      "\tInput: 94164\tPrediction: 124154\n",
      "Epoch 274, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 116.02583312988281, average loss 1.1602583312988282\n",
      "Batch 2, Loss: 140.80738830566406, average loss 1.2841661071777344\n",
      "Batch 3, Loss: 115.82406616210938, average loss 1.2421909586588542\n",
      "Batch 4, Loss: 164.31777954101562, average loss 1.3424376678466796\n",
      "Batch 5, Loss: 150.8242950439453, average loss 1.3755987243652343\n",
      "Batch 6, Loss: 176.60165405273438, average loss 1.4406683603922525\n",
      "Batch 7, Loss: 128.8771514892578, average loss 1.4189688110351562\n",
      "Batch 8, Loss: 143.59976196289062, average loss 1.421097412109375\n",
      "Batch 9, Loss: 138.9203338623047, average loss 1.4175536261664496\n",
      "Batch 10, Loss: 146.13070678710938, average loss 1.421928970336914\n",
      "Total loss: 1457.4719619750977\n",
      "Validation loss: 1.4574719619750978\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 640019472\n",
      "Epoch 275, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 144.5637664794922, average loss 1.4456376647949218\n",
      "Batch 2, Loss: 122.99441528320312, average loss 1.3377909088134765\n",
      "Batch 3, Loss: 150.17282104492188, average loss 1.3924366760253906\n",
      "Batch 4, Loss: 154.43365478515625, average loss 1.4304116439819337\n",
      "Batch 5, Loss: 187.41156005859375, average loss 1.5191524353027344\n",
      "Batch 6, Loss: 119.77963256835938, average loss 1.4655930836995443\n",
      "Batch 7, Loss: 174.5890350341797, average loss 1.5056355503627232\n",
      "Batch 8, Loss: 137.4771270751953, average loss 1.4892775154113769\n",
      "Batch 9, Loss: 117.1134262084961, average loss 1.453928265041775\n",
      "Batch 10, Loss: 139.93353271484375, average loss 1.4484689712524415\n",
      "Total loss: 1375.729866027832\n",
      "Validation loss: 1.375729866027832\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172475\n",
      "\tInput: 6824\tPrediction: 6624\n",
      "Epoch 276, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 134.4096221923828, average loss 1.3440962219238282\n",
      "Batch 2, Loss: 175.50595092773438, average loss 1.5495778656005859\n",
      "Batch 3, Loss: 141.61163330078125, average loss 1.5050906880696615\n",
      "Batch 4, Loss: 170.1865234375, average loss 1.554284324645996\n",
      "Batch 5, Loss: 137.4228057861328, average loss 1.5182730712890624\n",
      "Batch 6, Loss: 159.5550537109375, average loss 1.5311526489257812\n",
      "Batch 7, Loss: 129.62655639648438, average loss 1.4975973510742187\n",
      "Batch 8, Loss: 114.67730712890625, average loss 1.4537443161010741\n",
      "Batch 9, Loss: 131.02035522460938, average loss 1.4377953423394096\n",
      "Batch 10, Loss: 141.38528442382812, average loss 1.4354010925292968\n",
      "Total loss: 1357.7294235229492\n",
      "Validation loss: 1.3577294235229491\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9338531\n",
      "\tInput: 135\tPrediction: 15\n",
      "Epoch 277, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 164.35336303710938, average loss 1.6435336303710937\n",
      "Batch 2, Loss: 161.62965393066406, average loss 1.6299150848388673\n",
      "Batch 3, Loss: 157.714111328125, average loss 1.6123237609863281\n",
      "Batch 4, Loss: 118.92771911621094, average loss 1.5065621185302733\n",
      "Batch 5, Loss: 139.3243408203125, average loss 1.4838983764648437\n",
      "Batch 6, Loss: 140.05299377441406, average loss 1.47000363667806\n",
      "Batch 7, Loss: 124.63956451416016, average loss 1.4380596378871373\n",
      "Batch 8, Loss: 128.89649963378906, average loss 1.4194228076934814\n",
      "Batch 9, Loss: 146.802490234375, average loss 1.4248230404324003\n",
      "Batch 10, Loss: 114.92279052734375, average loss 1.397263526916504\n",
      "Total loss: 1422.1416473388672\n",
      "Validation loss: 1.4221416473388673\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 6343364\n",
      "Epoch 278, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 150.64793395996094, average loss 1.5064793395996094\n",
      "Batch 2, Loss: 176.44515991210938, average loss 1.6354654693603516\n",
      "Batch 3, Loss: 99.0880355834961, average loss 1.420603764851888\n",
      "Batch 4, Loss: 140.77664184570312, average loss 1.4173944282531739\n",
      "Batch 5, Loss: 149.78216552734375, average loss 1.4334798736572265\n",
      "Batch 6, Loss: 110.33636474609375, average loss 1.3784605026245118\n",
      "Batch 7, Loss: 136.54171752929688, average loss 1.376597170148577\n",
      "Batch 8, Loss: 141.4728240966797, average loss 1.3813635540008544\n",
      "Batch 9, Loss: 108.9808349609375, average loss 1.34896853129069\n",
      "Batch 10, Loss: 160.60023498535156, average loss 1.3746719131469727\n",
      "Total loss: 1339.0752334594727\n",
      "Validation loss: 1.3390752334594727\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7213664430\n",
      "\tInput: 11002\tPrediction: 44422\n",
      "Epoch 279, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 165.8880615234375, average loss 1.658880615234375\n",
      "Batch 2, Loss: 133.78627014160156, average loss 1.4983716583251954\n",
      "Batch 3, Loss: 153.06243896484375, average loss 1.5091225687662762\n",
      "Batch 4, Loss: 126.22198486328125, average loss 1.44739688873291\n",
      "Batch 5, Loss: 133.16305541992188, average loss 1.4242436218261718\n",
      "Batch 6, Loss: 116.07675170898438, average loss 1.3803309377034505\n",
      "Batch 7, Loss: 139.2583465576172, average loss 1.382081298828125\n",
      "Batch 8, Loss: 117.94456481933594, average loss 1.3567518424987792\n",
      "Batch 9, Loss: 157.73785400390625, average loss 1.381265920003255\n",
      "Batch 10, Loss: 149.855712890625, average loss 1.3929950408935547\n",
      "Total loss: 1377.1351623535156\n",
      "Validation loss: 1.3771351623535155\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 6467473\n",
      "Epoch 280, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 121.50701141357422, average loss 1.2150701141357423\n",
      "Batch 2, Loss: 149.47698974609375, average loss 1.35492000579834\n",
      "Batch 3, Loss: 120.93394470214844, average loss 1.3063931528727213\n",
      "Batch 4, Loss: 112.15760803222656, average loss 1.2601888847351075\n",
      "Batch 5, Loss: 108.48625183105469, average loss 1.2251236114501953\n",
      "Batch 6, Loss: 150.70098876953125, average loss 1.2721046574910482\n",
      "Batch 7, Loss: 119.96397399902344, average loss 1.2617525264195033\n",
      "Batch 8, Loss: 166.20172119140625, average loss 1.3117856121063232\n",
      "Batch 9, Loss: 143.68357849121094, average loss 1.3256800757514107\n",
      "Batch 10, Loss: 172.94093322753906, average loss 1.3660530014038086\n",
      "Total loss: 1435.8578567504883\n",
      "Validation loss: 1.4358578567504883\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 2445499\n",
      "Epoch 281, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 141.5896453857422, average loss 1.415896453857422\n",
      "Batch 2, Loss: 131.1938018798828, average loss 1.363917236328125\n",
      "Batch 3, Loss: 111.30536651611328, average loss 1.2802960459391277\n",
      "Batch 4, Loss: 112.8591079711914, average loss 1.2423698043823241\n",
      "Batch 5, Loss: 131.15509033203125, average loss 1.2562060241699218\n",
      "Batch 6, Loss: 106.02293395996094, average loss 1.2235432434082032\n",
      "Batch 7, Loss: 163.7552947998047, average loss 1.2826874869210378\n",
      "Batch 8, Loss: 147.5147705078125, average loss 1.3067450141906738\n",
      "Batch 9, Loss: 171.4960479736328, average loss 1.352102288140191\n",
      "Batch 10, Loss: 124.27106475830078, average loss 1.3411631240844726\n",
      "Total loss: 1326.990592956543\n",
      "Validation loss: 1.3269905929565429\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 1752019\n",
      "Epoch 282, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 128.71768188476562, average loss 1.2871768188476562\n",
      "Batch 2, Loss: 119.43009948730469, average loss 1.2407389068603516\n",
      "Batch 3, Loss: 109.49125671386719, average loss 1.192130126953125\n",
      "Batch 4, Loss: 142.94847106933594, average loss 1.2514687728881837\n",
      "Batch 5, Loss: 114.57633209228516, average loss 1.2303276824951173\n",
      "Batch 6, Loss: 157.27294921875, average loss 1.287394650777181\n",
      "Batch 7, Loss: 147.6650848388672, average loss 1.3144312504359654\n",
      "Batch 8, Loss: 168.79425048828125, average loss 1.3611201572418212\n",
      "Batch 9, Loss: 136.46900939941406, average loss 1.361516816880968\n",
      "Batch 10, Loss: 139.71502685546875, average loss 1.36508016204834\n",
      "Total loss: 1359.9971923828125\n",
      "Validation loss: 1.3599971923828125\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 8960600812\n",
      "\tInput: 925\tPrediction: 95\n",
      "Epoch 283, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 102.49437713623047, average loss 1.0249437713623046\n",
      "Batch 2, Loss: 134.63034057617188, average loss 1.1856235885620117\n",
      "Batch 3, Loss: 129.89390563964844, average loss 1.2233954111735026\n",
      "Batch 4, Loss: 111.75677490234375, average loss 1.1969384956359863\n",
      "Batch 5, Loss: 180.25814819335938, average loss 1.3180670928955078\n",
      "Batch 6, Loss: 160.3398895263672, average loss 1.3656223932902019\n",
      "Batch 7, Loss: 167.429443359375, average loss 1.4097183990478515\n",
      "Batch 8, Loss: 100.0909423828125, average loss 1.3586172771453857\n",
      "Batch 9, Loss: 144.69076538085938, average loss 1.3684273189968532\n",
      "Batch 10, Loss: 116.82347106933594, average loss 1.3484080581665039\n",
      "Total loss: 1342.4399795532227\n",
      "Validation loss: 1.3424399795532227\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607927\n",
      "\tInput: 94164\tPrediction: 1941674\n",
      "Epoch 284, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 135.31942749023438, average loss 1.3531942749023438\n",
      "Batch 2, Loss: 130.228759765625, average loss 1.327740936279297\n",
      "Batch 3, Loss: 119.33012390136719, average loss 1.2829277038574218\n",
      "Batch 4, Loss: 147.09776306152344, average loss 1.329940185546875\n",
      "Batch 5, Loss: 142.7707977294922, average loss 1.3494937438964845\n",
      "Batch 6, Loss: 173.31793212890625, average loss 1.4134413401285808\n",
      "Batch 7, Loss: 136.88775634765625, average loss 1.4070750863211496\n",
      "Batch 8, Loss: 141.24676513671875, average loss 1.4077491569519043\n",
      "Batch 9, Loss: 125.9263687133789, average loss 1.391250771416558\n",
      "Batch 10, Loss: 105.02804565429688, average loss 1.3571537399291993\n",
      "Total loss: 1343.3345260620117\n",
      "Validation loss: 1.3433345260620118\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 69201019472\n",
      "Epoch 285, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 137.48721313476562, average loss 1.3748721313476562\n",
      "Batch 2, Loss: 106.84536743164062, average loss 1.2216629028320312\n",
      "Batch 3, Loss: 162.17576599121094, average loss 1.355027821858724\n",
      "Batch 4, Loss: 145.22682189941406, average loss 1.379337921142578\n",
      "Batch 5, Loss: 176.90386962890625, average loss 1.457278076171875\n",
      "Batch 6, Loss: 112.29891967773438, average loss 1.4015632629394532\n",
      "Batch 7, Loss: 139.5601806640625, average loss 1.4007116263253347\n",
      "Batch 8, Loss: 127.81304168701172, average loss 1.3853889751434325\n",
      "Batch 9, Loss: 99.67658233642578, average loss 1.3422086249457466\n",
      "Batch 10, Loss: 114.03900146484375, average loss 1.3220267639160157\n",
      "Total loss: 1360.589469909668\n",
      "Validation loss: 1.360589469909668\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172475\n",
      "\tInput: 6824\tPrediction: 6524\n",
      "Epoch 286, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 121.0464096069336, average loss 1.210464096069336\n",
      "Batch 2, Loss: 165.74429321289062, average loss 1.433953514099121\n",
      "Batch 3, Loss: 126.02114868164062, average loss 1.3760395050048828\n",
      "Batch 4, Loss: 166.97418212890625, average loss 1.4494650840759278\n",
      "Batch 5, Loss: 106.68687438964844, average loss 1.372945816040039\n",
      "Batch 6, Loss: 124.91175079345703, average loss 1.3523077646891275\n",
      "Batch 7, Loss: 126.64949798583984, average loss 1.3400487954275948\n",
      "Batch 8, Loss: 113.19075012207031, average loss 1.3140311336517334\n",
      "Batch 9, Loss: 120.2082748413086, average loss 1.3015924241807726\n",
      "Batch 10, Loss: 131.1051025390625, average loss 1.3025382843017579\n",
      "Total loss: 1381.2423782348633\n",
      "Validation loss: 1.3812423782348633\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9338331\n",
      "\tInput: 135\tPrediction: 435\n",
      "Epoch 287, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 164.2498779296875, average loss 1.642498779296875\n",
      "Batch 2, Loss: 136.3402099609375, average loss 1.502950439453125\n",
      "Batch 3, Loss: 173.29864501953125, average loss 1.5796291097005208\n",
      "Batch 4, Loss: 115.82489013671875, average loss 1.4742840576171874\n",
      "Batch 5, Loss: 149.74497985839844, average loss 1.4789172058105469\n",
      "Batch 6, Loss: 119.14631652832031, average loss 1.4310081990559895\n",
      "Batch 7, Loss: 110.51304626464844, average loss 1.3844542367117745\n",
      "Batch 8, Loss: 98.19706726074219, average loss 1.3341437911987304\n",
      "Batch 9, Loss: 130.8043975830078, average loss 1.3312438117133247\n",
      "Batch 10, Loss: 104.77912139892578, average loss 1.302898551940918\n",
      "Total loss: 1317.8332977294922\n",
      "Validation loss: 1.3178332977294922\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 9643193\n",
      "Epoch 288, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 157.01528930664062, average loss 1.5701528930664062\n",
      "Batch 2, Loss: 171.3687744140625, average loss 1.6419203186035156\n",
      "Batch 3, Loss: 109.1558837890625, average loss 1.4584664916992187\n",
      "Batch 4, Loss: 124.19376373291016, average loss 1.4043342781066894\n",
      "Batch 5, Loss: 116.72518920898438, average loss 1.3569178009033203\n",
      "Batch 6, Loss: 111.38359832763672, average loss 1.3164041646321614\n",
      "Batch 7, Loss: 119.86627197265625, average loss 1.2995839582170758\n",
      "Batch 8, Loss: 123.3231201171875, average loss 1.2912898635864258\n",
      "Batch 9, Loss: 105.49518585205078, average loss 1.2650300852457683\n",
      "Batch 10, Loss: 145.45654296875, average loss 1.2839836196899415\n",
      "Total loss: 1282.4230575561523\n",
      "Validation loss: 1.2824230575561524\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7215664450\n",
      "\tInput: 11002\tPrediction: 4002\n",
      "Epoch 289, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 181.69110107421875, average loss 1.8169110107421875\n",
      "Batch 2, Loss: 121.1270751953125, average loss 1.5140908813476563\n",
      "Batch 3, Loss: 139.2322540283203, average loss 1.4735014343261719\n",
      "Batch 4, Loss: 114.52960205078125, average loss 1.391450080871582\n",
      "Batch 5, Loss: 102.71511840820312, average loss 1.318590301513672\n",
      "Batch 6, Loss: 114.64179992675781, average loss 1.2898949178059895\n",
      "Batch 7, Loss: 142.47103881835938, average loss 1.3091542707170758\n",
      "Batch 8, Loss: 115.25891876220703, average loss 1.2895836353302002\n",
      "Batch 9, Loss: 158.7721710205078, average loss 1.3227100880940754\n",
      "Batch 10, Loss: 136.15838623046875, average loss 1.3265974655151367\n",
      "Total loss: 1357.4063034057617\n",
      "Validation loss: 1.3574063034057617\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 66773\n",
      "Epoch 290, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 114.73565673828125, average loss 1.1473565673828126\n",
      "Batch 2, Loss: 150.32424926757812, average loss 1.325299530029297\n",
      "Batch 3, Loss: 127.78228759765625, average loss 1.3094739786783853\n",
      "Batch 4, Loss: 119.98516845703125, average loss 1.282068405151367\n",
      "Batch 5, Loss: 115.42086029052734, average loss 1.2564964447021485\n",
      "Batch 6, Loss: 129.82394409179688, average loss 1.2634536107381185\n",
      "Batch 7, Loss: 117.7452163696289, average loss 1.2511676897321429\n",
      "Batch 8, Loss: 169.12652587890625, average loss 1.3061798858642577\n",
      "Batch 9, Loss: 138.12770080566406, average loss 1.3145240105523004\n",
      "Batch 10, Loss: 170.6741943359375, average loss 1.353745803833008\n",
      "Total loss: 1317.8831787109375\n",
      "Validation loss: 1.3178831787109375\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 2454466\n",
      "Epoch 291, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 129.18759155273438, average loss 1.2918759155273438\n",
      "Batch 2, Loss: 131.5093994140625, average loss 1.3034849548339844\n",
      "Batch 3, Loss: 123.00079345703125, average loss 1.2789926147460937\n",
      "Batch 4, Loss: 118.35403442382812, average loss 1.2551295471191406\n",
      "Batch 5, Loss: 132.093505859375, average loss 1.2682906494140624\n",
      "Batch 6, Loss: 112.31462097167969, average loss 1.2440999094645182\n",
      "Batch 7, Loss: 156.63345336914062, average loss 1.2901334272112166\n",
      "Batch 8, Loss: 130.18177795410156, average loss 1.2915939712524414\n",
      "Batch 9, Loss: 176.8220977783203, average loss 1.3445525275336372\n",
      "Batch 10, Loss: 101.90850830078125, average loss 1.3120057830810548\n",
      "Total loss: 1314.0840301513672\n",
      "Validation loss: 1.3140840301513672\n",
      "Examples:\n",
      "\tInput: 21\tPrediction: 21\n",
      "\tInput: 76206\tPrediction: 179246\n",
      "Epoch 292, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 125.54164123535156, average loss 1.2554164123535156\n",
      "Batch 2, Loss: 104.67193603515625, average loss 1.151067886352539\n",
      "Batch 3, Loss: 108.60313415527344, average loss 1.1293890380859375\n",
      "Batch 4, Loss: 134.30050659179688, average loss 1.1827930450439452\n",
      "Batch 5, Loss: 98.1588134765625, average loss 1.1425520629882813\n",
      "Batch 6, Loss: 142.94088745117188, average loss 1.190361531575521\n",
      "Batch 7, Loss: 137.08847045898438, average loss 1.2161505562918526\n",
      "Batch 8, Loss: 173.25765991210938, average loss 1.2807038116455078\n",
      "Batch 9, Loss: 117.91921997070312, average loss 1.2694247436523438\n",
      "Batch 10, Loss: 146.2134246826172, average loss 1.2886956939697265\n",
      "Total loss: 1283.7790832519531\n",
      "Validation loss: 1.2837790832519531\n",
      "Examples:\n",
      "\tInput: 8960600812\tPrediction: 89160600812\n",
      "\tInput: 925\tPrediction: 95\n",
      "Epoch 293, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 124.59725189208984, average loss 1.2459725189208983\n",
      "Batch 2, Loss: 107.15138244628906, average loss 1.1587431716918946\n",
      "Batch 3, Loss: 135.23313903808594, average loss 1.2232725779215494\n",
      "Batch 4, Loss: 107.35558319091797, average loss 1.185843391418457\n",
      "Batch 5, Loss: 151.405029296875, average loss 1.2514847717285156\n",
      "Batch 6, Loss: 144.95291137695312, average loss 1.2844921620686849\n",
      "Batch 7, Loss: 161.0020751953125, average loss 1.3309962463378906\n",
      "Batch 8, Loss: 113.49794006347656, average loss 1.306494140625\n",
      "Batch 9, Loss: 132.5493927001953, average loss 1.308605228000217\n",
      "Batch 10, Loss: 132.59423828125, average loss 1.3103389434814454\n",
      "Total loss: 1289.7481842041016\n",
      "Validation loss: 1.2897481842041016\n",
      "Examples:\n",
      "\tInput: 607927\tPrediction: 607927\n",
      "\tInput: 94164\tPrediction: 192454\n",
      "Epoch 294, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 118.12429809570312, average loss 1.1812429809570313\n",
      "Batch 2, Loss: 119.70402526855469, average loss 1.1891416168212892\n",
      "Batch 3, Loss: 101.20770263671875, average loss 1.130120086669922\n",
      "Batch 4, Loss: 136.78759765625, average loss 1.1895590591430665\n",
      "Batch 5, Loss: 128.42979431152344, average loss 1.2085068359375\n",
      "Batch 6, Loss: 159.69808959960938, average loss 1.2732525126139322\n",
      "Batch 7, Loss: 134.41281127929688, average loss 1.2833775983537947\n",
      "Batch 8, Loss: 130.2435760498047, average loss 1.2857598686218261\n",
      "Batch 9, Loss: 133.38485717773438, average loss 1.291103057861328\n",
      "Batch 10, Loss: 107.19406127929688, average loss 1.2691868133544921\n",
      "Total loss: 1270.3015747070312\n",
      "Validation loss: 1.2703015747070312\n",
      "Examples:\n",
      "\tInput: 041\tPrediction: 041\n",
      "\tInput: 64009172\tPrediction: 671003472\n",
      "Epoch 295, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 122.2837142944336, average loss 1.2228371429443359\n",
      "Batch 2, Loss: 101.96737670898438, average loss 1.12125545501709\n",
      "Batch 3, Loss: 153.61007690429688, average loss 1.2595372263590494\n",
      "Batch 4, Loss: 138.246337890625, average loss 1.2902687644958497\n",
      "Batch 5, Loss: 171.06005859375, average loss 1.3743351287841796\n",
      "Batch 6, Loss: 106.78059387207031, average loss 1.323246930440267\n",
      "Batch 7, Loss: 127.61005401611328, average loss 1.316511731828962\n",
      "Batch 8, Loss: 125.61239624023438, average loss 1.3089632606506347\n",
      "Batch 9, Loss: 94.61820220947266, average loss 1.2686542341444227\n",
      "Batch 10, Loss: 102.08102416992188, average loss 1.2438698348999024\n",
      "Total loss: 1255.2090530395508\n",
      "Validation loss: 1.2552090530395508\n",
      "Examples:\n",
      "\tInput: 268172475\tPrediction: 268172475\n",
      "\tInput: 6824\tPrediction: 66532\n",
      "Epoch 296, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 127.6485595703125, average loss 1.276485595703125\n",
      "Batch 2, Loss: 144.48065185546875, average loss 1.3606460571289063\n",
      "Batch 3, Loss: 156.08932495117188, average loss 1.4273951212565104\n",
      "Batch 4, Loss: 170.01806640625, average loss 1.495591506958008\n",
      "Batch 5, Loss: 109.214599609375, average loss 1.4149024047851562\n",
      "Batch 6, Loss: 116.75235748291016, average loss 1.3736725997924806\n",
      "Batch 7, Loss: 125.86659240722656, average loss 1.3572430746895927\n",
      "Batch 8, Loss: 107.7402572631836, average loss 1.322263011932373\n",
      "Batch 9, Loss: 106.64117431640625, average loss 1.2938350931803386\n",
      "Batch 10, Loss: 133.94070434570312, average loss 1.298392288208008\n",
      "Total loss: 1369.475814819336\n",
      "Validation loss: 1.369475814819336\n",
      "Examples:\n",
      "\tInput: 9338531\tPrediction: 9538531\n",
      "\tInput: 135\tPrediction: 135\n",
      "Epoch 297, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 158.4717559814453, average loss 1.5847175598144532\n",
      "Batch 2, Loss: 134.1345672607422, average loss 1.4630316162109376\n",
      "Batch 3, Loss: 150.21058654785156, average loss 1.476056365966797\n",
      "Batch 4, Loss: 127.50701904296875, average loss 1.4258098220825195\n",
      "Batch 5, Loss: 160.54214477539062, average loss 1.4617321472167968\n",
      "Batch 6, Loss: 119.93061065673828, average loss 1.417994473775228\n",
      "Batch 7, Loss: 101.40760803222656, average loss 1.3602918461390905\n",
      "Batch 8, Loss: 93.06523132324219, average loss 1.3065869045257568\n",
      "Batch 9, Loss: 130.70469665527344, average loss 1.3066380225287544\n",
      "Batch 10, Loss: 121.00010681152344, average loss 1.2969743270874023\n",
      "Total loss: 1370.2629318237305\n",
      "Validation loss: 1.3702629318237305\n",
      "Examples:\n",
      "\tInput: 909840\tPrediction: 909840\n",
      "\tInput: 9303634\tPrediction: 13323334\n",
      "Epoch 298, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 153.5326385498047, average loss 1.5353263854980468\n",
      "Batch 2, Loss: 149.8165740966797, average loss 1.516746063232422\n",
      "Batch 3, Loss: 104.02560424804688, average loss 1.3579160563151043\n",
      "Batch 4, Loss: 135.32952880859375, average loss 1.3567608642578124\n",
      "Batch 5, Loss: 118.93727111816406, average loss 1.3232832336425782\n",
      "Batch 6, Loss: 112.75108337402344, average loss 1.2906545003255208\n",
      "Batch 7, Loss: 108.89460754394531, average loss 1.2618390110560826\n",
      "Batch 8, Loss: 137.34451293945312, average loss 1.2757897758483887\n",
      "Batch 9, Loss: 102.49205017089844, average loss 1.2479154120551215\n",
      "Batch 10, Loss: 132.687744140625, average loss 1.2558116149902343\n",
      "Total loss: 1307.9272994995117\n",
      "Validation loss: 1.3079272994995117\n",
      "Examples:\n",
      "\tInput: 7215664450\tPrediction: 7215664450\n",
      "\tInput: 11002\tPrediction: 40102\n",
      "Epoch 299, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 172.7279052734375, average loss 1.727279052734375\n",
      "Batch 2, Loss: 108.93147277832031, average loss 1.4082968902587891\n",
      "Batch 3, Loss: 127.7225112915039, average loss 1.3646062978108724\n",
      "Batch 4, Loss: 117.0097427368164, average loss 1.3159790802001954\n",
      "Batch 5, Loss: 99.09077453613281, average loss 1.2509648132324218\n",
      "Batch 6, Loss: 104.91275024414062, average loss 1.2173252614339192\n",
      "Batch 7, Loss: 127.17121887207031, average loss 1.2250948224748883\n",
      "Batch 8, Loss: 110.832275390625, average loss 1.2104983139038086\n",
      "Batch 9, Loss: 133.27381896972656, average loss 1.2240805223253037\n",
      "Batch 10, Loss: 124.71719360351562, average loss 1.226389663696289\n",
      "Total loss: 1228.821678161621\n",
      "Validation loss: 1.228821678161621\n",
      "Examples:\n",
      "\tInput: 94245934\tPrediction: 94245934\n",
      "\tInput: 6167476\tPrediction: 6467473\n",
      "Epoch 300, lr=[0.00015625]\n",
      "-----------------------\n",
      "Batch 1, Loss: 119.31863403320312, average loss 1.1931863403320313\n",
      "Batch 2, Loss: 124.38815307617188, average loss 1.218533935546875\n",
      "Batch 3, Loss: 113.11563110351562, average loss 1.1894080607096353\n",
      "Batch 4, Loss: 103.17448425292969, average loss 1.1499922561645508\n",
      "Batch 5, Loss: 101.27532958984375, average loss 1.122544464111328\n",
      "Batch 6, Loss: 145.79627990722656, average loss 1.178447519938151\n",
      "Batch 7, Loss: 115.12158203125, average loss 1.1745572771344865\n",
      "Batch 8, Loss: 146.59922790527344, average loss 1.2109866523742676\n",
      "Batch 9, Loss: 133.13961791992188, average loss 1.224365488688151\n",
      "Batch 10, Loss: 170.25015258789062, average loss 1.2721790924072265\n",
      "Total loss: 1269.6566619873047\n",
      "Validation loss: 1.2696566619873046\n",
      "Examples:\n",
      "\tInput: 425368\tPrediction: 425368\n",
      "\tInput: 21151199\tPrediction: 245409\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "losses = [] # store the validation losses during training\n",
    "bestIndex = 0\n",
    "\n",
    "bestLoss = np.inf\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}, lr={scheduler.get_last_lr()}\\n-----------------------\")\n",
    "    ocr_model.train()\n",
    "    train_epoch(trainGenerator, ocr_model, optimizer)\n",
    "    ocr_model.eval()\n",
    "    valLoss = validate(valGenerator, ocr_model)\n",
    "    losses.append(valLoss)\n",
    "    if valLoss < bestLoss:\n",
    "        bestIndex = t\n",
    "        bestLoss = valLoss\n",
    "\n",
    "    ocr_visualization.visualize_text(ocr_model, valGenerator, allChars, labelsToCharsDict, device=device) # helper function\n",
    "    scheduler.step()\n",
    "\n",
    "ocr_model.eval()\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c4713",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "Save the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74bdf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(ocr_model.state_dict(), \"ocr_model_weights.pth\")\n",
    "# torch.save(ocr_model, \"ocr_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48da070",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Cooper'\"> \n",
    "Visualize the performance of the trained model\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104abe0",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "In an appropriately labeled graph, plot the average validation loss (the quantity returned by the validation function) as a function of the number of epochs of training. For more meaningful visualization, only show results on the last 150 training epochs.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b2b488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(151,301)  # the last 150 training epochs.\n",
    "losses = losses[150:]\n",
    "sns.lineplot(x=x, y=losses)\n",
    "\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef86b5",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Consolas'\">\n",
    "Visualize the output of the best model on 10 example inputs from the validation generator, using ocr_visualization.visualize_image.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b38ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ground truth</th>\n",
       "      <th>recovered label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iivP7ux8U+Ltb1GManeeHtJspPJgEKYluWxzITkfL6Y/nmgD0CiuK+HOq6pe2eq6fq10t5Ppd89oLsD/Wgevv8A/Wo+KGoapp3hRW0q8+yTz3McG9Qd53Hop7H3oA7Wio4EMcEaMxYqoUsTknA60UASE4FcFd+ItZ8XXM2m+ElNtYoxjuNalX5RjgiEfxH3/lwa7S/sxqGnXNm00sKzxtGZISA6gjBKkggH8K4SH4PaTbxLFBr/AIjijX7qJeKoH0ASgDr/AA74fsvDWjxadZBiiks8jnLyOerMfU1zHj8C+8R+DtI4Im1L7S6+qxDP9TXYaTpsekaXb2EU08yQJtEk77nb3Y9zVW68PWd54ksdcleb7TZRPHEgI2fP1JGM5/GgDWooooA//9k=\"></td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,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\"></td>\n",
       "      <td>76206</td>\n",
       "      <td>1321019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgADEDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iivNPiB8Qr3SdQi0nw8scl0kqLdzum9IS/Cp/vHr9B9cAHpdFYfijxNb+GNNS4lhlubiZxDbW0KktNIeij0rE+GvibV/FOnale6qkUZiuzDHHGmAgABIz36igDt6K4bUtX8YarrmoWPh+1trGzsBhru/ib/AEiTGcIOm33/AMa0vAPiafxZ4Vh1K6hSK4EjRSCP7rFe49jmgDp6KKKAMHxlqWq6X4ZurjRbGW8vzhIkjQuVJ43YHXHWvHLu5NhY+HtM/wCEd19LptUS+u5Lm2xJeyLywXnk88D0+tfQNY+p+HLPVdb0rVbiScTaYztCiMAjFgAdwIycY4wRQBbuLkJpL3skRQxwmbZIBlCFJwfcdK4z4OQ+T8Porh+DcXE0zE/Xb/7LXR+JfDEXia3igm1TVLGOMMGWwuBGJQwAIfIORx+pqDwr4MtPCMcsVnqWp3MLqFWG7nDpFyT8ihQBnPNAGV4seDxf4Ku73RPEUlvb2ySmSS3JCyFV5Ruhx/j3q98Nrhbr4faTMlnFaBo2HlRLhchyN344z+NUZ/hR4fmvLiRbjU4LW5k82awhuttvI3XlcZ/XjtXaWtrBZWsVrbRJFBEgSONBgKo6AUAS0UUUAf/Z\"></td>\n",
       "      <td>413</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACkDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+ioLy7g0+ynvLmQRwQRtJI5/hUDJNcJpHxUh1LWtPtJtEvbSz1N2Syu5WBEpBx93HHPHBPUUAehUVwHiD4pWujapd2ttpN1qEGnlVv7mIgJbljgDpyc8dueK1PEfjq00K10w21rNqN3qmDZ20PBkBAOST0HI7d6AOrornPBni+Dxhpc11HayWs0ExhmhkO7awAPB79a6OgCrqF7Y2Fo02o3Nvb25O0vO4VTntk8V5eiXc/xm0yz15re4SG3kuNNSz+WOEHcQXGOThfzC16P4g0Cw8S6PLpmoxs1vJg5RsMpHQg+tZXhnwDo3ha6kvLVrq5vHQR/aLyXzHVP7owAAOB27UAeQ+JbXXvDGj+I9KutMLRavqavHe+aP3o3FlVV6knH4c1t+KEubzxLaabo8yWN74W0kzTXznOMRr8gHTuOfc+ld3Z/DfRLbXRq801/ezpIZYo7ufzI4nJzlVwO/rmm6/wDDPQvEWsvqlzJewTyoEnW2mCLMBjhhg+g6Y6CgCP4TQ2qfD2wmt7doXnLvMXbJkkDFS2ffaK7aq9jY22m2MNlZxLDbwIEjReigVYoA/9k=\"></td>\n",
       "      <td>398</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,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\"></td>\n",
       "      <td>6462577411</td>\n",
       "      <td>6462577411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgAC0DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iqmqala6PpdzqN6+y2t4zJI2MnA9B3PauDsfipI1zYy6p4du9O0jUJBHa37yBgxPQsMDAP1PryKAPR6Ko6xq9noWk3GpX8vl28C7mPc+gHqSeBXL3HxCh/4V0fFVvZvukJjgt5CCWk3lBnHbIz9KAO2orznw7qXjCbXLaO513QdTiY/6baW7qJLQY5xt5ODx35/OvRqAOa8f6Rda54H1OwslLXLxho0B++VYNt/HGK8y1bxB/wm3hfRfB+m6ZdrqiyQpciSIqtuI12lifTv9PevcqQKASQBk9TQB4z4415rvxe9jq2l6lNouloWjiggLLcT7eHc8Dauf09zWNaGbXPgY1ja2d0ZNKuVnlOzAkQu5JQ98A8+mK97ubeO7tZreXJjlRkbBwcEYNU9D0Wz8PaTDplgrrbQ7tgdyxGSSeT7k0AePXsnha+1bwqngKDZqq3KO7QRspSED5vNJ6n1PPGfWvcqr29hZ2kjyW1pBC7/AHmjjClvrgc1YoA//9k=\"></td>\n",
       "      <td>046</td>\n",
       "      <td>046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgADMDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+ignAzXJ2/jeO98S6dplrpt19lvPN2Xs6GJX8tdx2KfmI6cnA9M0AdZRVPVNTt9IsHvLrzTGpA2xRtI7EnAAVQSSTWZ4b8Ry69c6nFNps1gbKVIxHOw8whkDAsBwOCOMmgDformtY1TUrzWhoGhyRwXCxCe8vZE3i2RiQoVejO2DgHgAE81saVYyadYrbzX9zfSAlmmuSCxJ+gAA9BQBdooooAK5HXyP+FieD/pe/wDopa66uTu/AovNTh1CTxJrouIGkMBWWHEW/hgv7vpjjmgDq+1ct4ax/wAJZ4v/AOvyH/0QlbcmmySaONPGo3qOECfa1ZfO4x82du3Jxzx3rF0/wUNN1OS/j8Ra5JJLKks6ySxFZioAAbEYOMADgjigDGsfDtnr3jfxUdX3XMEU8AS0diIzmFcOwH3vQZ4HzetXvDUC6L411bQLF3OlpaQ3UcLOWFs7MylFz0BABx+VbGqeF7bUr8ahDe32nXuwRvPYyhDIg5AYMCpxk44yKsaJoFnoUUwtzNLPcP5lxc3D75ZmxgFm9h0AwB2FAGpRRRQB/9k=\"></td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgAHQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iivPZPE/ifxRqd5b+EILGHT7OUwyahfZIlkHUIB298HtQB6FRXA+GvE3iJPGc/hbxEljPOtt9pS4ss4AyOGB6fkP1ptz4m8TeItc1DTfCUOnxW+nSCG4vb0k7pOchVHbjrg0AegUV53pviTxVpHjTT/AA94kXT7pdQR2hnsshk2gnkHHHHp/KtrxBa+Np9S3aDf6TBZbANl1GzPu5yeFPHSgDqqK8mvNW+Idp4ksNBGqaNcXt0N7rBblvIjHV3yBgdceuK7fxV4kHhHw4L2ZDd3JKwRIMJ5sp/kOCaAOiorzqf/AIWitrJfNN4fgCqZDakMSoHOCx4/HNaej+Pre5+Ha+K9Uh+zIoYSRpzuYNtATPqfy/CgDsqK86S8+JWtW/8AaFlBpGmW7ruhtbnc8pHUbjjAJ/CtrwN4pu/ElhexajZra6np9wba5jU/KWHcdcdCPwoA6uiuAaw+J+Sf7X8Pgf8AXF//AImq/gTWfFuv61ePfX1jNpFm5i8+2gwtxJ6ITg4HUnHp60Aej0UUUARXVzBZ2stzcyLFBEheR2OAqgZJNeZ6aNd8T2xg8LQx+GfDJditz5X7+4yeWRf4QfXj69q73xFoUHiXRJ9Kuri5ggnxva3YK5AOcZIIxx6Vycfwm0+KNY4/EfiVEUBVVb5QAB0AGygC/Do2k/Dnw1qeqW8UtzcpCZZ7idt0s7DoC3YZ9PrXJ+Hfh23iKD/hKLvXrq0u9UHnsmlEQqm7tnnJ9ffNd3ong2x0WxvrN7zUNShvQFlXUZ/N+XBGBwMA5Oawh8ItDhZhZ6nrdnAxz5FvebU/VSf1oAwfCejL4a+Ls+mwXf8AawksTLLczjdNbnP3S3vx+Yr0DxX4lt/C+jNdyL5txIwitbdfvTSnooH86f4e8KaR4Xt5ItLttjSnMsrsWkkP+0x//VSXHheyvPE9vr11LcTz2sZS3gdh5MRPVlXGd3uSf0GACh4L8N3Gk28+qas4m13UT5t3L/c9I19Av+egrnvHTP4u8U2vgWOWG2gaEXlxcOm5+DwsYPf39/bn0uub8TeBtF8VTQ3F8k0V3CNsdzbSbJFHXGeQfxFAHCeKPhja6Z4Tu528VaqI7aEuI7qYNExA4Xbx1PA+tQ6zdG88DeArG6ihsbe9vIfOCrtQIvAOO2Q2a622+FWgR3Ec19c6nqgjOVjv7rzEB+gAz+NdFr3hrSvEmlf2bqVsHtwQU2/KYyOAVI6UAXL6/tdLsJr27mSG3hUu7scAAVxvwptZj4evNZuVZZdXvZbv5upUnA/qaW2+E3h+KWJru61TUYoiClveXW+Mf8BAFdwsSpCIogI1VdqhABtHbA6UAcP4w1W71rVE8F6JKUubhd+o3S/8usHcf7zdMe/vXX6VpdpoumW+nWMQitoECIo/mfUnqTVHw94YsfDiXRt5Li4uLuUzXFzcsGlkY+pAAwOwx3raoAKKKKAP/9k=\"></td>\n",
       "      <td>287807</td>\n",
       "      <td>287807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgAEsDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iormf7NbvKIpZSvSOJcsx9BVCDVpPtiW19ZPaNKrNExkV1bbyQSOhxz6deeKANSisb+32MP2tNNu3sMbvtChclf74TO4r36Zx0FJqF/tvrEtei20+VC/nqVCyPxtQsRgAgk++PzANqisjSrp59QvEiuzeWShSk3BCuSdyBgMMAAp9s4z6TXGrRw6va6esbSPMSHdT8sXyswz7nacD8aANGisubVrhZZfs+lXU8MTFXkBVSxHXYrEFsfhntmq95qayS6dMt79m02dDIbgYAdvl2IWYYUEFj6nGPqAblFY+mXbz6pdpDdm8sgikS5UhJMnKBgMHjBx1H41sUAUNY1JdK02S6IQsCqIHfYpZiFGWPQZPJ9KzLeKw1N5Fn1m3vb2WF4lEMi7YlYfNsQE/mcnjrXREAjBGRTQig5CgfQUAYsOoX9rZJbS6ZJ9oiQKZQy+QQB9/dnIXAzjGe1ZOnxJaWmjPq6NNaJYRqjmMtHHL1JZecEjbgnpgjIzztnw/D5Rt/tl2LItk2gddmP7udu7b7bsdunFawAAwOlAHMxzPFc3t3odi8lv5AHlqvlpNNu4Kg4HAzuI68DkioRNJaXOkxnTdQaVrxnlmdIwZHMMgJ4fj6dgMdq62o5LeKaSGSRAzQsXjP907SufyYj8aAK2pw389qV0+6jt5f7zx7s+w54+uD9K5+J7TZpVxcWz/2StrtjRlMiwyg4y+Ac8DAY8cHpmtp9FUySmG/vbeOVizxRSALk8kjIJXPsRV+CCK2t44IUCRRqFRR0AHQUAY2meRJrck2mwmOxMJEzKhSOSXcNpUYGSBuyR6gZOON2iigD/9k=\"></td>\n",
       "      <td>298438</td>\n",
       "      <td>298438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td><img src=\"data:image/jpeg;base64,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\"></td>\n",
       "      <td>4563808452</td>\n",
       "      <td>4563808452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # I have shut down the kernel, but I did not clear the training logs.\n",
    "# ocr_model.load_state_dict(torch.load('ocr_model_weights.pth', weights_only=True))\n",
    "ocr_visualization.visualize_image(ocr_model, valGenerator, allChars, labelsToCharsDict, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587ecb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homework",
   "language": "python",
   "name": "homework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
