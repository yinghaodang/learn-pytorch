{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Transformer Model\n",
    "\n",
    "I want to implement a transformer model by pytorch like the photo below.\n",
    "\n",
    "<img src=\"transformer.png\" alt=\"Transformer Image\">\n",
    "<!-- <div style=\"text-align: center;\">\n",
    "    <img src=\"transformer.png\" alt=\"Transformer Image\" style=\"width: 20%; height: auto;\">\n",
    "</div> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotate Position Embedding (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "batch_size = 64  # Batch size\n",
    "max_len = 1000  # Maximum length of a sequence\n",
    "d_model = 512  # Embedding size\n",
    "\n",
    "\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % 2 == 0, \"d_model should be devisible by 2.\" \n",
    "        \n",
    "        position = torch.arange(max_len).reshape(-1, 1)\n",
    "        inv_freq = torch.exp(- math.log(10000.0) * torch.arange(0, d_model, 2) / d_model)\n",
    "        pe_sin = torch.sin(position * inv_freq)\n",
    "        pe_cos = torch.cos(position * inv_freq)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, ::2] = pe_sin\n",
    "        pe[:, 1::2] = pe_cos\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape is [batch_size, seq_len, d_model]\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# m = PositionEmbedding(d_model=d_model, max_len=max_len, dropout=0.5)\n",
    "# x = torch.randn(10, 100, 512)\n",
    "# m(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Multi-Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query: torch.tensor, key: torch.tensor, value: torch.tensor, mask: torch.tensor, dropout: nn.Module):\n",
    "\n",
    "    q_d = query.size(-1)\n",
    "    k_d = key.size(-1)\n",
    "    assert q_d == k_d, \"q_d should equal to k_d\"\n",
    "\n",
    "    scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(k_d, dtype=torch.float32))\n",
    "    if mask is not None:\n",
    "        scores.masked_fill(mask==0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)    \n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    return p_attn @ value, p_attn\n",
    "\n",
    "\n",
    "class MultiAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0, \"d_model should be devisible by h\"\n",
    "\n",
    "        self.d_h = d_model // h\n",
    "        self.h = h  # heads num\n",
    "        self.linearlist = nn.ModuleList(\n",
    "            [nn.Linear(d_model, d_model) for _ in range(4)]\n",
    "        )\n",
    "        self.p_attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: [batch_size, seq_len, d_k]  d_k is the aggregation of the hidden dimensions of all attention heads.\n",
    "        key: [batch_size, seq_len, d_k]    we make d_k == d_v == d_h\n",
    "        value: [batch_size, seq_len, d_v]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query, key, value = [linear(x) for linear,x in zip(self.linearlist, (query, key, value))]        \n",
    "        query = query.view(batch_size, -1, self.h, self.d_h).transpose(1,2)\n",
    "        key = key.view(batch_size, -1, self.h, self.d_h).transpose(1,2)\n",
    "        value = value.view(batch_size, -1, self.h, self.d_h).transpose(1,2)\n",
    "\n",
    "        mat_attention, self.p_attn = attention(query, key, value, mask, self.dropout)\n",
    "        mat_attention = mat_attention.transpose(1, 2).contiguous().view(batch_size, -1, d_model)\n",
    "\n",
    "        return self.linearlist[-1](mat_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# d_model, h = 12, 4\n",
    "# x1 = torch.randn(3, 5, d_model)\n",
    "# x2 = torch.randn(3, 5, d_model)\n",
    "# x3 = torch.randn(3, 5, d_model)\n",
    "\n",
    "# m = MultiAttention(d_model, h)\n",
    "# r = m(x1, x1, x1)\n",
    "\n",
    "# r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, d_model:int, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, subnetwork):\n",
    "        return x + self.dropout(x + self.norm(subnetwork(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# size = 4\n",
    "# m = Residual(size, dropout=0.1)\n",
    "# x = torch.randn(5, size)\n",
    "# m(x, nn.Linear(size, size)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_a = nn.Linear(d_model, d_ff)\n",
    "        self.linear_b = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_b(self.dropout(F.relu(self.linear_a(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# d_model = 7\n",
    "# m = FeedForward(d_model, 5, 0.1)\n",
    "# x = torch.randn(6, d_model)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        attn: MultiAttention, \n",
    "        ffn: FeedForward, \n",
    "        res: Residual\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "        self.resduial = nn.ModuleList(\n",
    "            res for _ in range(2)\n",
    "        )\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn = self.resduial[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.resduial[1](attn, self.ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# d_model = 128\n",
    "# attn = MultiAttention(d_model=d_model, h=4, dropout=0.1)\n",
    "# ffn = FeedForward(d_model=d_model, d_ff=256)\n",
    "# res = Residual(d_model=d_model, dropout=0.1)\n",
    "# m = EncoderLayer(attn=attn, ffn=ffn, res=res)\n",
    "\n",
    "# x = torch.randn(3, 400, d_model) # [batch_size, seq_len, d_model]\n",
    "\n",
    "# # m(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nest the EncoderLayer to Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encodelayer: EncoderLayer, N):\n",
    "        super().__init__()\n",
    "        self.encodelayers = nn.ModuleList(\n",
    "            [encodelayer for _ in range(N)]\n",
    "        )\n",
    "        self.normlayer = nn.LayerNorm(encodelayer.d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.encodelayers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.normlayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# d_model = 128\n",
    "# attn = MultiAttention(d_model=d_model, h=4, dropout=0.1)\n",
    "# ffn = FeedForward(d_model=d_model, d_ff=256)\n",
    "# res = Residual(d_model=d_model, dropout=0.1)\n",
    "# m = Encoder(EncoderLayer(d_model=d_model, attn=attn, ffn=ffn, res=res), N=3)\n",
    "\n",
    "# x = torch.randn(3, 400, d_model) # [batch_size, seq_len, d_model]\n",
    "# m(x, mask=None).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int,\n",
    "        attn: MultiAttention, \n",
    "        ffn: FeedForward, \n",
    "        res: Residual, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attnlist = nn.ModuleList(\n",
    "            [attn for _ in range(2)]\n",
    "        )\n",
    "        self.ffn = ffn\n",
    "        self.reslist = nn.ModuleList(\n",
    "            [res for _ in range(3)]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        x = self.reslist[0](x, lambda x: self.attnlist[0](x, x, x, tgt_mask))\n",
    "        x = self.reslist[1](x, lambda x: self.attnlist[0](x, memory, memory, src_mask))\n",
    "        x = self.reslist[2](x, self.ffn)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# d_model = 128\n",
    "# attn = MultiAttention(d_model=d_model, h=4, dropout=0.1)\n",
    "# ffn = FeedForward(d_model=d_model, d_ff=256)\n",
    "# res = Residual(d_model=d_model, dropout=0.1)\n",
    "# m = Encoder(EncoderLayer(d_model=d_model, attn=attn, ffn=ffn, res=res), N=3)\n",
    "\n",
    "# x = torch.randn(3, 400, d_model) # [batch_size, seq_len, d_model]\n",
    "# m(x, src_mask=None, tgt_mask=None).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nest the DecoderLayers to Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoderlayer:DecoderLayer, N):\n",
    "        super().__init__()\n",
    "        self.decoderlayers = nn.ModuleList(\n",
    "            [decoderlayer for _ in range(N)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(decoderlayer.d_model)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for decoderlayer in self.decoderlayers:\n",
    "            x = decoderlayer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeneratorLayer\n",
    "\n",
    "Change the dim from d_model to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab, embedding_dim=d_model)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling all models together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "在实现的时候往往遇到这样一个问题, 在__init__方法中应该传入nn.Module对象还是传入更加具体的参数比如d_model。\n",
    "经过尝试, 我认为比较好的一个实践是考量后续对模型结构是否会进行调整。对于一些较为稳定且简单的结构, 比如Embedding, Linear可以不显式传入模型,\n",
    "在__init__中定义, 一般情况下, 我更青睐于在__init__中写清楚会用的组件, 尤其是一些自定义的模型。\n",
    "但是, Transformer很多时候都没有这种规范.\n",
    "\"\"\"\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model: int,\n",
    "        pos_embed: PositionEmbedding,\n",
    "        encode: Encoder,\n",
    "        decode: Decoder,\n",
    "        gen: Generator, \n",
    "        embed: Embeddings,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encode = encode\n",
    "        self.decode = decode\n",
    "        self.src_embed = embed\n",
    "        self.tgt_embed = embed\n",
    "        self.generator = gen\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # pre-process\n",
    "        self.src_process = nn.Sequential(self.src_embed, pos_embed)\n",
    "        self.tgt_process = nn.Sequential(self.tgt_embed, pos_embed)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src = self.src_process(src)\n",
    "        tgt = self.tgt_process(tgt)\n",
    "        tmp = self.encode(src, mask=None)\n",
    "        tmp = self.decode(tgt, tmp, src_mask=None, tgt_mask=None)\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "d_model = 512\n",
    "N = 6\n",
    "src_vocab = 100\n",
    "tgt_vocab = 100\n",
    "attn = MultiAttention(d_model, h=8)\n",
    "ffn = FeedForward(d_model, d_ff=2048, dropout=0.1)\n",
    "res = Residual(d_model, dropout=0.1)\n",
    "transformer = EncoderDecoder(\n",
    "    d_model=d_model,\n",
    "    pos_embed=PositionEmbedding(d_model, max_len=5000, dropout=0.1),\n",
    "    encode=Encoder(EncoderLayer(d_model, attn, ffn, res), N),\n",
    "    decode=Decoder(DecoderLayer(d_model, attn, ffn, res, dropout=0.1), N),\n",
    "    gen=Generator(d_model, tgt_vocab),\n",
    "    embed=Embeddings(d_model, tgt_vocab)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "x = torch.randint(0, 100, (10, 1000))\n",
    "transformer.forward(x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homework",
   "language": "python",
   "name": "homework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
